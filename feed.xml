<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Space Moon</title>
  <icon>https://www.gravatar.com/avatar/cbfa1112a0cbccdd92da7e3c728d4a63</icon>
  
  <link href="https://www.thespacemoon.com/feed.xml" rel="self"/>
  
  <link href="https://www.thespacemoon.com/"/>
  <updated>2026-02-22T00:00:00.000Z</updated>
  <id>https://www.thespacemoon.com/</id>
  
  <author>
    <name>Jihyung Moon</name>
    <email>mjihyung@gmail.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>The Shrinking Moat</title>
    <link href="https://www.thespacemoon.com/2026/02/22/the-shrinking-moat/"/>
    <id>https://www.thespacemoon.com/2026/02/22/the-shrinking-moat/</id>
    <published>2026-02-22T00:00:00.000Z</published>
    <updated>2026-02-22T00:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>In mid-2022, I typed a prompt into platform.openai.com and got backsomething that didn't feel like a demo.</p><p>It was text-davinci-002. Before ChatGPT, before the hype. But itwasn't the output that mattered. It was the feeling: this is going tobreak everything we know about building AI products.</p><p>Building a good AI model meant collecting data, training, andfine-tuning for specific tasks. Hard, slow, expensive work. Davinci-002could do a rough version of most of it with just a prompt — plainEnglish, no data, no training, no cost.</p><p>My co-founders and I were already building softly, an AI startup. Afew months later, ChatGPT launched and everyone else felt it too.<span id="more"></span></p><h2 id="the-arms-race">The arms race</h2><p>When OpenAI was just an API company, we thought small LLMs plus RAGwas our moat. We sold to other startups who wanted to steer models withtheir own knowledge — something OpenAI's raw API couldn't do out of thebox. That lasted about six months. OpenAI launched function calling inJune 2023, then the Assistants API with built-in retrieval thatNovember. RAG became a feature, not a product.</p><p>So we moved to agents. Multi-step reasoning, tool use, orchestrationacross systems. Agents felt defensible — they required domain knowledge,custom workflows, real integration work. Then o1 arrived in September2024. Reasoning became a service. The gap shrank again.</p><p>We retreated further into domain-specific agent software. Surely thecombination of deep vertical knowledge and agent architecture wouldhold. But by early 2025, o3 and o4-mini made it clear: every layer webuilt was getting absorbed into the next release.</p><p>Each moat lasted months, not years. And each time we moved, we movedfurther from what we were actually good at.</p><h2 id="what-i-got-wrong-as-an-ai-startup-founder">What I got wrong (asan AI startup founder)</h2><p>We didn't expect OpenAI to move that fast. But speed wasn't the onlything I got wrong.</p><p><strong>I underestimated LLM companies' appetite.</strong> I assumedthey'd stay in the infrastructure layer — selling APIs, not buildingapplications. They didn't. LLM companies turned out to be better atbuilding LLM-based software than anyone else, because they control thefoundation. Anything an application startup built on top, they couldabsorb — just by shipping it as a feature.</p><p>The application layer wasn't safe ground. It was their roadmap.</p><p><strong>I underestimated domain knowledge.</strong> I thought wecould learn how any industry works. We told ourselves we were fastlearners. But the real information in any industry is hidden. It livesin relationships, in operational details that nobody writes down, inyears of pattern-matching that insiders don't even recognize asknowledge. And we had no way in. No network, no warm introductions. Youcan't cold-call your way into domain expertise.</p><p><strong>I underestimated our own core strength.</strong> Our team wasbuilt to train AI models. That was what we were genuinely good at. Butthe arms race spooked us. Instead of leaning into model development, weran from it. We pivoted to AI-powered software — which requires productand go-to-market skills we didn't have. We traded our strongest card fora hand we couldn't play.</p><p><strong>I underestimated market size.</strong> Everyone talks aboutTAM in pitch decks. I nodded along like I understood. I didn't — notuntil I felt it. Our last pivot landed us in Korea's aesthetic medicinemarket — where a single procedure costs thousands of dollars, theindustry is growing fast, and Korea is genuinely world-class at it. Thedifference between a $10 market and a $10,000 market isn't just revenue— it's how much room you have to be wrong.</p><h2 id="now-its-saas">Now it's SaaS</h2><p>I'm watching the same pattern play out again — this time with B2BSaaS.</p><p>As LLMs got more capable, the next question became obvious: who wouldown the connection layer between the model and everything else? OpenAImoved first with GPTs — a closed ecosystem. Anthropic took the oppositeapproach — open the protocol, let others build on it. That's how MCP wasborn. MCP won. Even OpenAI adopted it when they launched <ahref="https://openai.com/index/introducing-apps-in-chatgpt/">Apps inChatGPT</a>.</p><p>The value is migrating from the UI to the chat. I stopped openingSlack, Jira, and Notion in separate tabs — <ahref="https://claude.com/code">Claude Code</a> talks to all of themthrough <a href="https://claude.com/connectors">MCP connectors</a>. <ahref="https://claude.com/product/cowork">Claude Cowork</a> does the samefor non-developers. The LLM is becoming the interface foreverything.</p><p>Wall Street traders are calling it the"SaaSpocalypse"<sup id="fnref:1"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="https://finance.yahoo.com/news/traders-dump-software-stocks-ai-115502147.html">[1]</span></a></sup>. SaaS companies are building their own AI agents,but they're competing against LLM plus MCP — one native interface foreverything.</p><p>When AI is the user, a beautiful dashboard doesn't matter.</p><p>Per-seat subscriptions stop making sense. The companies that surviveare the ones sitting on years of accumulated data — the rest becomecommoditized tools behind an LLM. The models aren't fully autonomousyet<sup id="fnref:2"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="https://www.anthropic.com/research/measuring-agent-autonomy">[2]</span></a></sup> — but as they get closer, this onlyaccelerates.</p><h2 id="b2c-is-next">B2C is next</h2><p>B2C is where I am now — and it's not there yet. OpenAI built an <ahref="https://github.com/openai/apps-sdk-ui">Apps SDK</a> to letdevelopers render rich UI inside ChatGPT. <ahref="https://mcpui.dev/">MCP-UI</a> is taking the open protocolapproach — GPTs versus MCP, all over again. But neither has gained realtraction.</p><p>The problem is deeper than interface. A recommendation withoutrejected alternatives doesn't feel like a decision — it feels like aguess. People choosing a clinic or a hotel need to browse, filter, andcompare. They build confidence by ruling out the wrong options, not justfinding the right one. Chat alone can't replace that.</p><p>The moat keeps shrinking. I don't know what the B2C version of thislooks like yet. But if the pattern holds, the last ones standing won'tbe the ones with the best interface or the smartest model. They'll bethe ones holding data that no LLM can generate on its own.</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;">https://finance.yahoo.com/news/traders-dump-software-stocks-ai-115502147.html<a href="#fnref:1" rev="footnote">↩︎</a></span></li><li id="fn:2"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;">https://www.anthropic.com/research/measuring-agent-autonomy<a href="#fnref:2" rev="footnote">↩︎</a></span></li></ol></div></div>]]></content>
    
    
    <summary type="html">&lt;p&gt;In mid-2022, I typed a prompt into platform.openai.com and got back
something that didn&#39;t feel like a demo.&lt;/p&gt;
&lt;p&gt;It was text-davinci-002. Before ChatGPT, before the hype. But it
wasn&#39;t the output that mattered. It was the feeling: this is going to
break everything we know about building AI products.&lt;/p&gt;
&lt;p&gt;Building a good AI model meant collecting data, training, and
fine-tuning for specific tasks. Hard, slow, expensive work. Davinci-002
could do a rough version of most of it with just a prompt — plain
English, no data, no training, no cost.&lt;/p&gt;
&lt;p&gt;My co-founders and I were already building softly, an AI startup. A
few months later, ChatGPT launched and everyone else felt it too.</summary>
    
    
    
    <category term="Startup" scheme="https://www.thespacemoon.com/categories/Startup/"/>
    
    
    <category term="startup" scheme="https://www.thespacemoon.com/tags/startup/"/>
    
    <category term="AI" scheme="https://www.thespacemoon.com/tags/AI/"/>
    
    <category term="LLM" scheme="https://www.thespacemoon.com/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Joining HealingPaper</title>
    <link href="https://www.thespacemoon.com/2025/07/20/joining-healingpaper/"/>
    <id>https://www.thespacemoon.com/2025/07/20/joining-healingpaper/</id>
    <published>2025-07-20T00:00:00.000Z</published>
    <updated>2025-07-20T00:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Our team has joined <ahref="https://www.healingpaper.com/">HealingPaper</a> — known as강남언니 (UNNI) in Korea. I'm leading AI product. Here's what we learnedgetting here. <span id="more"></span></p><h2 id="what-we-found">What we found</h2><p>At softly, we spent over three years building AI products — fivepivots in total. The last one landed us in Korea's aesthetic medicinemarket, where we built an AI agent to automate clinic consultations.</p><p>The agent worked — to a point. Fully automated customer support lookslike the easiest AI application — because it's easy for humans.Paradoxically, that makes it one of the hardest for AI. The knowledgethat clinic staff use to handle patients isn't written down anywhere. Itlives in their heads — how to read anxiety, how to nudge withoutpushing, how to read between the lines of a vague question.</p><p>To get better, the AI needed data. To get data, we needed cliniccontracts. And each contract took months. We were running out of time toreach the next step.</p><p>While we were struggling to get there, we witnessed something else.Patients couldn't find the right clinic. Sponsored influencer reviews,unverified content, and marketing budgets drowned out actual quality.The best clinics didn't win. The best marketers did.</p><h2 id="why-healingpaper">Why HealingPaper</h2><p>Two things happened at the same time. We were running out of moves,and the data we needed to build a defensible product was out of reach.And we got connected with a team sitting on exactly what we lacked.</p><p>HealingPaper had spent years building a platform with deep data onprocedures, pricing, and outcomes — scaling across Korea, Japan,Thailand, and China. They were also building CRM software for clinics —the same space we'd been struggling in.</p><p>We needed a platform with real data. They needed AI to acceleratewhat they'd been building for years. That's why we joined.</p><p>I'll be working on making it easy for anyone with an aestheticconcern to find the right doctor and clinic. Simple to say, hard tobuild.</p><h2 id="thank-you">Thank you</h2><p>Building softly was the hardest thing I've done. Every pivot, everyfailed experiment, every hard conversation led us here.</p><p>Thank you to everyone who crossed paths with us — the customers whotold us what was broken before we could see it ourselves, the investorswho bet on us early, and the founders who picked up the phone whenthings were falling apart.</p><p>And to the team. Especially those who couldn't stay until the end.What you built got us here. I haven't forgotten.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Our team has joined &lt;a
href=&quot;https://www.healingpaper.com/&quot;&gt;HealingPaper&lt;/a&gt; — known as
강남언니 (UNNI) in Korea. I&#39;m leading AI product. Here&#39;s what we learned
getting here.</summary>
    
    
    
    <category term="Startup" scheme="https://www.thespacemoon.com/categories/Startup/"/>
    
    
    <category term="startup" scheme="https://www.thespacemoon.com/tags/startup/"/>
    
    <category term="AI" scheme="https://www.thespacemoon.com/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Git의 다양한 머지 전략 비교 - 우리 팀은 어떤 전략을 도입해야 할까?</title>
    <link href="https://www.thespacemoon.com/2021/07/11/git-merge-strategies/"/>
    <id>https://www.thespacemoon.com/2021/07/11/git-merge-strategies/</id>
    <published>2021-07-10T16:03:00.000Z</published>
    <updated>2021-07-10T16:03:00.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Git은 한 브랜치에서 작업한 내용을 Main 브랜치에 병합(Merge)할 수 있는다양한 방법들을 제공한다. 이러한 방법들을 <strong>Merge전략</strong>이라고 부르는데, 다양한 방법들 중에서도 이번 글에서는 가장많이 사용되는 방법인 1) Merge Commit, 2) Squash and Merge, 3) Rebase andMerge에 대해 소개하려고 한다.</p><span id="more"></span><p><img src="/assets/images/git-merge-strategy-base.png?style=centerme" alt="현재 브랜치와 commit 상태" width=53%></p><p>위의 그림과 같은 상태의 commit이 생성되었다고 가정하자.<code>feat/multiply</code>라는 브랜치가 있고,<code>feat/sum</code>이라는 브랜치가 있다. 각 commit 내의 숫자는commit의 global 순서를 나타낸다.</p><h2 id="merge-commit">Merge Commit</h2><p><img src="/assets/images/git-merge.png?style=centerme" alt="(좌) Merge Commit의 개념도 (우) Merge Commit 이후 Github에서의 commit log" width=92%></p><p>브랜치의 commit log와 merge log가 동시에 기록된다. Commit log는commit을 행한 순서대로 기록되고, merge log는 merge가 된 순서대로기록된다. 동시에 기록되기 때문에 commit log가 verbose해지며 commit log의순서가 merge 순서와 다르기 때문에 history 관리 및 이해가 어렵다.</p><h2 id="squash-and-merge">Squash and Merge</h2><p><img src="/assets/images/git-squash-merge.png?style=centerme" alt="(좌) Squash and Merge의 개념도 (우) Squash and Merge 이후 Github에서의 commit log" width=82%></p><p>Merge된 순서대로 master/main 브랜치에 기록된다. 그리고 작업 완료된브랜치의 commit은 새로운 commit 으로 모두 squash되며, 새로운 commit의제목은 PR 제목이 되고, 합쳐진 commit의 제목은 새로운 commit의 상세내용이 된다.</p><p>이러한 특징 때문에 master/main 브랜치의 히스토리 관리가 쉬우나,atomic commit level로 rollback 하는 것은 불가능하다.</p><h2 id="rebase-and-merge">Rebase and Merge</h2><p><img src="/assets/images/git-rebase-merge.png?style=centerme" alt="(좌) Rebase and Merge 이후 main 브랜치의 변화 (우) Rebase and Merge 이후 Github에서의 commit log" width=60%></p><p>Commit 순서가 아닌 merge 순서대로 기록된다. 그래서 하나의 PR에 담긴commit message가 다른 PR의 commit message와 섞이지 않는다. 그리고 rebase덕분에 merge된 이후의 로그를 보았을 때 하나의 브랜치에서 연속적으로작업한 것과 같은 로그를 확인할 수 있다. 이 때문에 얼마든지 항상 원하는수준으로 rollback 이 가능하다.</p><p>하지만 잘 적용하기 위해서는 commit을 생성할 때부터 올바른 commit단위로 분리해야 하며, commit message 또한 설명력을 가지고 있어야 한다.그리고 다른 PR이 먼저 merge되는 경우, rebase 작업이 필요할 수 있고 이 때발생할 수 있는 conflict를 잘 해결할 수 있어야 한다.</p><h2 id="summary">Summary</h2><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>Merge Strategy</th><th>Pros</th><th>Cons</th></tr></thead><tbody><tr class="odd"><td><strong>Merge Commit</strong></td><td>아직 찾지 못함</td><td>불필요한 commit message가 생기고 merge 순서와 commit 순서가 별도로기록되어 history 관리가 어려움</td></tr><tr class="even"><td><strong>Squash and Merge</strong></td><td>Commit 단위 별로 꼼꼼하게 관리하지 않아도 PR title 만 제대로관리하면 history가 깔끔하게 정리됨</td><td>Atomic level의 rollback이 어려움</td></tr><tr class="odd"><td><strong>Rebase and Merge</strong></td><td>Atomic level의 rollback이 용이하며 commit 단위의 history 기록이됨</td><td>Commit을 잘 다루지 못하는 경우, rebase에 익숙하지 않은 경우 어려움이발생</td></tr></tbody></table><p>앞서 소개한 전략들의 장/단점을 정리하면 위와 같다. 개인적으로는 아직<strong>Merge Commit</strong>의 장점을 발견하지 못했다. 결론적으로 팀원전체가 git을 다루는데에 굉장히 익숙해서 commit 단위, commit message,rebase 등에 어려움이 없는 경우, 혹은 atomic level의 rollback이 필요한개발 상황에서 <strong>Rebase and Merge</strong>가 선호된다. 하지만atomic level 까지의 rollback은 필요하지 않고 팀이 이제 막 git으로버전관리하는 법을 배우기 시작했다면 <strong>Squash and Merge</strong>가좋을 것이다.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Git은 한 브랜치에서 작업한 내용을 Main 브랜치에 병합(Merge)할 수 있는
다양한 방법들을 제공한다. 이러한 방법들을 &lt;strong&gt;Merge
전략&lt;/strong&gt;이라고 부르는데, 다양한 방법들 중에서도 이번 글에서는 가장
많이 사용되는 방법인 1) Merge Commit, 2) Squash and Merge, 3) Rebase and
Merge에 대해 소개하려고 한다.&lt;/p&gt;</summary>
    
    
    
    <category term="Tech" scheme="https://www.thespacemoon.com/categories/Tech/"/>
    
    <category term="Engineering" scheme="https://www.thespacemoon.com/categories/Tech/Engineering/"/>
    
    
    <category term="git" scheme="https://www.thespacemoon.com/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch의 view, transpose, reshape 함수의 차이점 이해하기</title>
    <link href="https://www.thespacemoon.com/2021/03/03/pytorch-view-reshape-transpose/"/>
    <id>https://www.thespacemoon.com/2021/03/03/pytorch-view-reshape-transpose/</id>
    <published>2021-03-02T19:22:00.000Z</published>
    <updated>2021-03-02T19:22:00.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>최근에 pytorch로 간단한 모듈을 재구현하다가 loss와 dev score가 원래구현된 결과와 달라서 의아해하던 찰나, tensor 차원을 변경하는 과정에서의도하지 않은 방향으로 구현된 것을 확인하게 되었다. 그리고 그 이유는<code>transpose</code> 와 <code>view</code> 의 기능을 헷갈려했기때문이었다. 두 함수의 차이는 <code>contiguous</code> 를 이해해야 알 수있는 내용이었고, 혹시 이 개념이 헷갈리는 사람들을 위해 간단한 예시를바탕으로 정리해보았다.</p><span id="more"></span><p><code>contiguous</code> 란 matrix 의 눈에 보이는 (advertised)순차적인 shape information 과 실제로 matrix 의 각 데이터가 저장된 위치가같은지의 여부이다. 아래의 예시에서 <code>t</code> 는<code>contiguous</code> 하다. 왜냐하면 <code>t[0][0][0]</code> →<code>t[0][0][1]</code> → <code>t[0][1][0]</code> ... 의 데이터 포인터위치 (<code>0</code> → <code>1</code> → <code>2</code> ... ) 또한연속적이기 때문이다. 아직 이해가 되지 않아도 괜찮다. 예시를 좀 더보자!</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = torch.tensor([[[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>]], \</span><br><span class="line">                 [[<span class="number">6</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>]], \</span><br><span class="line">                 [[<span class="number">12</span>, <span class="number">13</span>], [<span class="number">14</span>, <span class="number">15</span>], [<span class="number">16</span>, <span class="number">17</span>]], \</span><br><span class="line">                 [[<span class="number">18</span>, <span class="number">19</span>], [<span class="number">20</span>, <span class="number">21</span>], [<span class="number">22</span>, <span class="number">23</span>]]])  <span class="comment"># (4, 3, 2)</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><figcaption><span>print(t) >folded</span></figcaption><table><tr><td class="code"><pre><span class="line">tensor([[[ 0,  1],</span><br><span class="line">         [ 2,  3],</span><br><span class="line">         [ 4,  5]],</span><br><span class="line"></span><br><span class="line">        [[ 6,  7],</span><br><span class="line">         [ 8,  9],</span><br><span class="line">         [10, 11]],</span><br><span class="line"></span><br><span class="line">        [[12, 13],</span><br><span class="line">         [14, 15],</span><br><span class="line">         [16, 17]],</span><br><span class="line"></span><br><span class="line">        [[18, 19],</span><br><span class="line">         [20, 21],</span><br><span class="line">         [22, 23]]])</span><br></pre></td></tr></table></figure><h2 id="view">view</h2><p><code>view</code> 를 통해 <code>t</code> 라는 tensor의 shape를변경시켜보았다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tv = t.view(<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><figcaption><span>print(tv) >folded</span></figcaption><table><tr><td class="code"><pre><span class="line">tensor([[[ 0,  1,  2],</span><br><span class="line">         [ 3,  4,  5]],</span><br><span class="line"></span><br><span class="line">        [[ 6,  7,  8],</span><br><span class="line">         [ 9, 10, 11]],</span><br><span class="line"></span><br><span class="line">        [[12, 13, 14],</span><br><span class="line">         [15, 16, 17]],</span><br><span class="line"></span><br><span class="line">        [[18, 19, 20],</span><br><span class="line">         [21, 22, 23]]])</span><br></pre></td></tr></table></figure><p>shape이 <code>(4, 2, 3)</code> 으로 잘 바뀐 것을 확인할 수 있다.그리고 <code>tv[0][0][0]</code> → <code>tv[0][0][1]</code> →<code>tv[0][0][2]</code> ... 의 데이터 포인터 위치 (<code>0</code> →<code>1</code> → <code>2</code> ... ) 또한 연속적이기 때문에<code>tv</code> 는 <code>contiguous</code> 하다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tv.is_contiguous()</span><br><span class="line">---</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><p>데이터의 tensor index 순서대로 tensor를 flatten 시켜주는 함수를 통해<code>t</code> 와 <code>tv</code> 를 비교하면 동일하게 나오는 것을확인할 수 있다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t.flatten() == tv.flatten()</span><br><span class="line">---</span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>,</span><br><span class="line">        <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure><p>또한 <code>t</code> 와 <code>tv</code> 의 데이터는 pointer 값이동일하여 한 쪽의 tensor data 를 수정하면 다른 쪽의 값도 동시에변경된다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t.storage().data_ptr() == tv.storage().data_ptr()  <span class="comment"># data pointer 값이 일치함</span></span><br><span class="line">---</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Modifying view tensor changes base tensor as well.</span></span><br><span class="line">t[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>] = <span class="number">99</span></span><br><span class="line">tv[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">---</span><br><span class="line">tensor(<span class="number">99</span>)</span><br></pre></td></tr></table></figure><h2 id="transpose">transpose</h2><p><code>transpose</code> 를 통해 <code>t</code> 라는 텐서의 shape을변경시켜보았다. shape은 <code>tv</code>와 동일하나, 구성이 조금 다른것을 확인할 수 있다. 참고로, 보통<code>(batch_size, hidden_dim, input_dim)</code> 을<code>(batch_size, input_dim, hidden_dim)</code> 으로 바꿔주는 작업을 할때에 <code>transpose</code> 를 사용한다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt = t.transpose(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># (4, 2, 3)</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><figcaption><span>print(tt) >folded</span></figcaption><table><tr><td class="code"><pre><span class="line">tensor([[[ 0,  2,  4],</span><br><span class="line">         [ 1,  3,  5]],</span><br><span class="line"></span><br><span class="line">        [[ 6,  8, 10],</span><br><span class="line">         [ 7,  9, 11]],</span><br><span class="line"></span><br><span class="line">        [[12, 14, 16],</span><br><span class="line">         [13, 15, 17]],</span><br><span class="line"></span><br><span class="line">        [[18, 20, 22],</span><br><span class="line">         [19, 21, 23]]])</span><br></pre></td></tr></table></figure><p>앞선 예시에서 <code>t</code> 의 데이터 포인터는 <code>0</code> →<code>1</code> → <code>2</code> ... 순서대로 저장된 것을 알 수 있었다.<code>t</code>와 <code>tv</code> 모두 데이터 포인터의 물리적 순서와shape 상에서의 데이터 순서가 같았기 때문에 <code>contiguous</code> 했다.하지만 <code>tt</code> 의 경우 <code>0</code> → <code>1</code> →<code>2</code> ... ≠ <code>tt[0][0][0]</code> → <code>tt[0][0][1]</code>→ <code>tt[0][0][2]</code> ... 이기 때문에 <code>contiguous</code> 하지않다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt.is_contiguous()</span><br><span class="line">---</span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure><p><code>tt</code> 를 flatten 시키면 물리적 순서 (<code>0</code> →<code>1</code> → <code>2</code> ... ) 와 shape 상의 순서가 다른 것을확인할 수 있다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt.flatten()</span><br><span class="line">---</span><br><span class="line">tensor([ <span class="number">0</span>,  <span class="number">2</span>,  <span class="number">4</span>,  <span class="number">1</span>,  <span class="number">3</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">8</span>, <span class="number">10</span>,  <span class="number">7</span>,  <span class="number">9</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">13</span>, <span class="number">15</span>, <span class="number">17</span>,</span><br><span class="line">        <span class="number">18</span>, <span class="number">20</span>, <span class="number">22</span>, <span class="number">19</span>, <span class="number">21</span>, <span class="number">23</span>])</span><br></pre></td></tr></table></figure><h2 id="contiguous">contiguous</h2><p>그렇다면 강제로 물리적 위치를 연속적으로 만들어버리면 어떻게 될까?겉보기에는 <code>tt</code> 와 별 차이가 없어보인다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt.contiguous() == tt</span><br><span class="line">---</span><br><span class="line">tensor([[[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">         [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]]])</span><br></pre></td></tr></table></figure><p>하지만 각 데이터 포인터를 비교하면 <code>tt.contiguous()</code> 는<code>0</code> → <code>2</code> → <code>4</code> ... 이고<code>tt</code> 는 <code>0</code> → <code>1</code> → <code>2</code> 이기때문에 아래의 값이 False가 나오는 것을 예상해볼 수 있다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt.contiguous().storage().data_ptr() == tt.storage().data_ptr()</span><br><span class="line">---</span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure><h2 id="reshape">reshape</h2><p><code>contiguous</code> 개념을 이해했다면, <code>reshape</code> 과<code>view</code> 함수의 차이도 알 수 있다. 쉽게 얘기하면<code>reshape() == contiguous().view()</code> 와 같다.</p><p><code>view</code> 는 <code>contiguous</code> 하지 않은<code>tensor</code> 에 대해서는 적용할 수 없다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt.view(<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>)  <span class="comment"># tt.shape() == (4, 2, 3)</span></span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">RuntimeError                              Traceback (most recent call last)</span><br><span class="line">&lt;ipython-<span class="built_in">input</span>-<span class="number">89</span>-785954c0ff12&gt; <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">----&gt; <span class="number">1</span> tt.view(<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>)  <span class="comment"># tt.shape() == (4, 2, 3)</span></span><br><span class="line"></span><br><span class="line">RuntimeError: view size <span class="keyword">is</span> <span class="keyword">not</span> compatible <span class="keyword">with</span> <span class="built_in">input</span> tenso<span class="string">r&#x27;s size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt.contiguous().view(<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">---</span><br><span class="line">tensor([[[ <span class="number">0</span>,  <span class="number">2</span>],</span><br><span class="line">         [ <span class="number">4</span>,  <span class="number">1</span>],</span><br><span class="line">         [ <span class="number">3</span>,  <span class="number">5</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">6</span>,  <span class="number">8</span>],</span><br><span class="line">         [<span class="number">10</span>,  <span class="number">7</span>],</span><br><span class="line">         [ <span class="number">9</span>, <span class="number">11</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">12</span>, <span class="number">14</span>],</span><br><span class="line">         [<span class="number">16</span>, <span class="number">13</span>],</span><br><span class="line">         [<span class="number">15</span>, <span class="number">17</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">18</span>, <span class="number">20</span>],</span><br><span class="line">         [<span class="number">22</span>, <span class="number">19</span>],</span><br><span class="line">         [<span class="number">21</span>, <span class="number">23</span>]]])</span><br></pre></td></tr></table></figure><p>하지만 <code>reshape</code> 은 강제로 tensor를<code>contiguous</code> 하게 만들면서 shape을 변경하기 때문에가능하다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt.reshape(<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">---</span><br><span class="line">tensor([[[ <span class="number">0</span>,  <span class="number">2</span>],</span><br><span class="line">         [ <span class="number">4</span>,  <span class="number">1</span>],</span><br><span class="line">         [ <span class="number">3</span>,  <span class="number">5</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">6</span>,  <span class="number">8</span>],</span><br><span class="line">         [<span class="number">10</span>,  <span class="number">7</span>],</span><br><span class="line">         [ <span class="number">9</span>, <span class="number">11</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">12</span>, <span class="number">14</span>],</span><br><span class="line">         [<span class="number">16</span>, <span class="number">13</span>],</span><br><span class="line">         [<span class="number">15</span>, <span class="number">17</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">18</span>, <span class="number">20</span>],</span><br><span class="line">         [<span class="number">22</span>, <span class="number">19</span>],</span><br><span class="line">         [<span class="number">21</span>, <span class="number">23</span>]]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tt.reshape(<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>).is_contiguous()</span><br><span class="line">---</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><h2 id="summary">Summary</h2><ul><li><code>view</code> : tensor 에 저장된 데이터의 물리적 위치 순서와index 순서가 일치할 때 (<code>contiguous</code>) shape을 재구성한다. 이때문에 항상 <code>contiguous</code> 하다는 성질이 보유된다.</li><li><code>transpose</code> : tensor 에 저장된 데이터의 물리적 위치순서와 <strong>상관없이</strong> 수학적 의미의 transpose를 수행한다. 즉,물리적 위치와 transpose가 수행된 tensor 의 index 순서는 같다는 보장이없으므로 항상 <code>contiguous</code> 하지 않다.</li><li><code>reshape</code> : tensor 에 저장된 데이터의 물리적 위치 순서와index 순서가 일치하지 않아도 shape을 재구성한 이후에 강제로 일치시킨다.이 때문에 항상 <code>contiguous</code> 하다는 성질이 보유된다.</li></ul><h2 id="references">References</h2><ul><li><ahref="https://inmoonlight.github.io/notebooks/html/2021-03-03-PyTorch-view-transpose-reshape.html">https://inmoonlight.github.io/notebooks/html/2021-03-03-PyTorch-view-transpose-reshape.html</a></li><li><ahref="https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2">https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;최근에 pytorch로 간단한 모듈을 재구현하다가 loss와 dev score가 원래
구현된 결과와 달라서 의아해하던 찰나, tensor 차원을 변경하는 과정에서
의도하지 않은 방향으로 구현된 것을 확인하게 되었다. 그리고 그 이유는
&lt;code&gt;transpose&lt;/code&gt; 와 &lt;code&gt;view&lt;/code&gt; 의 기능을 헷갈려했기
때문이었다. 두 함수의 차이는 &lt;code&gt;contiguous&lt;/code&gt; 를 이해해야 알 수
있는 내용이었고, 혹시 이 개념이 헷갈리는 사람들을 위해 간단한 예시를
바탕으로 정리해보았다.&lt;/p&gt;</summary>
    
    
    
    <category term="Tech" scheme="https://www.thespacemoon.com/categories/Tech/"/>
    
    <category term="Engineering" scheme="https://www.thespacemoon.com/categories/Tech/Engineering/"/>
    
    
    <category term="pytorch" scheme="https://www.thespacemoon.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch의 IterableDataset을 사용해서 데이터 불러오기</title>
    <link href="https://www.thespacemoon.com/2021/02/21/pytorch-iterabledataset/"/>
    <id>https://www.thespacemoon.com/2021/02/21/pytorch-iterabledataset/</id>
    <published>2021-02-21T14:22:00.000Z</published>
    <updated>2021-02-21T14:22:00.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>PyTorch 1.2 이상부터 <code>torch.utils.data</code> 에서는 크게map-style dataset (<code>torch.utils.data.Dataset</code>) 과 iterabledataset (<code>torch.utils.data.IterableDataset</code>) 의 두 종류의데이터 클래스를 지원하고 있다. 데이터 사이즈가 클 때는<code>IterableDataset</code> 을 사용하는 것이 좋은데,<code>Dataset</code> 과는 딜리 아직 개발되어야 할 기능이 더 필요한클래스라서 사용할 때에 유의할 점이 있어 정리해보게 되었다.</p><span id="more"></span><h2 id="map-style-dataset">Map-style Dataset</h2><p>1.2 이하 버전에서 사용되던 map-style dataset은 memory에 모든 데이터를업로드할 수 있을 때 사용하는 가장 일반적인 dataset type 이다. customdataset class를 생성하고자 할 때 <code>torch.utils.data.Dataset</code>을 상속받아 <code>__len__</code> , <code>__getitem__</code> 을 구현하면된다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyMapDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[<span class="string">&#x27;text&#x27;</span>][index]</span><br></pre></td></tr></table></figure><h2 id="iterable-dataset">Iterable Dataset</h2><p>하지만 학습 데이터가 메모리에 다 올라가지 않는 경우가 발생할 수 있다.이 문제를 해결할 수 있는 다양한 방법 중에 하나로,<code>torch.utils.data.IterableDataset</code> 을 사용하는 방법이 있다.Map-style Dataset과 비슷하게<code>torch.utils.data.IterableDataset</code> 을 상속받아서 customdataset class를 생성하고, <code>__iter__</code> 를 선언하면 된다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> IterableDataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyIterableDataset</span>(<span class="title class_ inherited__">IterableDataset</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_path</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data_path = data_path</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        iter_csv = pd.read_csv(<span class="variable language_">self</span>.data_path, sep=<span class="string">&#x27;\t&#x27;</span>, iterator=<span class="literal">True</span>, chunksize=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> iter_csv:</span><br><span class="line">            line = line[<span class="string">&#x27;text&#x27;</span>].item()</span><br><span class="line">            <span class="keyword">yield</span> line</span><br></pre></td></tr></table></figure><p><code>Dataset</code>이 batch data를 생성할 때<code>map_dataset[index]</code>를 사용한다면,<code>IterableDataset</code>은 <code>next(iterable_dataset)</code> 을사용한다. 이 때문에 <code>DataLoader</code>를 통해<code>IterableDataset</code>을 불러와서 사용하게 되면<code>sampler</code> 옵션의 사용이 어렵다. 그래서 random suffling 을하고 싶다면 미리 데이터셋을 shuffling 한 이후에 불러오는 것이 좋다.</p><h2 id="going-parallel">Going Parallel</h2><p><ahref="https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset">PyTorch공식문서</a>에 따르면 <code>IterableDataset</code>을<code>num_workers &gt; 0</code>의 조건에서 사용할 때 특별히 다음을유념할 것을 제안하고 있다.</p><blockquote><p>When <code>num_workers &gt; 0</code>, each worker process will have adifferent copy of the dataset object, so it is often desired toconfigure each copy independently to avoid having duplicate datareturned from the workers. <code>get_worker_info()</code>, when calledin a worker process, returns information about the worker. It can beused in either the dataset’s <code>__iter__()</code> method or the<code>DataLoader</code> ‘s <code>worker_init_fn</code> option to modifyeach copy’s behavior.</p></blockquote><p>위의 문장을 이해하려면 <code>num_workers</code> 에 대한 이해와,<code>num_workers &gt; 0</code> 일 때 <code>IterDataset</code> 에서 어떤현상이 일어나는지 알아야한다.</p><p><img src="/assets/images/pytorch-iterabledataset-numworkers.png?style=centerme" alt="num_workers == 2 인 경우 발생하는 모습이다. 위의 두 라인은 subprocess이며, 맨 아래 라인은 main process이다. 파란색 박스는 single datapoint를 로딩하는 것을 의미하며 붉은색 박스는 로딩된 데이터를 모델에 전달하는 프로세스를 의미한다. (image credit: https://medium.com/speechmatics/how-to-build-a-streaming-dataloader-with-pytorch-a66dd891d9dd)" width=90%></p><p><code>num_workers</code>는 데이터셋을 불러올 때 사용할 subprocess의개수이다. <code>num_workers == 0</code> 은 main process에서 데이터를불러오고 모델에 pass하는 작업을 모두 수행한다는 뜻이며,<code>num_workers == 2</code>는 subprocess 2개에서 데이터를 불러오고main process에서는 subprocess에서 불러온 데이터를 model에 pass하는역할만 담당한다. 따라서 <code>num_workers &gt; 0</code> 일 때 dataloading에서의 병목이 줄어들며 gpu utilization 을 100% 가까이 끌어올릴 수있다.</p><p>그럼, <code>num_workers &gt; 0</code> 일 때 어떤 현상이 발생하는지살펴보자.</p><h3 id="map-style-dataset-1">Map-Style Dataset</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset, IterableDataset</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyMapDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        worker = torch.utils.data.get_worker_info()</span><br><span class="line">        worker_id = worker.<span class="built_in">id</span> <span class="keyword">if</span> worker <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        start = time.time()</span><br><span class="line">        time.sleep(<span class="number">0.1</span>)</span><br><span class="line">        end = time.time()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[index], worker_id, start, end</span><br><span class="line"></span><br><span class="line">data = <span class="built_in">range</span>(<span class="number">16</span>)</span><br><span class="line">map_dataset = MyMapDataset(data)</span><br></pre></td></tr></table></figure><ul><li><code>num_workers == 0</code> 인 경우</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loader = DataLoader(map_dataset, batch_size=<span class="number">4</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> loader:</span><br><span class="line">    batch, worker_ids, starts, ends = d</span><br><span class="line">    <span class="built_in">print</span>(batch, worker_ids)</span><br><span class="line"></span><br><span class="line">-----</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>])</span><br></pre></td></tr></table></figure><ul><li><code>num_workers == 2</code> 인 경우</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loader = DataLoader(map_dataset, batch_size=<span class="number">4</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> loader:</span><br><span class="line">    batch, worker_ids, starts, ends = d</span><br><span class="line">    <span class="built_in">print</span>(batch, worker_ids)</span><br><span class="line"></span><br><span class="line">-----</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>의도한대로, subprocess 별로 서로 다른 데이터를 불러오는 것을 알 수있다.</p><h3 id="iterable-dataset-1">Iterable Dataset</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset, IterableDataset</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyIterableDataset</span>(<span class="title class_ inherited__">IterableDataset</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = data</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="variable language_">self</span>.data:</span><br><span class="line">            worker = torch.utils.data.get_worker_info()</span><br><span class="line">            worker_id = worker.<span class="built_in">id</span> <span class="keyword">if</span> worker <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">            start = time.time()</span><br><span class="line">            time.sleep(<span class="number">0.1</span>)</span><br><span class="line">            end = time.time()</span><br><span class="line">        </span><br><span class="line">            <span class="keyword">yield</span> x, worker_id, start, end</span><br><span class="line"></span><br><span class="line">data = <span class="built_in">range</span>(<span class="number">16</span>)</span><br><span class="line">iterable_dataset = MyIterableDataset(data)</span><br></pre></td></tr></table></figure><ul><li><code>num_workers == 0</code></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loader = DataLoader(iterable_dataset, batch_size=<span class="number">4</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> loader:</span><br><span class="line">    batch, worker_ids, starts, ends = d</span><br><span class="line">    <span class="built_in">print</span>(batch, worker_ids)</span><br><span class="line"></span><br><span class="line">-----</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>])</span><br></pre></td></tr></table></figure><ul><li><code>num_workers == 2</code></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loader = DataLoader(iterable_dataset, batch_size=<span class="number">4</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> loader:</span><br><span class="line">    batch, worker_ids, starts, ends = d</span><br><span class="line">    <span class="built_in">print</span>(batch, worker_ids)</span><br><span class="line"></span><br><span class="line">----</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>⚠️ worker 0과 worker 1에서 같은 데이터를 로딩하고 있다. 이 점이공식문서에서 지적하고 있는 내용이다. 각 워커별로 같은<code>__iter__()</code>를 사용하기 때문에 같은 데이터를 로딩하게 된다.<strong>이를 방지하기 위해서는 <code>worker_init_fn</code> 에서 직접워커 별 데이터를 재분배 시켜줘야 한다.</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">worker_init_fn</span>(<span class="params">_</span>):</span><br><span class="line">    worker_info = torch.utils.data.get_worker_info()</span><br><span class="line">    </span><br><span class="line">    dataset = worker_info.dataset</span><br><span class="line">    worker_id = worker_info.<span class="built_in">id</span></span><br><span class="line">    split_size = <span class="built_in">len</span>(dataset.data) // worker_info.num_workers</span><br><span class="line">    </span><br><span class="line">    dataset.data = dataset.data[worker_id * split_size: (worker_id + <span class="number">1</span>) * split_size]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loader = DataLoader(iterable_dataset, batch_size=<span class="number">4</span>, num_workers=<span class="number">2</span>, worker_init_fn=worker_init_fn)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> loader:</span><br><span class="line">    batch, worker_ids, starts, ends = d</span><br><span class="line">    <span class="built_in">print</span>(batch, worker_ids)</span><br><span class="line"></span><br><span class="line">-----</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]) tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">tensor([<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]) tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p><code>worker_init_fn</code> 을 통해 분배시켜준 결과 worker 0과 worker1 에서 다른 데이터를 순차적으로 불러오는 것을 알 수 있다 🙂</p><h2 id="conclusions">Conclusions</h2><ul><li><code>IterableDataset</code> 은 데이터가 메모리에 올라가지 않을만큼클 때 사용하면 좋다.</li><li><code>Dataset</code>과 다르게 <code>__iter__()</code>를 선언해서데이터를 부른다.<ul><li>하지만 이 특징 때문에 <code>Sampler</code> 와 함께 사용할 수없다.</li><li>또한 <code>num_workers &gt; 0</code> 인 세팅에서는 각 워커에서 다른데이터를 불러올 수 있도록 <code>worker_init_fn</code>을 선언해야한다.</li></ul></li></ul><h2 id="references">References</h2><ul><li><ahref="https://medium.com/speechmatics/how-to-build-a-streaming-dataloader-with-pytorch-a66dd891d9dd">Howto Build a Streaming DataLoader with PyTorch | by David MacLeod |Speechmatics | Medium</a></li><li><ahref="https://inmoonlight.github.io/notebooks/html/2021-02-21-PyTorch-Dataset.html">https://inmoonlight.github.io/notebooks/html/2021-02-21-PyTorch-Dataset.html</a></li><li><ahref="https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset">torch.utils.data— PyTorch 1.7.1 documentation</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;PyTorch 1.2 이상부터 &lt;code&gt;torch.utils.data&lt;/code&gt; 에서는 크게
map-style dataset (&lt;code&gt;torch.utils.data.Dataset&lt;/code&gt;) 과 iterable
dataset (&lt;code&gt;torch.utils.data.IterableDataset&lt;/code&gt;) 의 두 종류의
데이터 클래스를 지원하고 있다. 데이터 사이즈가 클 때는
&lt;code&gt;IterableDataset&lt;/code&gt; 을 사용하는 것이 좋은데,
&lt;code&gt;Dataset&lt;/code&gt; 과는 딜리 아직 개발되어야 할 기능이 더 필요한
클래스라서 사용할 때에 유의할 점이 있어 정리해보게 되었다.&lt;/p&gt;</summary>
    
    
    
    <category term="Tech" scheme="https://www.thespacemoon.com/categories/Tech/"/>
    
    <category term="Engineering" scheme="https://www.thespacemoon.com/categories/Tech/Engineering/"/>
    
    
    <category term="pytorch" scheme="https://www.thespacemoon.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Pandas Dataframe의 다양한 iteration 방법 비교</title>
    <link href="https://www.thespacemoon.com/2021/02/04/pandas-dataframe-iteration-methods/"/>
    <id>https://www.thespacemoon.com/2021/02/04/pandas-dataframe-iteration-methods/</id>
    <published>2021-02-04T05:22:00.000Z</published>
    <updated>2021-02-04T05:22:00.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><code>pandas</code>는 데이터를 다루는 사람들이라면 누구나 쓸 수 밖에없는 오픈소스 라이브러리이다. table 형식의 데이터를 다루기에 편리하지만오픈소스라는 특징과 다양한 기능 지원 때문에 속도 면에서는 최적화되어있지 않은 편이다. 이번 글에서는 <code>pandas</code>의 여러 기능 중에서<code>iteration</code>하는 여러 방법을 속도와 사용성 측면에서 비교해본내용을 아주 간단하게 정리해 보았다.</p><span id="more"></span><h2 id="summary">Summary</h2><table><thead><tr class="header"><th>rank</th><th>method</th><th>time</th><th><code>iterrows</code> 대비 속도</th></tr></thead><tbody><tr class="odd"><td>1</td><td><strong><code>itertuples</code></strong></td><td><strong>7.7ms</strong></td><td><strong>x8.1</strong></td></tr><tr class="even"><td>2</td><td><code>at</code> / <code>iat</code></td><td>15.8ms</td><td>x4</td></tr><tr class="odd"><td>3</td><td><code>loc</code> / <code>iloc</code></td><td>24.6ms</td><td>x2.5</td></tr><tr class="even"><td>4</td><td><code>iterrows</code></td><td>62.7ms</td><td>x1</td></tr><tr class="odd"><td></td><td></td><td></td><td></td></tr><tr class="even"><td>번외</td><td><code>values</code></td><td>7.1ms</td><td>x8.8</td></tr><tr class="odd"><td>번외</td><td><code>apply</code> + <code>to_dict</code></td><td>9.91 ms</td><td>x6.3</td></tr></tbody></table><h2 id="introduction">Introduction</h2><p>실험에 사용한 데이터는 아래와 같이 <code>id</code>,<code>text</code>, <code>title</code> 정보로 이루어진 위키피디아를처리한 table 형식의 데이터이다. <code>text</code>는 위키피디아 문서를일정 길이 단위로 잘라서 가공한 문장들이고, <code>title</code>은 해당문장이 속한 위키피디아 문서의 제목을 의미한다. <code>id</code>는 각문장들의 고유 번호이다.</p><p><img src="/assets/images/pandas-data-example.png?style=centerme" width=70%><br></p><p>데이터의 row 별로 iteration을 하면서 처리할 내용은 1) 아래의<code>cut_text</code>를 통해 <code>text</code>의 길이를 줄이고, 2) table의 내용을 <code>list_of_dict</code> 형식으로 변환하는 것이다.<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cut_text</span>(<span class="params">text, max_len: <span class="built_in">int</span> = <span class="number">100</span></span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(text.split()[:max_len])</span><br></pre></td></tr></table></figure></p><p>실험할 함수는 크게 <code>iterrows</code>,<code>loc</code>/<code>iloc</code>, <code>at</code>/<code>iat</code>,<code>itertuples</code>, 그리고 속도 면에서는 장점이 있으나 약간의단점이 있는 <code>values</code>, 그리고 이번 task 에 overfitting 된<code>apply</code> + <code>to_dict</code> 가 있다. 하나하나 살펴보도록하자!</p><h2 id="iterrows">iterrows</h2><p>많이 사용되는 함수이지만 가장 성능이 좋지 않다. <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> i, row <span class="keyword">in</span> data.iterrows():</span><br><span class="line">    short_text = cut_text(row[<span class="string">&#x27;text&#x27;</span>])</span><br><span class="line">    instance = &#123;</span><br><span class="line">        <span class="string">&#x27;id&#x27;</span>: row[<span class="string">&#x27;id&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;text&#x27;</span>: short_text,</span><br><span class="line">        <span class="string">&#x27;title&#x27;</span>: row[<span class="string">&#x27;title&#x27;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(instance)</span><br></pre></td></tr></table></figure></p><p><strong>62.7 ms</strong> ± 729 µs per loop (mean ± std. dev. of 7runs, 10 loops each)</p><h2 id="loc-iloc">loc / iloc</h2><p><code>iterrows</code> 다음으로 많이 사용되는 방식이다.<code>iterrows</code>에 비해 2.5배 정도 빠른 속도를 보인다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> data.index:</span><br><span class="line">    short_text = cut_text(data.loc[idx, <span class="string">&#x27;text&#x27;</span>])</span><br><span class="line">    instance = &#123;</span><br><span class="line">        <span class="string">&#x27;id&#x27;</span>: data.loc[idx, <span class="string">&#x27;id&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;text&#x27;</span>: short_text,</span><br><span class="line">        <span class="string">&#x27;title&#x27;</span>: data.loc[idx, <span class="string">&#x27;title&#x27;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(instance)</span><br></pre></td></tr></table></figure><p><strong>24.6 ms</strong> ± 235 µs per loop (mean ± std. dev. of 7runs, 10 loops each)</p><p>:warning: 다만, <code>loc</code>을 잘못 쓰게 되면<code>iterrows</code>를 썼을 때보다도 더 오랜 시간이 소요된다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> data.index:</span><br><span class="line">    short_text = cut_text(data.loc[idx][<span class="string">&#x27;text&#x27;</span>])  <span class="comment"># diff</span></span><br><span class="line">    instance = &#123;</span><br><span class="line">        <span class="string">&#x27;id&#x27;</span>: data.loc[idx][<span class="string">&#x27;id&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;text&#x27;</span>: short_text,</span><br><span class="line">        <span class="string">&#x27;title&#x27;</span>: data.loc[idx][<span class="string">&#x27;title&#x27;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(instance)</span><br></pre></td></tr></table></figure><p><strong>261 ms</strong> ± 1.12 ms per loop (mean ± std. dev. of 7runs, 1 loop each)</p><p>:warning: 미리 row를 받으면 조금 더 빨라지지만, 그럼에도<code>iterrows</code>대비 느리다. <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> data.index:</span><br><span class="line">    row = data.loc[idx]</span><br><span class="line">    short_text = cut_text(row[<span class="string">&#x27;text&#x27;</span>])  <span class="comment"># diff</span></span><br><span class="line">    instance = &#123;</span><br><span class="line">        <span class="string">&#x27;id&#x27;</span>: row[<span class="string">&#x27;id&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;text&#x27;</span>: short_text,</span><br><span class="line">        <span class="string">&#x27;title&#x27;</span>: row[<span class="string">&#x27;title&#x27;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(instance)</span><br></pre></td></tr></table></figure> <strong>99.4 ms</strong>± 904 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)</p><h2 id="at-iat">at / iat</h2><p><code>loc</code> / <code>iloc</code> 과 유사하지만, 특정 column과row에 해당하는 값을 받고 싶을 때 사용한다. <code>at</code> 함수에 대한상세한 설명은 <ahref="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.at.html">pandas공식 문서</a>에서 확인할 수 있다. <code>iterrows</code>에 비해 4배 정도빠른 속도를 보인다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> data.index:</span><br><span class="line">    short_text = cut_text(data.at[idx, <span class="string">&#x27;text&#x27;</span>])</span><br><span class="line">    instance = &#123;</span><br><span class="line">        <span class="string">&#x27;id&#x27;</span>: data.at[idx, <span class="string">&#x27;id&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;text&#x27;</span>: short_text,</span><br><span class="line">        <span class="string">&#x27;title&#x27;</span>: data.at[idx, <span class="string">&#x27;title&#x27;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(instance)</span><br></pre></td></tr></table></figure><p><strong>15.8 ms</strong> ± 49.6 µs per loop (mean ± std. dev. of 7runs, 100 loops each)</p><h2 id="itertuples">itertuples</h2><p><code>iterrows</code>와 유사하지만, Series가 return되는<code>iterrows</code>와는 다르게 NamedTuple이 return 된다. column에대응되는 값에 접근하기도 쉽고, 속도도 8배 이상 빠르다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> data.itertuples():</span><br><span class="line">    short_text = cut_text(row.text)</span><br><span class="line">    instance = &#123;</span><br><span class="line">        <span class="string">&#x27;id&#x27;</span>: row.<span class="built_in">id</span>,</span><br><span class="line">        <span class="string">&#x27;text&#x27;</span>: short_text,</span><br><span class="line">        <span class="string">&#x27;title&#x27;</span>: row.title</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(instance)</span><br></pre></td></tr></table></figure><p><strong>7.7 ms</strong> ± 21.9 µs per loop (mean ± std. dev. of 7runs, 100 loops each)</p><h2 id="values">values</h2><p>여기서부터는 번외 느낌인데, values는 속도가 가장 빠르다는 장점이있지만 column에 대응되는 값을 불러올 때 불편한 점이 있다. 이 점을감안해서 써도 무관하다면 가장 좋은 선택이 될 것 같다. <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">result = []</span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> data.values:</span><br><span class="line">    short_text = cut_text(value[<span class="number">1</span>])</span><br><span class="line">    instance = &#123;</span><br><span class="line">        <span class="string">&#x27;id&#x27;</span>: value[<span class="number">0</span>],</span><br><span class="line">        <span class="string">&#x27;text&#x27;</span>: short_text,</span><br><span class="line">        <span class="string">&#x27;title&#x27;</span>: value[<span class="number">2</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    result.append(instance)</span><br></pre></td></tr></table></figure></p><p><strong>7.1 ms</strong> ± 43.8 µs per loop (mean ± std. dev. of 7runs, 100 loops each)</p><h2 id="apply-to_dict">apply + to_dict</h2><p><code>for</code> 문 안에서 처리할 내용이 복잡하지 않은 이번태스크같은 경우에 쓰기 적합한 방식이다. 새로운 dataframe 혹은 새로운column을 생성해야 해서 메모리 측면에서 오는 단점은 있지만, 코드가 짧고깔끔하다는 장점이 있다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">result = data.copy()</span><br><span class="line">result[<span class="string">&#x27;text&#x27;</span>] = result[<span class="string">&#x27;text&#x27;</span>].apply(<span class="keyword">lambda</span> x: cut_text(x))</span><br><span class="line">result = result.to_dict(orient=<span class="string">&#x27;records&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>9.91 ms</strong> ± 19.6 µs per loop (mean ± std. dev. of 7runs, 100 loops each)</p><h2 id="references">References</h2><ul><li><ahref="https://inmoonlight.github.io/notebooks/html/2021-02-04-pandas-dataframe-iterations.html">https://inmoonlight.github.io/notebooks/html/2021-02-04-pandas-dataframe-iterations.html</a></li><li><ahref="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.at.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.at.html</a></li><li><ahref="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.itertuples.html">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.itertuples.html</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;code&gt;pandas&lt;/code&gt;는 데이터를 다루는 사람들이라면 누구나 쓸 수 밖에
없는 오픈소스 라이브러리이다. table 형식의 데이터를 다루기에 편리하지만
오픈소스라는 특징과 다양한 기능 지원 때문에 속도 면에서는 최적화되어
있지 않은 편이다. 이번 글에서는 &lt;code&gt;pandas&lt;/code&gt;의 여러 기능 중에서
&lt;code&gt;iteration&lt;/code&gt;하는 여러 방법을 속도와 사용성 측면에서 비교해본
내용을 아주 간단하게 정리해 보았다.&lt;/p&gt;</summary>
    
    
    
    <category term="Tech" scheme="https://www.thespacemoon.com/categories/Tech/"/>
    
    <category term="Engineering" scheme="https://www.thespacemoon.com/categories/Tech/Engineering/"/>
    
    
    <category term="pandas" scheme="https://www.thespacemoon.com/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>한국어 악성댓글 탐지를 위한 댓글 코퍼스 구축기</title>
    <link href="https://www.thespacemoon.com/2020/05/28/korean-hate-speech-dataset/"/>
    <id>https://www.thespacemoon.com/2020/05/28/korean-hate-speech-dataset/</id>
    <published>2020-05-28T12:00:00.000Z</published>
    <updated>2020-05-28T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>약 4-5개월동안 사이드로 진행했던 혐오 댓글프로젝트<sup id="fnref:1"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://github.com/kocohub/korean-hate-speech&gt;">[1]</span></a></sup>가 성공적으로 마무리되었다. 같은 문제의식을 가진사람들과 시작해서 각자 하고싶었던 내용을 조율하고, 혐오 댓글이무엇인가에 대해 깊게 고민해보는 과정들이 쉽진 않았지만 의미있는활동이라는 생각이 들었다. 또한, 사이드로 진행된 프로젝트임에도 불구하고원동력이 사라지지 않고 꾸준히 일이 진행되었던 것은 모두 구성원들의상호보완적인 역량 덕분이 아니었을까 싶다.</p><p>사실 이 글을 쓰게 된 계기는 논문에는 쓰지 못했던 데이터에 대한이야기를 하고 싶어서였다. 주어진 4장에 많은 내용을 담으려다보니 정작작업하면서 고려했던 세부사항이나 어려웠던 점, 지나고나니 아쉬웠던부분들에 대해 적진 못했기 때문이다. 아마 데이터셋을 활용하려고 생각하는사람들에게도 좋은 팁이 되지 않을까? <span id="more"></span></p><hr /><h2 id="어노테이션">어노테이션</h2><h3 id="왜-편견과-혐오인가">왜 <code>편견</code>과<code>혐오</code>인가?</h3><p><ahref="https://www.notion.so/c1ecb7cc52d446cc93d928d172ef8442">어노테이션가이드라인</a>에도 나와있듯이 우리는 크게 편견과 혐오라는 두 가지aspect에 대해서 label을 수집했다. 처음에는<code>성에 관련된 편견 및 혐오</code>와<code>그 외의 편견 및 혐오</code>로 나누었는데, 이보다는<code>편견</code>과 <code>혐오</code>로 구분하는 것이 낫다는 판단을했다.</p><p>가이드라인 작성을 위해 댓글을 직접 태깅하다 보니 편견만 존재하는댓글과 혐오만 존재하는 댓글이 존재했다. 항상 혐오가 편견으로부터시작되지는 않았고, 편견이 있음을 부끄러워하지 않고 세상의 진리인 것처럼이야기하는 댓글이 보였다. 혐오가 편견으로 시작된 경우는, 무리하게 개인의특성을 집단의 특성으로 확장해서 그 집단에 대한 혐오를 개인에게 표출할때였다. 그래서 이 둘의 관계를 데이터로 파악할 수 있도록, 또 편견과혐오를 구분지어 생각할 수 있도록 <code>편견</code>에 관련된 label과<code>혐오</code>에 관련된 label을 구분짓기로 했다.</p><p>언어학에 관심있는 사람들이라면 label을 바탕으로 댓글을 분석하는것으로도 재밌는 연구가 될 것 같다.</p><h3 id="표현의-자유와-혐오의-경계">표현의 자유와 혐오의 경계</h3><p>이 둘을 구분짓는 좋은 threshold를 결정하는 것은 무슨 목적으로활용하냐에 달려있다. 우리의 목적은 혐오 댓글의 피해자가 줄어들기를바라는 것이었으므로 익명인의 표현의 자유보다는 기사의 대상이 되는 사람의기분을 좀 더 신경쓰기로 했다. 그래서 태깅을 할 때에 당사자의 입장에서생각하도록 가이드했다.</p><h3 id="어노테이션이-어려웠던-댓글">어노테이션이 어려웠던 댓글</h3><blockquote><p>예전에 데뷔작에서 수영복입고 수중씬 기억난다 진짜 섹씨했는데</p></blockquote><p>연예인이라는 직업이 가지는 특수성 때문에 판단하기 어려웠던 경우이다.특히 여자연예인에 대해서는 외모에 대해 품평하는 댓글이 많았는데,스스로가 연예인이었던 적이 없으니 감정이입을 해서 이를 모욕이라고봐야할지도 모르겠고, 만약 의도적으로 외모를 부각해서 유명세를 얻은경우라면 모욕이라고 보기가 더 어렵다고 생각했다. 결국 각자의 판단에맡겨서 majority voting을 했지만 정말 어려웠던 케이스였다.</p><blockquote><p>신천지?</p></blockquote><p>"일반적으로 비난받을만한 행위로 인한 혐오는 어떻게 판단해야할까?" 를고민하게 만든 댓글이었다. 신천지 교도로 인해 코로나가 빠르게 퍼졌던 사건이후로 "신천지"는 부정적인 이미지로 굳어져 버렸는데, 이 맥락을 고려해서위의 댓글을 <code>혐오</code>라고 태깅하면 "신천지"라는 가치 중립적인단어가 <code>혐오</code>로 태깅되기에 굉장히 고민이 많았습니다.</p><blockquote><p>살빠진 마닷같애</p></blockquote><p>위와 비슷한 케이스로 이 댓글 또한 판단하기 무척이나 어려웠다 ㅠㅠ<code>offensive</code>로 판단하자니 마닷은 뭐가 되냐는...</p><h3 id="other-biases-라는-label"><code>Other biases</code> 라는label</h3><p>현재는 <code>bias</code> label이 <code>gender bias</code>,<code>other biases</code>, <code>none</code> 의 세가지로 구성되어 있다.이렇게 할 수 밖에 없었던 가장 큰 이유는 예산 문제였다 ㅠㅠ 돈이 많았다면gender 외에도 정치, 지역, 인종 등에 대한 편견도 label을 수집할 수있었을텐데 하는 아쉬움이... 인당 150만원 이상은 부담하고 싶지 않아서,그리고 연예 도메인은 성 편견이 가장 많은 비중을 차지하고 있어서 이런결정을 하게 되었다.</p><p>그러다보니 <code>others</code> 라는 label 은 온갖 종류의 편견이 모두모아져 있다. 아마도 모델이 곧장 others 를 예측하는 것은 쉽지 않을것이라고 생각한다. 이 task는, <ahref="https://arxiv.org/abs/2005.12503">논문</a>에 적혀있듯이, 2-stepclassification 문제를 푸는 방식이 낫지 않을까라고 생각한다. 먼저<code>gender / no-gender</code> 를 예측하고, 그 이후에<code>bias 유 / 무</code> 를 예측하면 <code>gender</code>,<code>others</code>, <code>none</code> 을 좀 더 쉽게 예측할 수 있을것이라고 생각한다.</p><h3 id="어노테이션-작업-시-context-미제공">어노테이션 작업 시 context미제공</h3><p>댓글에 포함된 편견 및 혐오를 더욱 정확하게 판단하기 위해서는 댓글이작성된 뉴스 기사에 대한 정보가 필요하다. 하지만 현실적인 이유들로포기했었다.</p><blockquote><p>"작업자가 기사를 읽어야 하는 번거로움을 감수할까?" <br> "태깅플랫폼에서 이 기능을 제공해줄까?" <br> "뉴스 기사의 내용에 대한 저작권은우리에게 없기 때문에 공개 데이터셋에 포함할 수 없고, 그럴거라면 태깅을컨텍스트 없이 하는게 좋지 않을까?"</p></blockquote><p>등의 질문들에 대해 명쾌한 답변을 내리지 못했고, 결국 댓글의 내용만보고 판단하는 방식을 가져갔다. 지나고나니 아쉬움이 남는 건 어쩔 수없는듯하다.</p><hr /><h2 id="testset-구성">Testset 구성</h2><p>현재 testset은 함께 작업했던 저와 <ahref="https://www.facebook.com/warnik.chow?__tn__=K-R&amp;eid=ARCNVgdXVouckswETyWV9lkDr_cQtrkWPysMCRo0j12ERGUOBQc35o_roiDziJvD-AI5QCjiPW9EQsqA&amp;fref=mentions&amp;__xts__%5B0%5D=68.ARBryv_ZUhMCc7-7G69xtrgWU5FmnqoTL_lLX8bkOVrEnrZ2TRtHphUFuujbvKft8qDDUco0ZJHr7AF9qnijRGiz1J7DMHiWCEXVK61XNr9g40o-7TImObl04dnqssnBZr1-Msp6i8aN8PFC2L9jDTvuS5DtI6w4tTkyVz4KvHHrjO-_oUPHzg6yhuxC7A8v2KWbj2wxuNs52cgI">조원익</a>,<ahref="https://www.facebook.com/Junbum.L?__tn__=K-R&amp;eid=ARAQ7IJ8TMPDz9gEkoGREztDVOhQ35RZDphJGLftwahcbbh0jQfEyAdsaWSGsuVPrtfxpv1Twpuw3vgF&amp;fref=mentions&amp;__xts__%5B0%5D=68.ARBryv_ZUhMCc7-7G69xtrgWU5FmnqoTL_lLX8bkOVrEnrZ2TRtHphUFuujbvKft8qDDUco0ZJHr7AF9qnijRGiz1J7DMHiWCEXVK61XNr9g40o-7TImObl04dnqssnBZr1-Msp6i8aN8PFC2L9jDTvuS5DtI6w4tTkyVz4KvHHrjO-_oUPHzg6yhuxC7A8v2KWbj2wxuNs52cgI">이준범</a>이직접 작업한 라벨이 달려있다. 우리의 의도와 부합하는, 가장 어노테이션이잘 되었다고 보장할 수 있는 데이터셋이라고 볼 수 있다. 하지만 지나고나니"시간 순으로 train, validation, testset을 구성했다면 어땠을까?" 하는아쉬움이 남았다.</p><p>댓글에는 많은 사회적 배경지식이 녹아져있다. 특히 인물의 이름이 가지고있는 정보가 있는데, 우리가 수집한 기간에는 승리와 정준영 등의 연예인이얽혀있던 단톡방 사건이 포함되어 있었다. 그래서 "승리"가 포함된 댓글은부정적인 맥락 속에서 판단되었다.</p><p>예를 들어 "승리가 뭘 잘못했다고 난리들인지...그냥 승리 부럽고베알꼴린 애들이 화난거로밖에 안보임ㅎ" 라는 댓글에서 "승리"를 제거하면성편견이 없는 것으로 태깅되었겠지만, "승리"가 포함되었기 때문에 성편견이존재하는 것으로 태깅된다.</p><p>Generalization을 잘 하는 모델이 진짜 잘하는 모델이라고 했을 때, 학습데이터에 "승리"가 없어도 위의 댓글에 달린 라벨을 잘 예측할 수 있는모델을 판별할 수 있게 testset을 구성했다면 더 좋았을 것 같다.</p><hr /><h2 id="kobert-tokenization">KoBERT tokenization</h2><p><img src="/assets/images/korean-hate-speech-model-result.png?style=centerme" width=50%><br></p><p>baseline으로 CharCNN, BiLSTM, BERT를 사용한 모델의 결과를논문<sup id="fnref:2"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://arxiv.org/abs/2005.12503&gt;">[2]</span></a></sup>에첨부했다. 여러 task 모두 BERT가 가장 좋은 성능을 보였다.</p><p>댓글은 맞춤법을 지키는 문장과는 거리가 멀고, 줄임말, 신조어,연예인명, 그리고 그 외의 고유명사 등이 많이 등장한다. 그래서 BERTtokenization 결과를 보면 한 글자 씩 분리되는 경우가 빈번했다.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">페지해라 가세연. 페지가 답이다 아님말고식 증거도없이 유재석 언급하노</span><br><span class="line">[&#x27;▁페&#x27;, &#x27;지&#x27;, &#x27;해&#x27;, &#x27;라&#x27;, &#x27;▁&#x27;, &#x27;가&#x27;, &#x27;세&#x27;, &#x27;연&#x27;, &#x27;.&#x27;, &#x27;▁페&#x27;, &#x27;지가&#x27;, &#x27;▁답&#x27;, &#x27;이다&#x27;, &#x27;▁아&#x27;, &#x27;님&#x27;, &#x27;말&#x27;, &#x27;고&#x27;, &#x27;식&#x27;, &#x27;▁증거&#x27;, &#x27;도&#x27;, &#x27;없이&#x27;, &#x27;▁유재석&#x27;, &#x27;▁언급&#x27;, &#x27;하&#x27;, &#x27;노&#x27;]</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">god 박준형이 이 기사를 싫어합니다.</span><br><span class="line">`[&#x27;▁&#x27;, &#x27;go&#x27;, &#x27;d&#x27;, &#x27;▁박&#x27;, &#x27;준&#x27;, &#x27;형&#x27;, &#x27;이&#x27;, &#x27;▁이&#x27;, &#x27;▁기사&#x27;, &#x27;를&#x27;, &#x27;▁싫어&#x27;, &#x27;합니다&#x27;, &#x27;.&#x27;]</span><br></pre></td></tr></table></figure><p>koBERT 학습 데이터에 자주 등장했던 연예인 이름은 원본 그대로 보존되는반면, 그렇지 못한 연예인은 이름이 쪼개진다.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">조개갖고 개ㅈㄹ하는 태콩이나 개촐싹대는조샌징들이나 ㅈㄴ웃김ㅋㅋㅋ </span><br><span class="line">[&#x27;▁O&#x27;, &#x27;O&#x27;, &#x27;O&#x27;, &#x27;▁조&#x27;, &#x27;개&#x27;, &#x27;갖&#x27;, &#x27;고&#x27;, &#x27;▁개&#x27;, &#x27;ᄌᄅ&#x27;, &#x27;하는&#x27;, &#x27;▁태&#x27;, &#x27;콩&#x27;, &#x27;이나&#x27;, &#x27;▁개&#x27;, &#x27;촐&#x27;, &#x27;싹&#x27;, &#x27;대&#x27;, &#x27;는&#x27;, &#x27;조&#x27;, &#x27;샌&#x27;, &#x27;징&#x27;, &#x27;들이&#x27;, &#x27;나&#x27;, &#x27;▁&#x27;, &#x27;ᄌᄂ&#x27;, &#x27;웃&#x27;, &#x27;김&#x27;, &#x27;ᄏ&#x27;, &#x27;ᄏ&#x27;, &#x27;ᄏ&#x27;]</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ㅅㅂ 더럽게 메갈어로 제목뽑는거 봐라</span><br><span class="line">[&#x27;▁&#x27;, &#x27;ᄉᄇ&#x27;, &#x27;▁더&#x27;, &#x27;럽&#x27;, &#x27;게&#x27;, &#x27;▁메&#x27;, &#x27;갈&#x27;, &#x27;어&#x27;, &#x27;로&#x27;, &#x27;▁제&#x27;, &#x27;목&#x27;, &#x27;뽑&#x27;, &#x27;는&#x27;, &#x27;거&#x27;, &#x27;▁봐&#x27;, &#x27;라&#x27;]</span><br></pre></td></tr></table></figure><p>"ㅈㄴ", "ㅈㄹ", "ㅅㅂ" 같은 단어가 tokenization에서는 쪼개지지않는다.</p><hr /><p>어려웠던 작업이었고, 완벽했다고는 할 수 없지만 좋은 시작점이 될 수있는 프로젝트였다고는 생각한다. 이번에 해결할 수 없었던 여러 한계점들을극복하는 다른 좋은 결과들이 많이 나올 수 있길 :)</p><p>이제 진짜 끝!</p><h2 id="references">References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://github.com/kocohub/korean-hate-speech">https://github.com/kocohub/korean-hate-speech</a><a href="#fnref:1" rev="footnote">↩︎</a></span></li><li id="fn:2"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/abs/2005.12503">https://arxiv.org/abs/2005.12503</a><a href="#fnref:2" rev="footnote">↩︎</a></span></li></ol></div></div>]]></content>
    
    
    <summary type="html">&lt;p&gt;약 4-5개월동안 사이드로 진행했던 혐오 댓글
프로젝트&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; rel=&quot;footnote&quot;&gt;&lt;span
class=&quot;hint--top hint--error hint--medium hint--rounded hint--bounce&quot;
aria-label=&quot;&amp;lt;https://github.com/kocohub/korean-hate-speech&amp;gt;
&quot;&gt;[1]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;가 성공적으로 마무리되었다. 같은 문제의식을 가진
사람들과 시작해서 각자 하고싶었던 내용을 조율하고, 혐오 댓글이
무엇인가에 대해 깊게 고민해보는 과정들이 쉽진 않았지만 의미있는
활동이라는 생각이 들었다. 또한, 사이드로 진행된 프로젝트임에도 불구하고
원동력이 사라지지 않고 꾸준히 일이 진행되었던 것은 모두 구성원들의
상호보완적인 역량 덕분이 아니었을까 싶다.&lt;/p&gt;
&lt;p&gt;사실 이 글을 쓰게 된 계기는 논문에는 쓰지 못했던 데이터에 대한
이야기를 하고 싶어서였다. 주어진 4장에 많은 내용을 담으려다보니 정작
작업하면서 고려했던 세부사항이나 어려웠던 점, 지나고나니 아쉬웠던
부분들에 대해 적진 못했기 때문이다. 아마 데이터셋을 활용하려고 생각하는
사람들에게도 좋은 팁이 되지 않을까?</summary>
    
    
    
    <category term="Tech" scheme="https://www.thespacemoon.com/categories/Tech/"/>
    
    <category term="ML" scheme="https://www.thespacemoon.com/categories/Tech/ML/"/>
    
    
    <category term="NLP" scheme="https://www.thespacemoon.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>수학으로 이해하는 양자컴퓨터의 기초</title>
    <link href="https://www.thespacemoon.com/2019/11/07/basics-of-quantum-computings-ko/"/>
    <id>https://www.thespacemoon.com/2019/11/07/basics-of-quantum-computings-ko/</id>
    <published>2019-11-06T15:07:00.000Z</published>
    <updated>2019-11-06T15:07:00.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>최근에 있었던 구글의 양자 우월성 (Quantum Supremacy) 달성은전세계적으로 큰 화제였다. 물 들어올 때 노 저으라고 하지 않았던가, 이때가 아니면 또 언제 양자컴퓨터에 대해 공부할까 싶어서 텍스트와 동영상을넘나들며 관련 지식을 습득해보았다. <span id="more"></span></p><p>아마 나와 같이 관련 기사나 여러 블로그 글, 유튜브 등을 찾아본사람들이라면 어렵지 않게 아래의 정보는 얻었을 것이다.</p><ul><li>n개의 qbit은 bit와 달리 \(2^n\)의 state를 표현할 수 있다.</li><li>superposition이란 동시에 0과 1의 상태를 띠는 성질로, 병렬연산이가능해져서 고전컴퓨터에 비해 계산 속도의 이점이 생긴다.</li></ul><p>텍스트만 보면 "아 그렇구나." 싶은 내용들이다. 이해가 된 것일까싶었지만 스스로에게 세 질문을 던졌을 때 답하지 못하는 것을 보며 제대로이해하지 못했음을 인지했다. <br></p><p>    <strong>Q1.</strong> n개의 bit로도 \(2^n\)을 표현할 수 있는거아닌가? 3개의 bit가 000, 001, 010, 011, 100, 101, 110, 111 이렇게 8개의상태를 표현할 수 있으니까. <br>     <strong>Q2.</strong> 양자 세계는불확정성 원리에 지배받는다고 하는데, 대체 양자컴퓨터로 어떻게 연산하고있는 것이며, 이 성질이 어떻게 계산 비용을 감소시킬 수 있는걸까? <br>    <strong>Q3.</strong> Entanglement는 qbit들이 어떻게 된상태인거지?</p><p>이 질문들에 제대로 답하기 위해서는 수학이 필요하다는 생각이 들었다.4차원 이상의 공간을 제대로 시각화하지 못하듯이 양자 세계를 자연어로표현한다는 것 자체가 말이 되지 않는 것 같았기 때문이다. 그래서 수학으로설명된 자료를 찾으려고 부던히 애를 썼고, 끝내 "내 수준에 맞는" (= 이글을 읽을 모두가 다 이해할 수 있는) 수학으로 설명된 자료를 찾았다.</p><iframe width="560" height="315" src="https://www.youtube.com/embed/F_Riqjdh2oM?style=centerme" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p><ahref="https://speakerdeck.com/ahelwer/quantum-computing-for-computer-scientists">[slide]</a></p><p>이 영상을 보는 것을 추천하지만 무려 한시간이 넘는지라 글로도 정리를해보았다. 아래에 기술된 내용은 내 방식대로 위의 영상과 자료를 재구성한것이다.</p><p>사실 이 자료를 다 보더라도 양자컴퓨터에 대해 많은 것을 알았다고 보긴어렵다. python을 처음 접한 사람이 <code>print("Hello World!")</code>를성공했다고 해서 python을 잘 알았다고 하기 어려운 것처럼. 그리고 딥러닝에관심있는 사람이 tutorial을 따라해보며 CNN을 돌려봤다고 해서 딥러닝을 잘알았다고 하기 어려운 것처럼.</p><p><strong>그렇지만 양자 세계에 한 번은 <code>Hello World!</code>를날려봐야 하지 않을까?</strong></p><h2 id="introduction">Introduction</h2><p>The Deutsch-Jozsa problem 이라는 아주 간단한 문제를 통해 양자컴퓨터가고전컴퓨터에 비해 어떻게 연산 속도에서 이점을 보이는지 알아보려고 한다.이 과정을 이해하기 위해 양자컴퓨터가 연산하는 방법에 대해 소개할 것이며matrix 연산과 기초적인 논리회로에 대한 내용을 짚고 넘어갈 것이다.</p><p>추가로, entanglement에 대한 간단한 설명이 있다.</p><h2 id="basics">Basics</h2><h3 id="qubit-qbit">Qubit / Qbit</h3><p>Qubit 혹은 Qbit은 양자컴퓨터 계산의 기본적인 단위이다. 조금이라도양자컴퓨터에 대해 알아본 사람들이라면 qbit은 지겹도록 보고 들었을것이다.</p><p>Qbit은 언제나 다음의 조건을 만족시킨다.</p><blockquote><p>A qbit, represented by \(\begin{pmatrix} \alpha \\ \beta\end{pmatrix}\) where \(\alpha\) and \(\beta\) are complex numbers mustbe constrained by the equation \(||\alpha||^2 + ||\beta||^2 = 1\)</p></blockquote><p>따라서 아래의 예시들은 qbit에 해당된다.</p><p>\[ \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}\end{pmatrix} \hspace{10pt} \begin{pmatrix} \frac{1}{2} \\\frac{\sqrt{3}}{2} \end{pmatrix} \hspace{10pt} \begin{pmatrix} 1 \\ 0\end{pmatrix} \hspace{10pt} \begin{pmatrix} 0 \\ -1 \end{pmatrix} \]</p><p>그리고 이 모든 벡터들의 basis가 되는 \(\begin{pmatrix} 1 \\0\end{pmatrix}\)과 \(\begin{pmatrix} 0 \\ 1 \end{pmatrix}\)은 각각 \(\mid0\rangle\)과 \(\mid 1\rangle\)이라는 특별한 기호로 정의한다.</p><h3 id="superposition">Superposition</h3><p>양자컴퓨터의 qbit을 설명할 때 빠지지 않는 성질이다. "동시에 0과 1을가진다."는 문장으로 자주 설명되지만 이보다는 슈뢰딩거의 고양이 느낌이물씬 나는 "When we measure a qbit, it collapses to an actual value of 0or 1." 이라는 문장이 더 좋은 설명인 것 같다.</p><p>위에서 qbit이라고 언급했던 벡터 하나를 예시로 들어보자.</p><p>\[\begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}\end{pmatrix}\]</p><p>이 qbit은 \(0\) 혹은 \(1\)로 collapse될 확률이 \(\frac{1}{2}\) ( \(=|| \frac{1}{\sqrt{2}} || ^2\)) 이다.</p><p>감사하게도 <a href="https://quantum-computing.ibm.com/">IBM은 자사의양자컴퓨터를 사용할 수 있는 API</a>를 만들어 놓았다. 여기서 이 qbit을만들고 1024번 관측해보면 0과 1이 50%씩 나오는 것을 확인할 수 있다.</p><p><img src="/assets/images/qubit_1_2_example.png?style=centerme" width=20% alt="H gate를 통해 |0> 은 예시로 든 qbit으로 바뀐다. (이 내용은 밑에서 다룬다.)"></p><p><img src="/assets/images/qubit_1_2_result.png?style=centerme" width=95% alt="1024번 관측한 결과다. 50% 확률로 0과 1을 나타낸다."></p><p>\(\begin{pmatrix} \frac{1}{2} \\ \frac{\sqrt{3}}{2} \end{pmatrix}\)은 \(0\)으로 collapse 될 확률이 \(\frac{1}{4}\), \(1\)로 collapse 될확률이 \(\frac{3}{4}\)인 qbit이다.</p><p>\(|0\rangle\)은 0으로만 collapse 한다.</p><p><img src="/assets/images/qubit_0_example.png?style=centerme" height=20% alt="|0>"></p><p><img src="/assets/images/qubit_0_result.png?style=centerme" height=95% alt="1024번 관측한 결과로, 100% 0으로 관측된다."></p><h3 id="tensor-product">Tensor product</h3><p>여러 개의 qbit을 나타내기 위해 Tensor product 개념이 필요하다.수학적으로 엄밀한 표현은 아니지만, n개의 qbit 연산을 표현하기 위해서는아래의 표기 방식을 따르는 것이 좋다.</p><p>\[ \binom{x_0}{x_1} \otimes \binom{y_0}{y_1} = \begin{pmatrix} x_0\binom{y_0}{y_1} \\ x_1 \binom{y_0}{y_1} \end{pmatrix} = \begin{pmatrix}x_0 y_0 \\ x_0 y_1 \\ x_1 y_0 \\ x_1 y_1 \end{pmatrix} \]</p><p>이를 응용하면 2개, 3개의 qbit도 벡터처럼 표현할 수 있다.</p><p>\[ |01\rangle = \binom{1}{0} \otimes \binom{0}{1} = \begin{pmatrix} 0\\ 1 \\ 0 \\ 0 \end{pmatrix} \hspace{10pt} |100\rangle = \binom{0}{1}\otimes \binom{1}{0} \otimes \binom{1}{0} = \begin{pmatrix} 0\\ 0\\0\\0\\ 1 \\ 0 \\ 0 \\ 0 \end{pmatrix} \]</p><p>이와 같이 tensor product의 결과로 표현된 벡터는 product state라고한다. 여기서 우리는 \(n\)개 qbit의 product state 크기가 \(2^n\) 이라는것을 알 수 있다. 만약 \( \binom{\frac{1}{\sqrt{2}}}{\frac{1}{\sqrt{2}}}\otimes \binom{\frac{1}{\sqrt{2}}}{\frac{1}{\sqrt{2}}} = \begin{pmatrix}\frac{1}{2} \\ \frac{1}{2} \\ \frac{1}{2} \\ \frac{1}{2} \end{pmatrix}\) 의 multiple qbits 이 있다면 \(\mid 00\rangle\), \(\mid 01\rangle\),\(\mid 10\rangle\), \(\mid 11\rangle\)으로 collapse될 확률이 모두\(\frac{1}{4}\)이므로 동시에 4개의 state를 표현할 수 있게 된다. 즉,qbit이 bit와는 다르게 \(2^n\)개의 state를 표현할 수 있다고 한 것은<em>동시에</em> 가질 수 있는 최대 state 관점에서 비교한 것이다. bit는절대로 동시에 2개 이상의 state를 가질 수 없으므로 한 번에 계산할 수 있는정보는 1개 뿐이다.</p><p>또한 product state는, 뒤의 entanglement와 구분되는 중요한 성질로,<strong>독립적인 state들로 factorize가 가능</strong>하다는 점이있다.</p><p>Multiple qbits의 product state 또한 single qbit과 같은 성질을만족시킨다.</p><p>\[ \binom{a}{b} \otimes \binom{c}{d} = \begin{pmatrix} ac \\ ad \\ bc\\ bd \end{pmatrix} \]</p><p>\[ \text{where, } ||ac||^2 + ||ad||^2 + ||bc||^2 + ||bd||^2 = 1\]</p><h3 id="bit-operations">1-bit operations</h3><p>1-bit에서 가능한 연산은 Identity, Negation, Constant-0, Constant-1의총 4가지가 있다.</p><p><img src="/assets/images/1bit_operations.png?style=centerme" width=40% alt="1-bit 연산의 4 종류"></p><p>각각의 연산은 matrix로 표현할 수 있다.</p><p>\[ \text{Identity} = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1\end{pmatrix} \] \[ \text{Negation} = \begin{pmatrix} 0 &amp; 1 \\ 1&amp; 0 \end{pmatrix} \] \[ \text{Constant-0} = \begin{pmatrix} 1 &amp;1 \\ 0 &amp; 0 \end{pmatrix} \] \[ \text{Constant-1} = \begin{pmatrix} 0&amp; 0 \\ 1 &amp; 1 \end{pmatrix} \]</p><p><img src="/assets/images/1bit_matrix.png?style=centerme" width=70% alt="1-bit 연산의 matrix 표현"></p><h3 id="cnot-one-of-the-2-bit-operations">CNOT (one of the 2-bitoperations)</h3><p>CNOT 연산은 control bit와 target bit로 구성된 2-bit가 있을 때 controlbit가 0이면 target bit를 바꾸지 않고, control bit가 1일 때 target bit를바꾸는 연산이다.</p><p><img src="/assets/images/CNOT_operation.png?style=centerme" width=25% alt="CNOT"></p><p>마찬가지로 이 연산도 matrix로 표현할 수 있다.</p><p>\[ C = \begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0&amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\\end{pmatrix} \]</p><p><img src="/assets/images/CNOT_examples.png?style=centerme" width=60% alt="2-qbits에 적용한 CNOT 예시"></p><p><a href="#bit-operations">2.4</a>와 <ahref="#cnot-one-of-the-2-bit-operations">2.5</a>에서 operation들을matrix화 한 것에 주목하자. 확률이 지배하는 양자 세계에서 deterministic한연산을 하기 위해서는 matrix를 관측하지 않은 qbit에 곱하는 것이 유일한방법이기 때문이다. 아래의 예시에서 우리가 확신할 수 있는 정보는 qbit이0과 1로 관측될 확률이 반대가 되었다는 것이다.</p><p>\[ \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}\begin{pmatrix} \frac{1}{2} \\ \frac{\sqrt{3}}{2} \end{pmatrix} =\begin{pmatrix} \frac{\sqrt{3}}{2} \\ \frac{1}{2} \end{pmatrix} \]</p><p>항상 0 혹은 1로 관측되는 \(\mid0\rangle\)이나 \(\mid1\rangle\)을 쓰면matrix 연산을 고집하지 않아도 되지만 이런 qbit만 사용할거라면고전컴퓨터를 쓰면 그만이다. 굳이 0K 가까이 되는 험악한 조건을 유지해가며계산할 필요가 없다.</p><p>그래서 matrix 연산은 양자 컴퓨팅에서 굉장히 중요하다. 여기에는 한가지추가조건이 있는데, 반드시 연산에 사용되는 matrix는<strong>reversible</strong>해야한다는 것이다. 따라서 앞서 본 1-bit 연산중 Constant-1과 Constant-0를 계산하기 위해서는 단순 matrix를 곱하는 것외의 다른 방법이 필요하다.</p><h2 id="the-deutsch-jozsa-problem">The Deutsch-Jozsa problem</h2><p>이 문제<sup id="fnref:1"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://en.wikipedia.org/wiki/Deutsch%E2%80%93Jozsa_algorithm#Problem_statement&gt;">[1]</span></a></sup>는 양자컴퓨터가 고전컴퓨터에 비해 계산적인 이점을가지는 아주 간단한 (<del><em>동시에 쓸데없는</em></del>) 문제다.</p><blockquote><p>1-bit를 입력받아서 1-bit를 내뱉는 어떤 함수가 있다고 하자. 만약 이함수가 Constant(Contant-0, Constant-1)인지, 아니면 Variable(Identity,Negation)인지 알기 위해서는 최소 몇 번의 query를 날려야 할까?</p></blockquote><h3 id="classical-computer">Classical computer</h3><p>고전컴퓨터에서는 0과 1을 입력해야하므로 총 두 번의 연산이필요하다.</p><h3 id="quantum-computer">Quantum computer</h3><p>예상했듯이 정답은 한 번이다. 왜인지 알기위해서는 추가적인 개념이필요하다.</p><h4 id="hadamard-gate">Hadamard gate</h4><p>앞서 언급된 적 있는 H gate이다.</p><p>\[ H = \begin{pmatrix} \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}} \end{pmatrix} \]</p><p>Hadamard gate는 0- 혹은 1-qbit을 받아서 0과 1을 같은 확률로 가지는qbit으로 바꿔준다.</p><p>\[ H\mid0\rangle = \begin{pmatrix} \frac{1}{\sqrt{2}} &amp;\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}}\end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix}\frac{1}{} \\ \frac{1}{\sqrt{2}} \end{pmatrix} \]</p><p>\[ H\mid1\rangle = \begin{pmatrix} \frac{1}{\sqrt{2}} &amp;\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}}\end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix}\frac{1}{} \\ \frac{-1}{\sqrt{2}} \end{pmatrix} \]</p><p>Hadamard gate는 또 다른 중요한 성질이 있다. 0과 1을 같은 확률로가지는 qbit을 다시 0- 과 1-qbit으로 돌려보낸다는 것이다.</p><p>\[ \begin{pmatrix} \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}} \end{pmatrix}\begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix} =\begin{pmatrix} 1 \\ 0 \end{pmatrix} \]</p><p>\[ \begin{pmatrix} \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}} \end{pmatrix}\begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}} \end{pmatrix}= \begin{pmatrix} 0 \\ 1 \end{pmatrix} \]</p><h4 id="x-gate">X gate</h4><p>X gate는 qbit의 위 아래를 바꿔준다.</p><p>\[ X = \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix} \]</p><p>\[ \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}\begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 1 \\ 0\end{pmatrix} \]</p><p>\[ \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}\begin{pmatrix} \frac{-1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix}= \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}}\end{pmatrix} \]</p><p>H gate와 X gate 연산을 이해하기 쉽게 표현하면 아래의 그림이 된다.붉은색이 X gate, 노란색이 H gate 연산의 방향이다.</p><p><img src="/assets/images/gates-visualization.png?style=centerme" width=80%></p><p>\(\begin{pmatrix} 1 \\ 0 \end{pmatrix}\)에 X - H - X - H - X gate를씌운 결과는 그림으로 보면 더 쉽게 이해된다. \(\begin{pmatrix} 1 \\ 0\end{pmatrix}\)에서 출발해 각 gate가 연산되는 방향으로 화살표를 움직이면마지막에 도달하는 곳이 결과값이 된다.</p><p><img src="/assets/images/quantum_logic_operation_example.png?style=centerme" width=80%></p><h4 id="non-reversible-matrix">non-reversible matrix</h4><p>앞서 양자컴퓨터는 non-reversible한 matrix 를 곱하는 연산은불가능하다고 했다. 1-bit 연산 중에서 Constant-0과 Constant-1은non-reversible하다. 그래서 양자 컴퓨팅에서는 2개의 qbit을 사용한다.</p><p><img src="/assets/images/quantum_non_reversible.png?style=centerme" width=45%></p><p>이 그림의 input과 output notation을 보면 "응?" 이라는 생각이 절로 들것이다. 영상에서도 사람들이 대체 왜 "Output"이 Input 쪽에 가 있는지에대해 끊임없이 묻는다. 아쉽게도 강연자는 속시원하게 답변을 해주지 않아서그런가보다 하고 넘어갔는데 다시 보니 이해가 된 부분이 있어 글로설명해보려고 한다.</p><p><code>Input'</code>과 <code>Output'</code>이 실제 1-bit 연산의input과 output을 나타낸다. 그리고 <code>Input</code>과<code>Output</code>은 <code>Input'</code>과 <code>Output'</code>이 BB이후에 있기 때문에 BB 이전에 <code>Input'</code>과<code>Output'</code>이 1-bit 연산의 input과 output이 되도록 넣어주는,양자컴퓨터 연산 방식 때문에 필요한 input들이다.</p><p>이 약속에 따라서 양자컴퓨터가 1-bit 연산을 어떻게 수행하는지 아래의예시를 통해 좀 더 이해해보자.</p><p><img src="/assets/images/quantum_constant_0.png?style=centerme" width=55% alt="Constant-0"></p><p><img src="/assets/images/quantum_constant_1.png?style=centerme" width=55% alt="Constant-1"></p><p><img src="/assets/images/quantum_identity.png?style=centerme" width=55% alt="Identity. 이미지의 operation은 CNOT"></p><p><img src="/assets/images/quantum_negation.png?style=centerme" width=55% alt="Negation"></p><p>Constant-0은 <code>Input'</code>이 \(\mid 0 \rangle\)일 때와 \(\mid 1\rangle\)일 때 모두 <code>Output'</code>이 \(\mid 0 \rangle\)이어야한다. 어떤 gate도 없는 왼쪽 위 그림의 회로에서 <code>Input</code>에\(\mid 0 \rangle\) 혹은 \(\mid 1 \rangle\)을 대입해보면<code>Input'</code>과 <code>Output'</code>이 Constant-0의 관계를 가지는것을 확인할 수 있다.</p><p>Indentity는 <code>Input'</code>이 \(\mid 0 \rangle\)일 때는<code>Output'</code>이 \(\mid 0 \rangle\)이고 <code>Input'</code>이\(\mid 1 \rangle\)일 때는 <code>Output'</code>이 \(\mid 1 \rangle\)인함수다. 왼쪽 아래 그림의 회로는 CNOT gate를 표현하고 있다. 색이 채워진원이 control bit 쪽을 나타내고 그렇지 않은 쪽 원은 target bit를나타낸다. <code>Input</code>이 \(\mid 0 \rangle\) 이면 control bit가0이므로 target bit도 그대로 유지한다. 그래서 <code>Input'</code>도\(\mid 0 \rangle\), <code>Output'</code>도 \(\mid 0 \rangle\)이 된다.<code>Input</code>이 \(\mid 1 \rangle\) 이면 control bit가 1이므로target bit가 바뀐다. 그래서 <code>Input'</code>도 \(\mid 1 \rangle\),<code>Output'</code>도 \(\mid 1 \rangle\)이 된다.</p><p>그럼 다시 The Deutsch-Jozsa problem로 돌아가서, 양자컴퓨터에서는어떻게 한 번에 구할 수 있을까? 정답은 아래의 그림이 설명해준다.</p><p><img src="/assets/images/quantum_one_query.png?style=centerme" width=60% alt="양자컴퓨터가 한 번에 문제를 푸는 법"></p><p>이 연산대로라면 BB가 Constant(Contant-0, Constant-1)이었을 경우, 측정결과가 \(\mid11\rangle\)이고, Variable(Identity, Negation)이었을경우에는 \(\mid01\rangle\)이 된다.</p><p>BB의 경우의 수를 따져가며 이해해보자.</p><h4 id="preprocessing-bb-입력-직전까지의-연산">preprocessing (BB 입력직전까지의 연산)</h4><p><img src="/assets/images/quantum_preprocessing.png?style=centerme" width=80%></p><p>BB에 들어가기 전 input (\(\mid 0 \rangle\)) 과 output qbit (\(\mid 0\rangle\)) 모두 X와 H gate를 거쳐서 \(\begin{pmatrix} \frac{1}{\sqrt{2}}\\ \frac{-1}{\sqrt{2}} \end{pmatrix}\) 가 된다.</p><h4 id="case-1-bb가-constant-0-이었을-경우">case 1) BB가 Constant-0이었을 경우</h4><p>Constant-0은 input과 output에 어떤 gate도 씌우지 않는다. 따라서 BB가Constant-0이었을 때 Input과 Output은 H gate만 통과한 이후 관측된다.</p><p><img src="/assets/images/const_0.png?style=centerme" width=50% alt="Constant-0"></p><p><img src="/assets/images/quantum_bb_const_0.png?style=centerme" width=80% alt="BB가 Constant-0인 경우 Input'과 Output'"></p><h4 id="case-2-bb가-contstant-1-이었을-경우">case 2) BB가 Contstant-1이었을 경우</h4><p>Constant-1은 output에만 X gate를 적용한다. 따라서 BB가Constant-1이었을 때는 Output에 X gate가 추가되고, 이후 Input과 Output모두에 H gate가 적용된다.</p><p><img src="/assets/images/const_1.png?style=centerme" width=55% alt="Constant-1"></p><p><img src="/assets/images/quantum_bb_const_1.png?style=centerme" width=80% alt="BB가 Constant-1인 경우 Input'과 Output'"></p><h4 id="case-3-bb가-identity-이었을-경우">case 3) BB가 Identity 이었을경우</h4><p>Identity는 CNOT gate를 통해 연산된다. 앞에서 CNOT 연산은 \(\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\\end{pmatrix} \) 을 곱하는 것과 같다고 설명했다. Preprocessing을 거친Input과 Output은 \( \begin{pmatrix} \frac{1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} \end{pmatrix} \) 이므로 CNOT연산은 아래와 같이표현할 수 있다.</p><p>\[ C \begin{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} \end{pmatrix} \otimes \begin{pmatrix}\frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}} \end{pmatrix} \end{pmatrix} =C \begin{pmatrix} \frac{1}{2} \\ \frac{-1}{2} \\ \frac{-1}{2} \\\frac{1}{2} \end{pmatrix} = \frac{1}{2} \begin{pmatrix} 1 &amp; 0 &amp;0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0&amp; 0 &amp; 1 &amp; 0 \\ \end{pmatrix} \begin{pmatrix} 1 \\ -1 \\ -1\\ 1 \end{pmatrix} = \frac{1}{2} \begin{pmatrix} 1 \\ -1 \\ 1 \\ -1\end{pmatrix} = \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}\end{pmatrix} \otimes \begin{pmatrix} \frac{1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} \end{pmatrix} \]</p><p>즉 Input은 \( \begin{pmatrix} \frac{1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} \end{pmatrix} \) 에서 \( \begin{pmatrix}\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix}\) 로 바뀌고Output은 그대로 \( \begin{pmatrix} \frac{1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} \end{pmatrix} \) 이 된다. 이 상태에서 H gate가 각각적용되어 최종 결과는 \(\mid 01 \rangle\)이 된다.</p><p><img src="/assets/images/identity.png?style=centerme" width=55% alt="Identity"></p><p><img src="/assets/images/quantum_bb_identity.png?style=centerme" width=80% alt="BB가 Identity인 경우 Input'과 Output'"></p><h4 id="case-4-bb가-negation-이었을-경우">case 4) BB가 Negation 이었을경우</h4><p>Negation은 Indentity의 결과 중 Output에만 X gate가 추가되는 연산이다.따라서 아래 그림처럼 연산이 이루어지고 Identity와 마찬가지로 최종 결과는\(\mid 01 \rangle\)이 된다.</p><p><img src="/assets/images/negation.png?style=centerme" width=55% alt="Negation"></p><p><img src="/assets/images/quantum_bb_negation.png?style=centerme" width=80% alt="BB가 Negation인 경우 Input'과 Output'"></p><p>정리하면, 양자컴퓨터에서는 특정 설계 상황에서 고정된 BB input에 대한BB output을 "한 번"만 관측하면 BB가 Constant인지 Variable인지 확인할 수있다는 것이다!</p><h2 id="entanglement">Entanglement</h2><p>Entanglement는 지금까지의 흐름에서는 동떨어진 이야기지만양자컴퓨터에서 항상 소개되는 내용이기 때문에 추가하였다.</p><p>앞서 qbit과 product state의 성질을 수학적으로 나타낸 것처럼entanglement도 수학적인 성질로 표현할 수 있다.</p><p>\( \begin{pmatrix} \frac{1}{\sqrt{2}} \\ 0 \\ 0 \\ \frac{1}{\sqrt{2}}\end{pmatrix} \) 는 entangle된 qbit인데, 그 모양새가 product state와닮아있다. 하지만 product state와는 중요한 성질에서 차이를 보인다. 위에서설명했듯이 product state는 개별적인 qbit으로 factorize된다. 하지만entanlged qbit은 개별적인 qbit으로 factorize 되지 않는다. (If theproduct state of two qbits <strong>cannot be factored</strong>, they aresaid to be <strong>entanlged</strong>.) 이 때문에 entangled qbit은차원이 늘어난 하나의 qbit으로 볼 수 있으며 일부를 관측했을 때 나머지일부의 상태가 유추된다.</p><p>\( \begin{pmatrix} \frac{1}{\sqrt{2}} \\ 0 \\ 0 \\ \frac{1}{\sqrt{2}}\end{pmatrix} \) 이 entangle 되었음을 증명하는 것은 간단하다. \(\begin{pmatrix} \frac{1}{\sqrt{2}} \\ 0 \\ 0 \\ \frac{1}{\sqrt{2}}\end{pmatrix} = \begin{pmatrix} a \\ b \end{pmatrix} \otimes\begin{pmatrix} c \\ d \end{pmatrix} \) 를 만족하는 \(a\), \(b\), \(c\),\(d\)는 존재하지 않기 때문에 이는 entanlge되어 있는 qbit이다.</p><p>Entanlged qbit은 CNOT과 H gate를 통해 쉽게 생성할 수 있다.</p><p><img src="/assets/images/entanlge.png?style=centerme" width=50% alt="Entangled qbit"></p><p>\[ CH_1 \begin{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} \otimes\begin{pmatrix} 1 \\ 0 \end{pmatrix} \end{pmatrix} = C \begin{pmatrix}\begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix}\otimes \begin{pmatrix} 1 \\ 0 \end{pmatrix} \end{pmatrix} =\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\\end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} \\ 0 \\\frac{1}{\sqrt{2}} \\ 0 \end{pmatrix} = \begin{pmatrix}\frac{1}{\sqrt{2}} \\ 0 \\ 0 \\ \frac{1}{\sqrt{2}} \end{pmatrix} \]</p><p>만약 이후에 이런 게이트의 조합을 본다면 곧바로 'entanlge 되었군!'이라고 생각하면 된다 :)</p><h2 id="conclusion">Conclusion</h2><p>개인적으로 이 영상을 본 이후, 속이 뻥 뚫리는 기분이 들었다. 아직matrix로 표현되는 qbit이 물리적으로 어떤 모습인지, gate들이 물리적으로어떻게 qbit에 적용되는지는 모르지만 (이건 실제 양자컴퓨터를 눈으로 보면이해가 되지 않을까) 이 정도라도 양자컴퓨터와 고전컴퓨터의 연산과정에서의차이를 구체적으로 알 수 있었기 때문에 만족할 수 있었다.</p><p>양자컴퓨터의 연산 과정을 이해하고나니 양자 우월성은 그냥 달성되는것은 아니었으며, 잘 설계된 gate가 뒷받침되었을 때 가능한 것임을 깨닫게되기도 했다.</p><p>이 정도면 양자 세계에 <code>Hello World!</code>를 했다고 볼 수 있지않을까?</p><h2 id="references">References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Deutsch%E2%80%93Jozsa_algorithm#Problem_statement">https://en.wikipedia.org/wiki/Deutsch–Jozsa_algorithm#Problem_statement</a><a href="#fnref:1" rev="footnote">↩︎</a></span></li></ol></div></div>]]></content>
    
    
    <summary type="html">&lt;p&gt;최근에 있었던 구글의 양자 우월성 (Quantum Supremacy) 달성은
전세계적으로 큰 화제였다. 물 들어올 때 노 저으라고 하지 않았던가, 이
때가 아니면 또 언제 양자컴퓨터에 대해 공부할까 싶어서 텍스트와 동영상을
넘나들며 관련 지식을 습득해보았다.</summary>
    
    
    
    <category term="Tech" scheme="https://www.thespacemoon.com/categories/Tech/"/>
    
    <category term="Quantum Computing" scheme="https://www.thespacemoon.com/categories/Tech/Quantum-Computing/"/>
    
    
    <category term="quantum computing" scheme="https://www.thespacemoon.com/tags/quantum-computing/"/>
    
  </entry>
  
  <entry>
    <title>Hello World, Quantum Computing</title>
    <link href="https://www.thespacemoon.com/2019/11/07/basics-of-quantum-computing-en/"/>
    <id>https://www.thespacemoon.com/2019/11/07/basics-of-quantum-computing-en/</id>
    <published>2019-11-06T15:07:00.000Z</published>
    <updated>2019-11-06T15:07:00.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>I kept reading that qbits can represent \(2^n\) statessimultaneously, and I kept nodding along. Then I asked myself threequestions and couldn't answer any of them.</p><span id="more"></span><ul><li><strong>Q1.</strong> Can't n classical bits also represent \(2^n\)states? Three bits give you 000, 001, 010, 011, 100, 101, 110, 111 --that's eight states.</li><li><strong>Q2.</strong> The quantum world is governed by theuncertainty principle, so how does a quantum computer actually performcomputations, and how does this reduce computational cost?</li><li><strong>Q3.</strong> What exactly does it mean for qbits to beentangled?</li></ul><p>Plain language kept giving me hand-wavy answers. The math turned outto be simpler than I expected.</p><iframe width="560" height="315" src="https://www.youtube.com/embed/F_Riqjdh2oM?style=centerme" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p><ahref="https://speakerdeck.com/ahelwer/quantum-computing-for-computer-scientists">[slide]</a></p><p>The video is over an hour, so what follows is my reorganization ofthe material -- a <code>Hello World!</code> for the quantum world.</p><p>A quantum computer can solve in one query what takes a classicalcomputer two. The Deutsch-Jozsa problem shows why. To get there, we needqbits, matrix operations, and a few logic gates.</p><h2 id="basics">Basics</h2><h3 id="qbit">Qbit</h3><p>A qbit is the fundamental unit of quantum computation. It alwayssatisfies the following condition:</p><blockquote><p>A qbit, represented by \(\begin{pmatrix} \alpha \\ \beta\end{pmatrix}\) where \(\alpha\) and \(\beta\) are complex numbers mustbe constrained by the equation \(||\alpha||^2 + ||\beta||^2 = 1\)</p></blockquote><p>So the following are all valid qbits:</p><p>\[ \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}\end{pmatrix} \hspace{10pt} \begin{pmatrix} \frac{1}{2} \\\frac{\sqrt{3}}{2} \end{pmatrix} \hspace{10pt} \begin{pmatrix} 1 \\ 0\end{pmatrix} \hspace{10pt} \begin{pmatrix} 0 \\ -1 \end{pmatrix} \]</p><p>The basis vectors for all of these, \(\begin{pmatrix} 1 \\0\end{pmatrix}\) and \(\begin{pmatrix} 0 \\ 1 \end{pmatrix}\), are giventhe special notation \(\mid 0\rangle\) and \(\mid 1\rangle\),respectively.</p><h3 id="superposition">Superposition</h3><p>This is the property you always hear about when people discuss qbits.It's often described as "being 0 and 1 at the same time," but a moreprecise description is: "When we measure a qbit, it collapses to anactual value of 0 or 1."</p><p>Let's take one of the qbit vectors from above as an example.</p><p>\[\begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}\end{pmatrix}\]</p><p>This qbit has a \(\frac{1}{2}\) ( \(= || \frac{1}{\sqrt{2}} || ^2\))probability of collapsing to either \(0\) or \(1\).</p><p>Thankfully, <a href="https://quantum-computing.ibm.com/">IBM has madetheir quantum computers accessible via an API</a>. If we create thisqbit there and measure it 1024 times, we can confirm that 0 and 1 eachappear about 50% of the time.</p><p><img src="/assets/images/qubit_1_2_example.png?style=centerme" width=20% alt="Through an H gate, |0> becomes the example qbit. (This is covered below.)"></p><p><img src="/assets/images/qubit_1_2_result.png?style=centerme" width=95% alt="Result of 1024 measurements. It shows 0 and 1 with roughly 50% probability each."></p><p>\(\begin{pmatrix} \frac{1}{2} \\ \frac{\sqrt{3}}{2} \end{pmatrix}\)is a qbit with a \(\frac{1}{4}\) probability of collapsing to \(0\) anda \(\frac{3}{4}\) probability of collapsing to \(1\).</p><p>\(|0\rangle\) always collapses to 0.</p><p><img src="/assets/images/qubit_0_example.png?style=centerme" height=20% alt="|0>"></p><p><img src="/assets/images/qubit_0_result.png?style=centerme" height=95% alt="Result of 1024 measurements: 100% observed as 0."></p><h3 id="tensor-product">Tensor product</h3><p>To represent multiple qbits, we need the concept of the tensorproduct. The notation works like this:</p><p>\[ \binom{x_0}{x_1} \otimes \binom{y_0}{y_1} = \begin{pmatrix} x_0\binom{y_0}{y_1} \\ x_1 \binom{y_0}{y_1} \end{pmatrix} = \begin{pmatrix}x_0 y_0 \\ x_0 y_1 \\ x_1 y_0 \\ x_1 y_1 \end{pmatrix} \]</p><p>Using this, we can represent 2 or 3 qbits as vectors.</p><p>\[ |01\rangle = \binom{1}{0} \otimes \binom{0}{1} = \begin{pmatrix} 0\\ 1 \\ 0 \\ 0 \end{pmatrix} \hspace{10pt} |100\rangle = \binom{0}{1}\otimes \binom{1}{0} \otimes \binom{1}{0} = \begin{pmatrix} 0\\ 0\\0\\0\\ 1 \\ 0 \\ 0 \\ 0 \end{pmatrix} \]</p><p>A vector expressed as the result of a tensor product is called aproduct state. From this, we can see that the product state of \(n\)qbits has size \(2^n\). If we have the following multi-qbit state: \(\binom{\frac{1}{\sqrt{2}}}{\frac{1}{\sqrt{2}}} \otimes\binom{\frac{1}{\sqrt{2}}}{\frac{1}{\sqrt{2}}} = \begin{pmatrix}\frac{1}{2} \\ \frac{1}{2} \\ \frac{1}{2} \\ \frac{1}{2} \end{pmatrix}\) then the probabilities of collapsing to \(\mid 00\rangle\), \(\mid01\rangle\), \(\mid 10\rangle\), and \(\mid 11\rangle\) are each\(\frac{1}{4}\), meaning it can represent all four statessimultaneously. In other words, when people say qbits can represent\(2^n\) states unlike classical bits, they're comparing the maximumnumber of states that can be held <em>simultaneously</em>. A classicalbit can never be in more than one state at a time, so it can onlyprocess one piece of information per operation.</p><p>Another important property of product states -- one thatdistinguishes them from entanglement, which we'll cover later -- is that<strong>they can be factored into independent states</strong>.</p><p>The product state of multiple qbits satisfies the same constraint asa single qbit.</p><p>\[ \binom{a}{b} \otimes \binom{c}{d} = \begin{pmatrix} ac \\ ad \\ bc\\ bd \end{pmatrix} \]</p><p>\[ \text{where, } ||ac||^2 + ||ad||^2 + ||bc||^2 + ||bd||^2 = 1\]</p><h3 id="bit-operations">1-bit operations</h3><p>There are exactly four possible 1-bit operations: Identity, Negation,Constant-0, and Constant-1.</p><p><img src="/assets/images/1bit_operations.png?style=centerme" width=40% alt="The four types of 1-bit operations"></p><p>Each operation can be represented as a matrix.</p><p>\[ \text{Identity} = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1\end{pmatrix} \] \[ \text{Negation} = \begin{pmatrix} 0 &amp; 1 \\ 1&amp; 0 \end{pmatrix} \] \[ \text{Constant-0} = \begin{pmatrix} 1 &amp;1 \\ 0 &amp; 0 \end{pmatrix} \] \[ \text{Constant-1} = \begin{pmatrix} 0&amp; 0 \\ 1 &amp; 1 \end{pmatrix} \]</p><p><img src="/assets/images/1bit_matrix.png?style=centerme" width=70% alt="Matrix representations of 1-bit operations"></p><h3 id="cnot-one-of-the-2-bit-operations">CNOT (one of the 2-bitoperations)</h3><p>The CNOT operation takes two bits -- a control bit and a target bit.If the control bit is 0, the target bit stays unchanged. If the controlbit is 1, the target bit is flipped.</p><p><img src="/assets/images/CNOT_operation.png?style=centerme" width=25% alt="CNOT"></p><p>This operation can also be represented as a matrix.</p><p>\[ C = \begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0&amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\\end{pmatrix} \]</p><p><img src="/assets/images/CNOT_examples.png?style=centerme" width=60% alt="CNOT examples applied to 2-qbit states"></p><p>Note how the operations in <a href="#bit-operations">2.4</a> and <ahref="#cnot-one-of-the-2-bit-operations">2.5</a> were expressed asmatrices. In the quantum world, where probability reigns, the only wayto perform deterministic operations is to multiply an unobserved qbit bya matrix. In the example below, the one thing we can be certain of isthat the probabilities of observing 0 and 1 have been swapped.</p><p>\[ \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}\begin{pmatrix} \frac{1}{2} \\ \frac{\sqrt{3}}{2} \end{pmatrix} =\begin{pmatrix} \frac{\sqrt{3}}{2} \\ \frac{1}{2} \end{pmatrix} \]</p><p>Of course, if we only use \(\mid0\rangle\) or \(\mid1\rangle\), whichalways collapse to 0 or 1, we wouldn't need matrix operations at all --but then we'd just be using a classical computer. There would be noreason to maintain the extreme conditions near 0K required for quantumcomputation.</p><p>So matrix operations are critical in quantum computing. There's oneadditional constraint: the matrices used must be<strong>reversible</strong>. This means that the Constant-0 andConstant-1 operations from the 1-bit operations above cannot be computedby simple matrix multiplication -- a different approach is needed.</p><h2 id="the-deutsch-jozsa-problem">The Deutsch-Jozsa problem</h2><p>This problem<sup id="fnref:1"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://en.wikipedia.org/wiki/Deutsch%E2%80%93Jozsa_algorithm#Problem_statement&gt;">[1]</span></a></sup> is a very simple (<del><em>and simultaneouslyuseless</em></del>) problem that demonstrates a computational advantageof quantum computers over classical ones.</p><blockquote><p>Suppose there is a function that takes a 1-bit input and produces a1-bit output. To determine whether this function is constant (Constant-0or Constant-1) or balanced (Identity or Negation), what is the minimumnumber of queries required?</p></blockquote><h3 id="classical-computer">Classical computer</h3><p>A classical computer must test both 0 and 1 as inputs, so it requirestwo queries.</p><h3 id="quantum-computer">Quantum computer</h3><p>As you might expect, the answer is one. To see why, we need a fewmore concepts.</p><h4 id="hadamard-gate">Hadamard gate</h4><p>This is the H gate mentioned earlier.</p><p>\[ H = \begin{pmatrix} \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}} \end{pmatrix} \]</p><p>The Hadamard gate takes a 0- or 1-qbit and transforms it into a qbitwith equal probability of being 0 or 1.</p><p>\[ H\mid0\rangle = \begin{pmatrix} \frac{1}{\sqrt{2}} &amp;\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}}\end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix}\frac{1}{} \\ \frac{1}{\sqrt{2}} \end{pmatrix} \]</p><p>\[ H\mid1\rangle = \begin{pmatrix} \frac{1}{\sqrt{2}} &amp;\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}}\end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix}\frac{1}{} \\ \frac{-1}{\sqrt{2}} \end{pmatrix} \]</p><p>The Hadamard gate has another important property: it sends a qbitwith equal probability of 0 and 1 back to a definite 0- or 1-qbit.</p><p>\[ \begin{pmatrix} \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}} \end{pmatrix}\begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix} =\begin{pmatrix} 1 \\ 0 \end{pmatrix} \]</p><p>\[ \begin{pmatrix} \frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}} \end{pmatrix}\begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}} \end{pmatrix}= \begin{pmatrix} 0 \\ 1 \end{pmatrix} \]</p><h4 id="x-gate">X gate</h4><p>The X gate swaps the top and bottom components of a qbit.</p><p>\[ X = \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix} \]</p><p>\[ \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}\begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 1 \\ 0\end{pmatrix} \]</p><p>\[ \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix}\begin{pmatrix} \frac{-1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix}= \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}}\end{pmatrix} \]</p><p>The H gate and X gate operations are easier to understand with thediagram below. Red indicates the X gate and yellow indicates the H gatedirection.</p><p><img src="/assets/images/gates-visualization.png?style=centerme" width=80%></p><p>The result of applying X - H - X - H - X gates to \(\begin{pmatrix} 1\\ 0 \end{pmatrix}\) is much easier to follow in the diagram. Startingfrom \(\begin{pmatrix} 1 \\ 0 \end{pmatrix}\), follow the arrows in thedirection of each gate -- where you end up is the result.</p><p><img src="/assets/images/quantum_logic_operation_example.png?style=centerme" width=80%></p><h4 id="non-reversible-matrix">non-reversible matrix</h4><p>Earlier I mentioned that quantum computers cannot multiply bynon-reversible matrices. Among the 1-bit operations, Constant-0 andConstant-1 are non-reversible. To handle these in quantum computing, weuse two qbits.</p><p><img src="/assets/images/quantum_non_reversible.png?style=centerme" width=45%></p><p>The notation looks backwards at first. <code>Input'</code> and<code>Output'</code> represent the actual 1-bit operation's input andoutput. <code>Input</code> and <code>Output</code> are what we feed in<em>before</em> the black box (BB) so that <code>Input'</code> and<code>Output'</code> -- which appear <em>after</em> BB -- land on theright values. They exist because every quantum operation must bereversible.</p><p>Some examples make this clearer.</p><p><img src="/assets/images/quantum_constant_0.png?style=centerme" width=55% alt="Constant-0"></p><p><img src="/assets/images/quantum_constant_1.png?style=centerme" width=55% alt="Constant-1"></p><p><img src="/assets/images/quantum_identity.png?style=centerme" width=55% alt="Identity. The operation in the image is CNOT."></p><p><img src="/assets/images/quantum_negation.png?style=centerme" width=55% alt="Negation"></p><p>For Constant-0, <code>Output'</code> must be \(\mid 0 \rangle\)regardless of whether <code>Input'</code> is \(\mid 0 \rangle\) or\(\mid 1 \rangle\). In the upper-left circuit (which has no gates),substituting \(\mid 0 \rangle\) or \(\mid 1 \rangle\) into<code>Input</code> confirms that <code>Input'</code> and<code>Output'</code> satisfy the Constant-0 relationship.</p><p>Identity means <code>Output'</code> equals \(\mid 0 \rangle\) when<code>Input'</code> is \(\mid 0 \rangle\), and <code>Output'</code>equals \(\mid 1 \rangle\) when <code>Input'</code> is \(\mid 1\rangle\). The lower-left circuit represents a CNOT gate: the filledcircle marks the control bit, and the open circle marks the target bit.If <code>Input</code> is \(\mid 0 \rangle\), the control bit is 0, sothe target bit stays unchanged -- both <code>Input'</code> and<code>Output'</code> are \(\mid 0 \rangle\). If <code>Input</code> is\(\mid 1 \rangle\), the control bit is 1, so the target bit flips --both <code>Input'</code> and <code>Output'</code> become \(\mid 1\rangle\).</p><p>Now let's return to the Deutsch-Jozsa problem: how can a quantumcomputer solve it in a single query? The answer is captured in thediagram below.</p><p><img src="/assets/images/quantum_one_query.png?style=centerme" width=60% alt="How a quantum computer solves the problem in one query"></p><p>With this circuit, if BB was constant (Constant-0 or Constant-1), themeasurement yields \(\mid11\rangle\), and if BB was balanced (Identityor Negation), the measurement yields \(\mid01\rangle\).</p><p>Let's walk through each possible BB case to see why.</p><h4 id="preprocessing-operations-before-bb">preprocessing (operationsbefore BB)</h4><p><img src="/assets/images/quantum_preprocessing.png?style=centerme" width=80%></p><p>Before entering BB, both the input (\(\mid 0 \rangle\)) and outputqbit (\(\mid 0 \rangle\)) pass through an X gate followed by an H gate,resulting in \(\begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}}\end{pmatrix}\).</p><h4 id="case-1-bb-is-constant-0">case 1) BB is Constant-0</h4><p>Constant-0 applies no gates to the input or output. So when BB isConstant-0, the Input and Output simply pass through the final H gatebefore being measured.</p><p><img src="/assets/images/const_0.png?style=centerme" width=50% alt="Constant-0"></p><p><img src="/assets/images/quantum_bb_const_0.png?style=centerme" width=80% alt="Input' and Output' when BB is Constant-0"></p><h4 id="case-2-bb-is-constant-1">case 2) BB is Constant-1</h4><p>Constant-1 applies an X gate only to the output. So when BB isConstant-1, an X gate is added to the Output, and then H gates areapplied to both Input and Output.</p><p><img src="/assets/images/const_1.png?style=centerme" width=55% alt="Constant-1"></p><p><img src="/assets/images/quantum_bb_const_1.png?style=centerme" width=80% alt="Input' and Output' when BB is Constant-1"></p><h4 id="case-3-bb-is-identity">case 3) BB is Identity</h4><p>Identity is computed using a CNOT gate. As shown earlier, the CNOToperation is equivalent to multiplying by \( \begin{pmatrix} 1 &amp; 0&amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp;1 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ \end{pmatrix} \). After preprocessing,both Input and Output are \( \begin{pmatrix} \frac{1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} \end{pmatrix} \), so the CNOT operation can bewritten as:</p><p>\[ C \begin{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} \end{pmatrix} \otimes \begin{pmatrix}\frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}} \end{pmatrix} \end{pmatrix} =C \begin{pmatrix} \frac{1}{2} \\ \frac{-1}{2} \\ \frac{-1}{2} \\\frac{1}{2} \end{pmatrix} = \frac{1}{2} \begin{pmatrix} 1 &amp; 0 &amp;0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0&amp; 0 &amp; 1 &amp; 0 \\ \end{pmatrix} \begin{pmatrix} 1 \\ -1 \\ -1\\ 1 \end{pmatrix} = \frac{1}{2} \begin{pmatrix} 1 \\ -1 \\ 1 \\ -1\end{pmatrix} = \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}\end{pmatrix} \otimes \begin{pmatrix} \frac{1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} \end{pmatrix} \]</p><p>So Input changes from \( \begin{pmatrix} \frac{1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} \end{pmatrix} \) to \( \begin{pmatrix}\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix}\), while Outputremains \( \begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{-1}{\sqrt{2}}\end{pmatrix} \). After the final H gate is applied to each, the resultis \(\mid 01 \rangle\).</p><p><img src="/assets/images/identity.png?style=centerme" width=55% alt="Identity"></p><p><img src="/assets/images/quantum_bb_identity.png?style=centerme" width=80% alt="Input' and Output' when BB is Identity"></p><h4 id="case-4-bb-is-negation">case 4) BB is Negation</h4><p>Negation is the Identity result with an additional X gate appliedonly to the Output. So the computation proceeds as shown below, and justlike Identity, the final result is \(\mid 01 \rangle\).</p><p><img src="/assets/images/negation.png?style=centerme" width=55% alt="Negation"></p><p><img src="/assets/images/quantum_bb_negation.png?style=centerme" width=80% alt="Input' and Output' when BB is Negation"></p><p>In summary, given the right circuit design, a quantum computer onlyneeds one measurement to determine whether BB is constant orbalanced.</p><h2 id="entanglement">Entanglement</h2><p>The CNOT gate in the Deutsch-Jozsa circuit hints at something deeper.When certain gate combinations act on multiple qbits, the result can'tbe separated back into individual qbits. That's entanglement.</p><p>\( \begin{pmatrix} \frac{1}{\sqrt{2}} \\ 0 \\ 0 \\ \frac{1}{\sqrt{2}}\end{pmatrix} \) is an entangled qbit state. It looks similar to aproduct state, but differs in one crucial way. As explained above, aproduct state can be factored into individual qbits. An entangled statecannot. (If the product state of two qbits <strong>cannot befactored</strong>, they are said to be <strong>entangled</strong>.)Because of this, an entangled qbit pair behaves as a single unit --measuring part of it immediately tells you the state of the rest.</p><p>Proving that \( \begin{pmatrix} \frac{1}{\sqrt{2}} \\ 0 \\ 0 \\\frac{1}{\sqrt{2}} \end{pmatrix} \) is entangled is straightforward.Since there are no values \(a\), \(b\), \(c\), \(d\) that satisfy \(\begin{pmatrix} \frac{1}{\sqrt{2}} \\ 0 \\ 0 \\ \frac{1}{\sqrt{2}}\end{pmatrix} = \begin{pmatrix} a \\ b \end{pmatrix} \otimes\begin{pmatrix} c \\ d \end{pmatrix} \), this is an entangled state.</p><p>Entangled qbits can be easily created using a CNOT gate and an Hgate.</p><p><img src="/assets/images/entanlge.png?style=centerme" width=50% alt="Entangled qbit"></p><p>\[ CH_1 \begin{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} \otimes\begin{pmatrix} 1 \\ 0 \end{pmatrix} \end{pmatrix} = C \begin{pmatrix}\begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix}\otimes \begin{pmatrix} 1 \\ 0 \end{pmatrix} \end{pmatrix} =\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\\end{pmatrix} \begin{pmatrix} \frac{1}{\sqrt{2}} \\ 0 \\\frac{1}{\sqrt{2}} \\ 0 \end{pmatrix} = \begin{pmatrix}\frac{1}{\sqrt{2}} \\ 0 \\ 0 \\ \frac{1}{\sqrt{2}} \end{pmatrix} \]</p><p>This is the same H + CNOT pattern that appears in the Deutsch-Jozsacircuit. Entanglement is doing work there even when it isn't thepoint.</p><h2 id="conclusion">Conclusion</h2><p>The thing that surprised me most: quantum supremacy doesn't justhappen. The Deutsch-Jozsa circuit works because someone designed theright sequence of H gates and CNOT operations to extract a globalproperty in one shot. Quantum advantage is engineered, not inherent.</p><p>I still don't know what a qbit looks like physically, or how gatesare applied to actual hardware. But I now understand <em>why</em> aquantum computer can answer in one query what a classical computer needstwo for. That feels like enough for a <code>Hello World!</code>.</p><h2 id="references">References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Deutsch%E2%80%93Jozsa_algorithm#Problem_statement">https://en.wikipedia.org/wiki/Deutsch–Jozsa_algorithm#Problem_statement</a><a href="#fnref:1" rev="footnote">↩︎</a></span></li></ol></div></div>]]></content>
    
    
    <summary type="html">&lt;p&gt;I kept reading that qbits can represent &#92;(2^n&#92;) states
simultaneously, and I kept nodding along. Then I asked myself three
questions and couldn&#39;t answer any of them.&lt;/p&gt;</summary>
    
    
    
    <category term="Tech" scheme="https://www.thespacemoon.com/categories/Tech/"/>
    
    <category term="Quantum Computing" scheme="https://www.thespacemoon.com/categories/Tech/Quantum-Computing/"/>
    
    
    <category term="quantum computing" scheme="https://www.thespacemoon.com/tags/quantum-computing/"/>
    
  </entry>
  
  <entry>
    <title>Naver News Comment Analysis (3)</title>
    <link href="https://www.thespacemoon.com/2019/09/23/naver-news-comments-analysis-3/"/>
    <id>https://www.thespacemoon.com/2019/09/23/naver-news-comments-analysis-3/</id>
    <published>2019-09-23T08:25:00.000Z</published>
    <updated>2019-09-23T08:25:00.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="tldr">TL;DR</h2><p>어뷰저는 존재한다. 하지만 그들을 완전히 통제하고 제거하는 것은불가능하다. 그렇다면 어뷰저를 막는 것에 집중하지 말고, 어뷰징은내버려두되 그 효과를 랭킹 시스템을 바꾸어 완화시키는 방법은 어떨까?</p><p>네이버 뉴스 플랫폼에서는 순공감과 공감비율을 통해 공감이 많은 의견을상위에 랭크시키고 있다. 각각의 알고리즘이 가진 결함도 문제지만, 과연정치적 의견의 장에서 공감이 많은 의견만이 우리가 듣고 보아야 할의견일까? 특히나 어뷰징이 있는 상황에서 공감수가 많은 의견은 더욱획일화된 주장을 펼칠 수 밖에 없으며 대중은 편향된 의견만 접하게 되어무의식적으로 다양한 사고에 대한 가능성을 차단받는다.</p><p>그래서 이번 글에서는 다양한 의견이 상위에 랭크될 수 있는 sortingalgorithm들을 제안한다. reddit과 yelp 등에서 사용하고 있는 알고리즘을비롯하여, 논쟁적인 댓글이 상위에 위치할 수 있는 알고리즘, 그리고비공감이 많은 댓글에 더 높은 점수를 부여하는 알고리즘 3가지를 소개한다.<span id="more"></span></p><h2 id="introduction">Introduction</h2><p>모두가 다 알고 있는 사실이지만, 어뷰저는 존재한다. 드루킹과<a href="/2019/08/03/naver-news-comments-analysis-2/" title="Naver News Comment Analysis (2)">Naver News Comments Analysis (2)</a> 에서 나온 결론으로도 뒷받침될 수 있지만 트위터에<code>m.news.naver.com/comment</code> 라고 검색하기만 해도 아래와 같이댓글 조작의 흔적을 쉽게 발견할 수 있다.</p><p><img src="/assets/images/twitter.png?style=centerme" width=60% alt="https://twitter.com/search?q=m.news.naver.com%2Fcomment&src=recent_search_click"><br></p><p>이렇듯 쉽게 어뷰저의 존재를 찾을 수 있음에도 네이버가 어뷰저를 잡지않는 이유는 그 일이 생각처럼 쉬운 일이 아니기 때문이다.</p><p>n초 안에 여러번 공감과 비공감을 지속적으로 받은 댓글은 어뷰징의결과로 의심한다. 그 댓글을 지워야 할까? 만약 댓글을 쓴 유저가 어뷰저가아니었다면 문제가 될 수 있다.</p><p>사후 분석을 통해 어뷰저로 의심되는 댓글의 내용을 지우는 방법은어떨까? 뉴스라는 매체의 특성 상 시간이 지난 기사는 사람들이 관심있게보지 않는다. 그러므로 이 방법은 어뷰저를 막는다고 볼 수 없다.</p><p>분석을 통해 어뷰저라고 강하게 의심되는 유저를 차단한다고 하더라도새로운 패턴으로 어뷰징을 하는 유저들이 생겨날 것이다. 어뷰저의 기준을세우는 것은 어려운 반면 새로운 방식으로 어뷰징을 하는 것은 좀 더 쉽기때문에 이렇게 물고 물리는 싸움은 어뷰저에게 유리하다.</p><p>그렇다면 어뷰저를 차단하는 것에만 집중하지 말고, 어뷰징은 내버려두되그 효과를 완화시키는 방법은 어떨까? <strong>지금 네이버 뉴스 댓글 랭킹방식은 그것이 미치는 영향력에 비해 너무 간단하고 단편적이다.</strong>구글의 검색 랭킹이 신뢰도를 가지고 있는 이유는 상위에 랭크된 글이'조작'을 통해 만들어지지 않았다는 점 때문일 것이다. 그 이유는 정보가되는 글에 대한 정보량, 품질 기준이 보다 엄격하고 단편적인 면으로만순위를 매기지 않기 때문이다. 만약 구글 랭킹이 웹문서의 클릭수로만 되어있었다면 어땠을까? 많은 기업들이 본인의 홈페이지를 상위에 랭크시키기위해 많은 조작이 일어났을 것이다.</p><p>그래서 이번 글에서는 그렇게 간단하다고는 볼 수 없는 다른 랭킹algorithm에 대해 소개해보려고 한다. 현재 네이버 뉴스 댓글 랭킹 방식 중순공감순, 공감비율순, 답글순의 한계점을 살펴보고 reddit과 yelp에서신뢰도있게 쓰이는 best 랭킹과 새로운 관점의 controversial 랭킹algorithm을 소개한다.</p><h2 id="naver-news-comment-sorting-system">Naver News Comment SortingSystem</h2><h3 id="sorting-algorithms">Sorting Algorithms</h3><p>2019년 9월 기준, 총 5개의 정렬방으로 서비스되고 있다. 드루킹 논란이후 댓글 제공 여부와 정렬방식을 언론사가 선택하는 방식으로바뀌었다.</p><ul><li><strong>순공감순</strong>: 공감 -비공감<sup id="fnref:1"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="2017년 11월 30일부터 호감순에서 순공감순으로 변경되면서 다시 2016년 이전의 호감순처럼 호감도를 “공감-비공감”으로 계산하게 되었다.">[1]</span></a></sup></li><li><strong>공감비율순:</strong> 공감 / (공감 + 비공감)</li><li><strong>답글순</strong></li><li><strong>최신순</strong></li><li><strong>과거순</strong></li></ul><p>이 중, 댓글에 대한 사용자의 인터랙션(공감, 비공감, 답글)으로 순위를매기는 순공감순, 공감비율순, 답글순에 대한 문제점을 하나씩 짚어보고자한다.</p><h3 id="limitations">Limitations</h3><h4 id="순공감순">순공감순</h4><p>순공감순은 우리의 직관과 벗어나는 랭킹이라는 점에서 한계가 있다.우리는 절대적인 공감 수치보다, 공감비율로 댓글의 신뢰도를 평가한다.</p><p>아래의 사례는 네이버 뉴스댓글<sup id="fnref:2"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="홍준표 “나경원, 아들 이중국적 여부 밝혀라…1억 피부과 연상”, 네이버 뉴스">[2]</span></a></sup>의 실제 예시이다. 첫번째 댓글은 순공감 344개(= 455- 111) 로, 300개(= 316 - 16)의 순공감을 지니는 두번째 댓글보다 더 높은순위에 자리한다. 하지만 각각의 댓글의 공감비율은 80.4%(= 455 / (455 +11)) 로, 두번째 댓글의 공감비율인 95.2% (= 316 / (316 + 16)) 보다작다.</p><p><img src="/assets/images/sgg_limit.png?style=centerme" width=70% alt="paid****: 순공감 344, 공감비율 80.4% | adam****: 순공감 300, 공감비율 95.2%"><br></p><h4 id="공감비율순">공감비율순</h4><p>앞서 설명한 것처럼 공감비율순이 좀 더 우리의 직관과 유사한 척도이다.하지만 공감비율순은 전체 공감, 비공감 수가 적을 때 문제가 된다.</p><p>소수의 사람들에게만 노출된 댓글은 공감과 비공감의 개수가 모두 적어100% 라는 공감비율이 쉽게 만들어지는 반면, 여러 명에게 노출된 댓글은하나의 비공감만 달리더라도 그보다 낮은 공감비율을 지니게 되는 문제가발생한다.</p><p>아래의 네이버 뉴스 댓글예시<sup id="fnref:3"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="재판에 넘겨진 조국 부인 정경심 교수…검찰 &#39;소환 임박&#39;, 네이버 뉴스">[3]</span></a></sup>에서 공감수가 20, 비공감수가 0인 댓글이 비공감을전혀 받지 않아 공감비율 100%가 되어 더 많은 사람들이 읽고 공감을 표한공감수 1021, 비공감수 58인 댓글보다 더 상단에 위치한다.</p><p><img src="/assets/images/ggratio_limit.png?style=centerme" width=70% alt="euic****, qkrs**** 공감비율 100% 공감+비공감수 20 | hang**** 공감비율 94.6% 공감+비공감수 1,079"><br></p><h4 id="답글순">답글순</h4><p>여러 개의 답글이 달리는 댓글은 주로 일찍 남겨진 댓글 중에인신공격이나 뉴스 외 주제에 대한 댓글인 경우가 많다. 댓글 공간에서는명확한 내용으로 구성된 댓글에 대해서는 대댓글 보다도 공감 혹은비공감으로 본인의 주장을 표시하는 것이 일반적이다. 그러나 감정적으로쓰여진 댓글은 그 댓글에 자극을 받은 다른 사용자의 답글로 이어지고 되므로답글 개수를 기준으로 댓글을 정렬하면 뉴스 내용과는 무관한 자극적인댓글들이 우선적으로 노출된다.</p><p>또한 일찍 쓰여진 댓글일수록 더 많은 사람들에게 노출될 가능성이있으므로 대부분 뉴스 작성 시점과 가까운 댓글이 상위에 랭크된다.</p><p>랭킹 algorithm으로 보기에는 정렬 기준이 controllable하지 않으며댓글의 유익한 속성이 높게 평가되어 정렬되는 랭킹이라고 볼 수 없다.</p><p>아래의 네이버 뉴스 댓글예시<sup id="fnref:4"><a href="#fn:4" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="대학교수 이어 의사 4400명도 “조국 퇴진, 조국 딸 퇴교” 시국선언문 서명, 네이버 뉴스">[4]</span></a></sup>를 보면 vote 수가 많지 않아도, 공감수가 전혀 없고비공감만 받더라도 top 10에 위치할 수 있다.</p><p><img src="/assets/images/replyCount_limit.png?style=centerme" width=70%></p><h2 id="reddit-comment-sorting-algorithms">Reddit Comment SortingAlgorithms</h2><p>댓글이 활발하게 생성되는 플랫폼은 비단 네이버 뉴스 뿐만은 아니다.네이버 쇼핑, 네이버 호텔, 망고 플레이트, reddit, stackoverflow, yelp,amazon 등의 다양한 플랫폼에서 수집되며 플랫폼에서는 다시 이 데이터를가공하여 사용자에게 유익한 정보를 제공한다.</p><p>그 중에서도 reddit의 랭킹 시스템이 앞서 비판했던 순공감순,공감비율순의 한계를 극복한 sorting algorithm을 제공하고 있기에 자세히살펴보려고 한다. reddit의 랭킹 방식에는 best, top, new, controversial,old, q&amp;a가 있다. top이 순공감순, new가 최신순, old가 과거순이다.</p><p><img src="/assets/images/reddit_sorting.png?style=centerme" width=20% alt="reddit의 sorting algorithms"></p><h3 id="best">Best</h3><p>Best ranking<sup id="fnref:5"><a href="#fn:5" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://redditblog.com/2009/10/15/reddits-new-comment-sorting-system&gt;">[5]</span></a></sup><sup id="fnref:6"><a href="#fn:6" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;http://www.evanmiller.org/how-not-to-sort-by-average-rating.html&gt;">[6]</span></a></sup> 은 Wilsonscore<sup id="fnref:7"><a href="#fn:7" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval&gt;">[7]</span></a></sup>로 정렬한 것으로, 공감비율순의 단점으로언급되었던, 전체 vote수가 적은 상황을 smoothing시켜준 algorithm이다.reddit뿐 아니라 yelp에서도 사용한다고한다<sup id="fnref:8"><a href="#fn:8" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://blog.yelp.com/2011/02/the-most-romantic-city-on-yelp-is&gt;">[8]</span></a></sup>.</p><p>Wilson score는 주어진 positive와 negative vote가 binomialdistribution을 따른다고 가정했을 때, positive 발생 확률을 95% 신뢰구간의최소값으로 추정한 값이다.</p><p>동전 뒤집기 상황에서 앞면을 positive, 뒷면을 negative라고 하자. n번던진 후 앞면이 나올 확률(\(p\))을 추정할 때 n이 충분히 큰 경우 centrallimit theorem에 의해 \(p\)는 normal distribution을 따른다. 따라서 95%의신뢰도로 \(p\)를 추정하여 \(p\)의 최소값, 최대값을 구할 수 있고 이 때최소값이 Wilson score가 된다. 자세한 수식은 <ahref="#appendix-a-wilson-score">Appendix A</a>에 정리해두었다.</p><p>\[ w^- = max(0, \frac{2n\hat{p} + z^2 - z\sqrt{z^2 +4n\hat{p}(1-\hat{p})}}{2(n+z^2)})=\text{wilson score} \]</p><p>위의 식을 함수로 구현하면 다음과 같다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># ref: http://www.evanmiller.org/how-not-to-sort-by-average-rating.html</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">best</span>(<span class="params">up, down</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        z = <span class="number">1.96</span>  <span class="comment"># 95% confidence level</span></span><br><span class="line">        n = up + down</span><br><span class="line">        p_up = up / n</span><br><span class="line">        p_down = <span class="number">1</span> - p_up</span><br><span class="line">        denominator = <span class="number">2</span> * (n + z**<span class="number">2</span>)</span><br><span class="line">        numerator = <span class="number">2</span> * n * p_up + z**<span class="number">2</span> - z * np.sqrt(z**<span class="number">2</span> + <span class="number">4</span> * n * p_up * p_down)</span><br><span class="line">        lower = numerator / denominator</span><br><span class="line">    <span class="keyword">except</span> ZeroDivisionError <span class="keyword">as</span> e:</span><br><span class="line">        lower = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(<span class="number">0</span>, lower)</span><br></pre></td></tr></table></figure><p>아래의 예시는 네이버 뉴스 댓글에 Best ranking algorithm을 적용해본결과이다. 공감비율순 정렬이었다면 "<em>원칙대로만 하시면 됩니다 역사에부끄럽지 않게 잘 해 주세요</em>"는 1000개 이상의 vote를 가진 "<em>법대로해라 법은 만인 앞에 평등하다</em>"는 댓글을 제치고 상위에 랭크되었을것이다. 하지만 Best 정렬방식에서는 vote 수가 적은 경우 약간의 penalty를받기 때문에 하위에 랭크되었다.</p><ul><li>MB '정치보복' 반발에 문무일 총장 "법적 절차대로하겠다"<sup id="fnref:9"><a href="#fn:9" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="MB &#39;정치보복&#39; 반발에 문무일 총장 “법적 절차대로 하겠다”, 네이버 뉴스">[9]</span></a></sup></li></ul><table style="width:100%;"><colgroup><col style="width: 59%" /><col style="width: 7%" /><col style="width: 8%" /><col style="width: 10%" /><col style="width: 14%" /></colgroup><thead><tr class="header"><th>comments</th><th>공감수</th><th>비공감수</th><th>best score</th><th>공감비율</th></tr></thead><tbody><tr class="odd"><td>법대로 해라 법은 만인 앞에 평등하다</td><td>1091</td><td>55</td><td>0.938</td><td>0.952006980803</td></tr><tr class="even"><td>법대로 하면 사형인데 !!</td><td>562</td><td>39</td><td>0.936</td><td>0.935108153078</td></tr><tr class="odd"><td>제발 법대로만 해주세요. 그래도 나라를 지옥으로 만든 죄는 물을 법도없다. 이 악마야!!!</td><td>252</td><td>14</td><td>0.933</td><td>0.947368421053</td></tr><tr class="even"><td>지금까지 반발하고 나서 살아남은 넘을 못봤다.</td><td>565</td><td>38</td><td>0.933</td><td>0.936981757877</td></tr><tr class="odd"><td>혓바닥몇번 낼름거릴까나했더니 찔렸나보네ㅎㅎ</td><td>595</td><td>37</td><td>0.932</td><td>0.941455696203</td></tr><tr class="even"><td>본인이 구린짓을 했으니까 먼저 발광하는거겠지..</td><td>686</td><td>43</td><td>0.931</td><td>0.941015089163</td></tr><tr class="odd"><td>법대로 하는 것보다 더 정의로운 절차는 세상에 없다</td><td>4146</td><td>317</td><td>0.926</td><td>0.928971543805</td></tr><tr class="even"><td>당연히 법대로 하셔야죠</td><td>296</td><td>14</td><td>0.921</td><td>0.954838709677</td></tr><tr class="odd"><td>원칙대로만 하시면 됩니다 역사에 부끄럽지 않게 잘 해 주세요</td><td>302</td><td>13</td><td>0.921</td><td>0.95873015873</td></tr><tr class="even"><td>법대로 합시다</td><td>919</td><td>51</td><td>0.92</td><td>0.947422680412</td></tr></tbody></table><p>기본적으로 공감수가 많은 댓글을 상위에 랭크시키는 알고리즘이기 때문에어뷰징 작업으로 공감수가 부풀려진 댓글이 top 10 밖으로 밀려나지는못한다. 하지만 vote수가 적더라도 경향성을 파악해 댓글을 정렬시키기때문에 단순한 순공감이나 공감비율순으로는 하위권에 있던 댓글이 상위권에위치할 기회를 증가시켰다.</p><p>어뷰저 입장에서는 쉽게 계산할 수 있는 정렬방식이 아니기 때문에 조작이어려워질 것이다. 어뷰징을 할 때 고의로 공감과 비공감을 섞어서 해당댓글을 상위에 랭크시키는데, Best 정렬이라면 "적당"한 비율을 맞추기까다로워질 것이다.</p><h3 id="controversial">Controversial</h3><p>controversial<sup id="fnref:10"><a href="#fn:10" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://www.reddit.com/r/NoStupidQuestions/comments/3xmlh8/what_does_something_being_labeled_controversial/?sort=confidence&gt;">[10]</span></a></sup>은 이름 그대로, 공감과 비공감이 팽팽하게 맞서는댓글을 상위에 위치시키려는 알고리즘이다. 단순히 팽팽하기만 하면 공감과비공감이 1:1인 상황과 10:10인 상황이 같다고 생각할 수 있기에 vote수도sorting algorithm에 포함시켜서 10:10이 1:1인 상황보다 더 controversial할수 있도록 만들어졌다.</p><p>아래의 식에서 upvote는 공감을, downvote는 비공감을 의미한다. upvote와downvote의 차이가 같아서 분모가 같아진 경우에는 그 크기가 큰 쪽이 높고,vote의 크기가 같은 경우에는 차이가 작은 쪽이 높다.</p><p>\[ \text{controversial} = \frac{match \times log(match + 1)}{| upvote- downvote | + 1},\text{ where }match=min(upvote, downvote) \]</p><p>python으로 구현한 식이다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">controversial</span>(<span class="params">upvote, downvote</span>):</span><br><span class="line">    <span class="keyword">match</span> = <span class="built_in">min</span>(upvote, downvote)</span><br><span class="line">    top = <span class="keyword">match</span> * math.log(<span class="keyword">match</span> + <span class="number">1</span>)</span><br><span class="line">    bottom = <span class="built_in">abs</span>(upvote - downvote) + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(top) / bottom</span><br></pre></td></tr></table></figure><p>좀 더 직관적인 이해를 돕기 위해 가공한 아래의 예시를 보자.</p><table><thead><tr class="header"><th>upvote</th><th>downvote</th><th>controversial score</th></tr></thead><tbody><tr class="odd"><td>1001</td><td>1000</td><td>3454.38</td></tr><tr class="even"><td>999</td><td>1000</td><td>3450.42</td></tr><tr class="odd"><td>100</td><td>100</td><td>461.52</td></tr><tr class="even"><td>101</td><td>100</td><td>230.76</td></tr><tr class="odd"><td>1000</td><td>700</td><td>15.24</td></tr><tr class="even"><td>130</td><td>100</td><td>14.89</td></tr><tr class="odd"><td>100</td><td>130</td><td>14.89</td></tr><tr class="even"><td>1</td><td>1</td><td>0.69</td></tr><tr class="odd"><td>1</td><td>2</td><td>0.35</td></tr></tbody></table><p>upvote, downvote의 비율이 비슷한 댓글 순서로 정렬되고, 그 비율내에서는 vote 수가 큰 댓글이 더 위에 놓이게 된다.</p><p>controversial algorithm을 네이버 뉴스 댓글에 적용해보았다. 예상대로공감과 비공감 수치가 비슷하면서도 vote수가 많은 댓글이 가장 먼저 보인다.vote수가 작은 이유는 이미 순공감 노출로 인해 vote를 받을 기회를 박탈당한댓글들이기 때문이다.</p><p>수치와는 무관하게 top 10 댓글의 내용은 얼마나 controversial하게구성되어 있는지 정성적으로 평가해보았다. 보도자료에 대한 찬성은 <spanstyle="color:blue">푸른색</span>, 반대는 <spanstyle="color:red">붉은색</span> , 애매한 문장은 표기하지 않았다.controversial하다면 뉴스 기사의 주제에 대해 찬성과 반대가 골고루섞여있어야 할 것이다.</p><ul><li>아베 "한미군사훈련 예정대로"…文대통령 "내정문제 거론곤란"(종합)<sup id="fnref:11"><a href="#fn:11" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="아베 “한미군사훈련 예정대로”…文대통령 “내정문제 거론 곤란”(종합), 네이버 뉴스">[11]</span></a></sup></li></ul><table><colgroup><col style="width: 9%" /><col style="width: 67%" /><col style="width: 9%" /><col style="width: 12%" /></colgroup><thead><tr class="header"><th>userId</th><th>comments</th><th>공감수</th><th>비공감수</th></tr></thead><tbody><tr class="odd"><td>user 1</td><td>대통령 각하, ‘사드 문제’ 갖고 거품무는 중국에도 내정 간섭이라고 거침없이 말씀해주세요</td><td>26</td><td>26</td></tr><tr class="even"><td>user 2</td><td><span style="color:red">이제는 한미일군사훈련을 해야한다.</span></td><td>81</td><td>85</td></tr><tr class="odd"><td>user 3</td><td><span style="color:red">근데 왜 중국한테는 대놓고 내정간섭 받는거죠,대통령님? 치욕스러웠던 조선시대가 그리운건가요?</span></td><td>22</td><td>22</td></tr><tr class="even"><td>user 4</td><td><span style="color:red">봐라 ㅋㅋㅋ 연기하지?철수 얘기나온다 백퍼ㅋㅋㅋㅋ 베트남꼴 나는거야 ㅋㅋㅋ 정신 좀 차리자</span></td><td>33</td><td>31</td></tr><tr class="odd"><td>user 5</td><td><span style="color:red">아베만도못한 문통; </span></td><td>11</td><td>11</td></tr><tr class="even"><td>user 6</td><td><span style="color:red">문재인 아가라 닥쳐라. 사드도 내정문제인데중국한테는 끽소리 못 하던 색히가 어디서 주둥아리 씨부리노. </span></td><td>10</td><td>10</td></tr><tr class="odd"><td>user 7</td><td><span style="color:red">ㅋㅋㅋㅋㅋㅋ 곧 양념단와서 또 평화올림픽울부짖겠네. </span></td><td>66</td><td>57</td></tr><tr class="even"><td>user 8</td><td><span style="color:red">미국이 한국을 버려야 할 듯.없네.</span></td><td>15</td><td>14</td></tr><tr class="odd"><td>user 9</td><td><span style="color:red">미국을 대변하는거다.국익을 최우선으로하는거지싫지만 아베가 똑똑하지않는냐.살자. </span></td><td>8</td><td>8</td></tr><tr class="even"><td>user 10</td><td><span style="color:red">얼마나 답답하면 저런말을 할지 생각안해보셨나요?? 북에서 원하는 대로 흘러가네요. 앞으로 한미군사훈련연기뿐만 아니라 축소되고 없어지고 난리나겠네</span></td><td>8</td><td>8</td></tr></tbody></table><p>분명 공감수와 비공감수는 controversial하지만 대부분이 당시의 여론과반대대는 내용으로 치우쳐있다. 정성적으로 controversial한 댓글은 공감:비공감이 1:1이 아닌 좀더 공감 비율이 높은 비율을 가진다는 사실을유추해볼 수 있다.</p><p>공감비율과 비슷하게 controversial도 vote수가 많은 경우에 불리해진다.controversial의 분모는 upvote와 downvote의 차이값인데, vote수가 많을수록한두개차이를 유지하기가 어려워진다. 공감 66, 비공감 57을 가진 댓글이공감 10, 비공감 10보다 아래에 놓인다.</p><h2 id="new-sorting-algorithms">New Sorting Algorithms</h2><p>reddit ranking algorithm 중에서 controversial의 문제점을 해결한새로운 controversial algorithm과 비공감이 많은 의견도 노출하는 best anti정렬방식을 제안하고자 한다.</p><h3 id="new-controversial">New controversial</h3><p>앞서 지적했듯이 controversial은 공감: 비공감의 비율 재조정과 vote수가 많은 경우 분모값의 기준을 완화시켜야하는 이슈가 있다.</p><ul><li><p><strong>공감 : 비공감</strong> 정성적으로 확인해보았을 때 공감:비공감 = 6.5 : 3.5 정도에서 기사 내용에 대한 찬성과 반대의 댓글이 골고루등장하였다. 때문에 new controversial에서 upvote와 downvote의 값을조정해주어야 한다.</p></li><li><p><strong>vote수가 많은 경우</strong> 이 문제는 공감비율순과비슷했다. upvote와 downvote의 절대치에 의존하기보다 wilson score로도출된 값을 upvote와 downvote로 대체하면 vote수가 많고 적음을고려하면서도 0과 1 사이의 값을 가지게 되어 upvote와 downvote의 차이에대한 효과가 완화된다.</p></li></ul><p>변경된 내용을 정리하면 다음과 같다.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">controversial</span>(<span class="params">upvote, downvote</span>):</span><br><span class="line">    p_up = best(upvote, downvote) * <span class="number">3.5</span></span><br><span class="line">    p_down = best(downvote, upvote) * <span class="number">6.5</span></span><br><span class="line">    <span class="keyword">match</span> = <span class="built_in">min</span>(p_up, p_down)</span><br><span class="line">    top = <span class="keyword">match</span> * math.log(<span class="keyword">match</span> + <span class="number">1</span>)</span><br><span class="line">    bottom = <span class="built_in">abs</span>(p_up - p_down) + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(top) / bottom</span><br></pre></td></tr></table></figure><ul><li>아베 "한미군사훈련 예정대로"…文대통령 "내정문제 거론 곤란"(종합)<sup id="fnref:11"><a href="#fn:11" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="아베 “한미군사훈련 예정대로”…文대통령 “내정문제 거론 곤란”(종합), 네이버 뉴스">[11]</span></a></sup></li></ul><table><colgroup><col style="width: 19%" /><col style="width: 38%" /><col style="width: 19%" /><col style="width: 23%" /></colgroup><thead><tr class="header"><th>userId</th><th>comments</th><th>공감수</th><th>비공감수</th></tr></thead><tbody><tr class="odd"><td>user 11</td><td><span style="color:red">아베한테 대하듯 똑같이 김정은하고 북한,중국한테도 당당하게 나와라! </span></td><td>16</td><td>11</td></tr><tr class="even"><td>user 12</td><td><span style="color:red">개~~새끼 아베 한테는 그렇게 당당하면서김정은한테는 왜 그렇게 꼬리를 내린다냐? 핵이 무섭긴 무서운가보다</span></td><td>11</td><td>7</td></tr><tr class="odd"><td>user 13</td><td><span style="color:blue">한미 동맹도 좋다 그러나 우리 나라 스스로강한 나라가 되어야 한다. 문대통형 수고 많으십니다 !!</span></td><td>9</td><td>6</td></tr><tr class="even"><td>user 14</td><td><span style="color:blue">아베에게 일침을 놔주신문 대통령님 지지합니다.나대지 마시오</span></td><td>9</td><td>6</td></tr><tr class="odd"><td>user 15</td><td><span style="color:blue">쪽바리 추종자들 많네!! 특히 벌레틀딱들~~</span></td><td>8</td><td>5</td></tr><tr class="even"><td>user 16</td><td><span style="color:blue">반대로 우리나라가 일본보고 자위대훈련하는거 보고 참견하면 일본이 가많이 있겠냐?벌레들아! 비판을 하려면국내 내정에 간섭하는 아베를 비판해야지 아베를 두둔하냐? 이스레기들아...</span></td><td>8</td><td>5</td></tr><tr class="odd"><td>user 17</td><td><span style="color:red">아베가 옳은말했네 지금이라고 김정은 참수한미연합훈련을 시작하라 빨갱이한테 이 나라를 줄 수 없다 </span></td><td>6</td><td>4</td></tr><tr class="even"><td>user 18</td><td><span style="color:red">대한민국은 다시 한번 망해봐야정신차리지..안된다.</span></td><td>6</td><td>4</td></tr><tr class="odd"><td>user 19</td><td><span style="color:blue">일본이 우방이란애들 멍청한거 아니냐일본애들도 그렇게 생각안하는데 왜 니혼자 망상해찐따새끼인가ㅋㅋㅋㅋㅋㅋ</span></td><td>6</td><td>4</td></tr><tr class="even"><td>user 20</td><td><span style="color:red">문재인씨 당신의 국적은 어디입니까? 다스실소유주를 밝히는 것보다 훨씬 더 중요한 문제입니다. </span></td><td>6</td><td>4</td></tr></tbody></table><p>공감 비율을 조금 높여주었을 때 기사 내용에 찬성하는 댓글과 반대하는댓글이 top 10에 골고루 섞이게 되었다. 또 wilson score로 변환한 상태에서비율을 조정해주게되어 vote수가 높은 경우에 up과 down의 차이에 덜민감해질 수 있었다.</p><h3 id="best-anti">Best-Anti</h3><p>꼭 공감수가 많은 것만 괜찮은 의견이라고 볼 수 있을까? 비공감수가 많은의견 또한 반대 진영의 입장을 대변하는 좋은 의견이라고도 볼 수 있지않을까?</p><p>네이버 뉴스 댓글은 대부분 당시의 여론에 따라 분위기가 흘러간다.순공감순이든 공감비율순이든 한가지 주장을 다른 방식으로 표현하고 있는댓글들이 top 10이 된다. 이를 보는 대중은 한쪽의 영향만 받게 되어 생각이더욱 치우쳐진다.</p><p>정치적 다양성을 수용하는 것은 의견의 객관성을 유지하는데에 도움이된다. 그런 의미에서 당시 여론과 반대되는 내용의 댓글 또한 보여주는 것은댓글에 영향을 받을 다른 사용자를 위해서도, 플랫폼의 중립성을 담보하기위해서도 중요하다고 생각한다.</p><p>Best-Anti는 negative vote에 대한 Wilson score를 구한 것이다.</p><p>\[ w_{neg}^- = max(0, \frac{2n(1-\hat{p}) + z^2 - z\sqrt{z^2 +4n\hat{p}(1-\hat{p})}}{2(n+z^2)}) \]</p><p>python 구현식은 다음과 같다. <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">best_anti</span>(<span class="params">up, down</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        z = <span class="number">1.96</span>  <span class="comment"># 95% confidence level</span></span><br><span class="line">        n = up + down</span><br><span class="line">        p_up = up / n</span><br><span class="line">        p_down = <span class="number">1</span> - p_up</span><br><span class="line">        denominator = <span class="number">2</span> * (n + z**<span class="number">2</span>)</span><br><span class="line">        numerator = <span class="number">2</span> * n * p_down + z**<span class="number">2</span> - z * np.sqrt(z**<span class="number">2</span> + <span class="number">4</span> * n * p_up * p_down)</span><br><span class="line">        lower = numerator / denominator</span><br><span class="line">    <span class="keyword">except</span> ZeroDivisionError <span class="keyword">as</span> e:</span><br><span class="line">        lower = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(<span class="number">0</span>, lower)</span><br></pre></td></tr></table></figure></p><ul><li>아베 "한미군사훈련 예정대로"…文대통령 "내정문제 거론곤란"(종합)<sup id="fnref:11"><a href="#fn:11" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="아베 “한미군사훈련 예정대로”…文대통령 “내정문제 거론 곤란”(종합), 네이버 뉴스">[11]</span></a></sup></li></ul><table><colgroup><col style="width: 15%" /><col style="width: 50%" /><col style="width: 15%" /><col style="width: 18%" /></colgroup><thead><tr class="header"><th>userId</th><th>comments</th><th>공감수</th><th>비공감수</th></tr></thead><tbody><tr class="odd"><td>user 21</td><td>평화협정후 미군철수 바랍니다</td><td>0</td><td>5</td></tr><tr class="even"><td>user 22</td><td>홍발정씨..트럼프도 좌파 빨갱이죠??</td><td>0</td><td>4</td></tr><tr class="odd"><td>user 23</td><td>늙다리 미치광이는 빠져 줄래!자주통일좀 하자!</td><td>0</td><td>4</td></tr><tr class="even"><td>user 24</td><td>자국당은 사형감 많던데... 미국철수 애기했다고 파면? 자국당 5월에는문정인으로 놀고먹겠군~!</td><td>0</td><td>4</td></tr><tr class="odd"><td>user 25</td><td>봐라. 지도자 하나가 이렇게나 세상을 바꿀 수 있다. 물론 촛불 들고,직접민주주의를 구현한 국민 또한 위대하지. 지방선거 때 투표 잘 하자.</td><td>0</td><td>4</td></tr><tr class="even"><td>user 26</td><td>아직도. 미국이 인계철선이라믿고 50년대 사고방식이 존재하는구나군사력 세계10위안에들고 1-1붙어도 안지니 너무 미군철수로 여론전말고참신한거없어요 ? 자한당분들?</td><td>1</td><td>6</td></tr><tr class="odd"><td>user 27</td><td>극우 자한당은 미국도 빨갱이란다 제비가 왔다고 봄은 아니람서ㅋㅋㅋ</td><td>0</td><td>3</td></tr><tr class="even"><td>user 28</td><td>원샷-빅딜!</td><td>0</td><td>3</td></tr><tr class="odd"><td>user 29</td><td>자한당분들께서 트럼프도 좌파래요..</td><td>0</td><td>3</td></tr><tr class="even"><td>user 30</td><td>잊지마세요 지금도 북한은 세계 최악의 인권유린 국가입니다 이시간에도북한 주민들은 김정은한테 총살당하거나 아오지탄광으로 끌려가고 있습니다북한 여성들은 김정은의 성노예가 되고 있구요 대한한공 조현민의 갑질화가나죠 미투운동으로 드러난 권력자들의 성폭력 정말 싫습니다 그런데이것보다 수백배는 더심한 갑질과 성폭력을 일삼는게 북한 김정은입니다</td><td>0</td><td>3</td></tr></tbody></table><h2 id="conclusions">Conclusions</h2><p>현재의 네이버 뉴스 댓글 정렬방식은 공감수가 높은 댓글을 위주로보여주고 있고, 기준 또한 쉽다. 조작에 들어가는 비용 대비 얻을 수 있는효과가 큰 상황에서 조작으로 인해 이익을 볼 집단은 당연히 어뷰징을 할 수밖에 없다. 그리고 이미 조직적인 세력이 되어버린 어뷰저들은 완벽히 차단할수 없다. 때문에 어뷰징을 해결할 수 있는 가장 좋은 방법은 현재의 정렬방식의 단점을 극복하면서 자연스럽게 기준이 복잡해지게 만드는 것과사람들이 조작된 의견에 크게 흔들리지 않을 수 있도록 다양한 의견을보여주는 것이다.</p><p>현재의 네이버 정렬 방식 중 순공감순과 공감비율순이 가지는 한계는reddit에서 사용하고 있는 best 정렬방식으로 해결된다. 공감수에 가중치를둔 정렬방식 외에 공감수와 비공감수가 비슷한 댓글에 가중치를 두는 방식,비공감수에 가중치를 두는 방식을 제안하였다.</p><p>한 쪽의 의견만 듣는 것은 언제나 편향된 결과를 야기한다고 생각한다. 한쪽이 명백히 잘못한 것처럼 보도될 때, 그 반대의 의견에도 귀를 기울일 수있는 플랫폼이 되길 바란다.</p><h2 id="future-works">Future works</h2><p>지금까지는 댓글의 contents보다는 댓글에 부과된 공감, 비공감의interaction 데이터로 문제점과 해결방식을 제안했다. controversial로의견의 다양성을 추구했지만 text를 보지 않았기 때문에 의견의 다양성을간접적으로 보장하기엔 불안정할 수 있다.</p><p>쇼핑 리뷰에서 가격, 내구성, 디자인 등 다양한 측면을 보여주듯이 정치적의견도 기사에서 언급된 중요한 단어들에 대한 사람들의 반응을 보는 방식도생각해보면 좋을 것 같다.</p><h2 id="appendix-a-wilson-score">Appendix A: Wilson score</h2><p>사실 본문에서 기술한 내용은 일반적인 Normal approximationinterval이다.</p><p>\[ p = \hat{p} \pm z\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \]</p><p>여기서 \(\)은 Bernoulli process의 성공확률을 의미한다.</p><p>Wilson score는 confidence interval을 \(\)가 아닌 \(p\)로 추정한 scoreinterval의 최소값이다.</p><p>\[ p = \hat{p} \pm z\sqrt{\frac{p(1-p)}{n}} \]</p><p>\(p\)에 대해 정리하여 \(p\)에 대한 2차방정식을 만든다.</p><p>\[ (1 + \frac{z^2}{n}) p^2 - (2\hat{p} + \frac{z^2}{n})p + \hat{p}^2= 0 \]</p><p>근의 공식을 사용해 \(p\)를 구한다.</p><p>\[ p = \frac{2n\hat{p} + z^2 \pm z\sqrt{z^2 +4n\hat{p}(1-\hat{p})}}{2(n+z^2)} \]</p><p>Wilson score는 \(p\)의 lower bound이므로 - 에 대해 정리하면 다음과같다.</p><p>\[ w^- = max(0, \frac{2n\hat{p} + z^2 - z\sqrt{z^2 +4n\hat{p}(1-\hat{p})}}{2(n+z^2)}) = \text{wilson score} \]</p><p>95%의 신뢰도로 고정하는 경우 z에 1.96을 대입할 수 있다. 그리고 이경우 본문의 python 함수에서 구현한 <code>best</code>가 된다.</p><h2 id="references">References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;">2017년11월 30일부터 호감순에서 순공감순으로 변경되면서 다시 2016년 이전의호감순처럼 호감도를 “공감-비공감”으로 계산하게되었다.<a href="#fnref:1" rev="footnote"> ↩︎</a></span></li><li id="fn:2"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;">홍준표“나경원, 아들 이중국적 여부 밝혀라…1억 피부과 연상”, 네이버뉴스<a href="#fnref:2" rev="footnote"> ↩︎</a></span></li><li id="fn:3"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;">재판에넘겨진 조국 부인 정경심 교수…검찰 '소환 임박', 네이버뉴스<a href="#fnref:3" rev="footnote"> ↩︎</a></span></li><li id="fn:4"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;">대학교수이어 의사 4400명도 “조국 퇴진, 조국 딸 퇴교” 시국선언문 서명, 네이버뉴스<a href="#fnref:4" rev="footnote"> ↩︎</a></span></li><li id="fn:5"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://redditblog.com/2009/10/15/reddits-new-comment-sorting-system">https://redditblog.com/2009/10/15/reddits-new-comment-sorting-system</a><a href="#fnref:5" rev="footnote">↩︎</a></span></li><li id="fn:6"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="http://www.evanmiller.org/how-not-to-sort-by-average-rating.html">http://www.evanmiller.org/how-not-to-sort-by-average-rating.html</a><a href="#fnref:6" rev="footnote">↩︎</a></span></li><li id="fn:7"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">7.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval">https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval</a><a href="#fnref:7" rev="footnote">↩︎</a></span></li><li id="fn:8"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">8.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://blog.yelp.com/2011/02/the-most-romantic-city-on-yelp-is">https://blog.yelp.com/2011/02/the-most-romantic-city-on-yelp-is</a><a href="#fnref:8" rev="footnote">↩︎</a></span></li><li id="fn:9"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">9.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;">MB'정치보복' 반발에 문무일 총장 “법적 절차대로 하겠다”, 네이버뉴스<a href="#fnref:9" rev="footnote"> ↩︎</a></span></li><li id="fn:10"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">10.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.reddit.com/r/NoStupidQuestions/comments/3xmlh8/what_does_something_being_labeled_controversial/?sort=confidence">https://www.reddit.com/r/NoStupidQuestions/comments/3xmlh8/what_does_something_being_labeled_controversial/?sort=confidence</a><a href="#fnref:10" rev="footnote">↩︎</a></span></li><li id="fn:11"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">11.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;">아베“한미군사훈련 예정대로”…文대통령 “내정문제 거론 곤란”(종합), 네이버뉴스<a href="#fnref:11" rev="footnote"> ↩︎</a></span></li></ol></div></div>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;어뷰저는 존재한다. 하지만 그들을 완전히 통제하고 제거하는 것은
불가능하다. 그렇다면 어뷰저를 막는 것에 집중하지 말고, 어뷰징은
내버려두되 그 효과를 랭킹 시스템을 바꾸어 완화시키는 방법은 어떨까?&lt;/p&gt;
&lt;p&gt;네이버 뉴스 플랫폼에서는 순공감과 공감비율을 통해 공감이 많은 의견을
상위에 랭크시키고 있다. 각각의 알고리즘이 가진 결함도 문제지만, 과연
정치적 의견의 장에서 공감이 많은 의견만이 우리가 듣고 보아야 할
의견일까? 특히나 어뷰징이 있는 상황에서 공감수가 많은 의견은 더욱
획일화된 주장을 펼칠 수 밖에 없으며 대중은 편향된 의견만 접하게 되어
무의식적으로 다양한 사고에 대한 가능성을 차단받는다.&lt;/p&gt;
&lt;p&gt;그래서 이번 글에서는 다양한 의견이 상위에 랭크될 수 있는 sorting
algorithm들을 제안한다. reddit과 yelp 등에서 사용하고 있는 알고리즘을
비롯하여, 논쟁적인 댓글이 상위에 위치할 수 있는 알고리즘, 그리고
비공감이 많은 댓글에 더 높은 점수를 부여하는 알고리즘 3가지를 소개한다.</summary>
    
    
    
    <category term="Tech" scheme="https://www.thespacemoon.com/categories/Tech/"/>
    
    <category term="ML" scheme="https://www.thespacemoon.com/categories/Tech/ML/"/>
    
    
    <category term="data analysis" scheme="https://www.thespacemoon.com/tags/data-analysis/"/>
    
  </entry>
  
  <entry>
    <title>Naver News Comment Analysis (2)</title>
    <link href="https://www.thespacemoon.com/2019/08/03/naver-news-comments-analysis-2/"/>
    <id>https://www.thespacemoon.com/2019/08/03/naver-news-comments-analysis-2/</id>
    <published>2019-08-03T08:59:00.000Z</published>
    <updated>2019-08-03T08:59:00.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><span style="color:red">NOTICE: 앞으로 소개될 내용은 NAVER와무관하며, 오히려 NAVER 뉴스가 정치적인 편향성을 가지고 있지 않은중립적인 플랫폼이라고 생각하기 때문에 분석을 하게 되었음을알립니다.</span></p><h2 id="tldr">TL;DR</h2><p>2015년 12월부터 2018년 5월까지의 데이터로 소위 말하는 어뷰저의 존재를확인해보았다. 여기서 말하는 어뷰저의 criteria는 다음과 같다.</p><ol type="1"><li>타인의 생각에 영향을 미칠 수 있도록 작성한 댓글이 top 10 내에 한 번이상 들었어야 한다. 실제 어뷰저였어도 top 댓글이 아니어서 타인에게영향을 미치지 못했다면 어뷰저라고 불릴 자격(?)이 없다.</li><li>발생하기 어려운 패턴을 보여야 한다.</li></ol><p>이 기준에 따라 분석한 결과, 총 386번 댓글을 남겼고 그 중에서369번(95.6%) top 10 내에 들었던 유저와 총 289번 댓글을 남기고269번(93.1%) top 10 내에 들었던 유저를 의심해보게 되었다.<span id="more"></span></p><h2 id="abuser-who-are-you">Abuser, who are you?</h2><h3 id="introduction">Introduction</h3><p><strong>"정말 2016년 4분기부터 정말 댓글 조작을 했던 사용자들이있었을까?"</strong> 라는 단순한 의문과 궁금증에서 분석을 시작하게되었다. 다만, 데이터에 <공감> <비공감> 을 눌렀던 interaction 정보가누락되어 있기에 (이 데이터는 네이버 뉴스 측에서 제공해주지 않는 이상얻을 수 없다) 적어도 댓글을 한 번이라도 남겼던 사용자에 대해서만어뷰저로 의심해 볼 수 있었다.</p><p>label이 없는 상황에서 어뷰저를 특정짓는 것과 그 사용자가 어뷰저임을다른 사람에게 설득하는 것은 어려운 일이다. 또한 무죄추정의 원칙에 의거해댓글 작성자를 함부로 어뷰저라고 단정지을 수도 없었다. 그래서 이번분석에서는, "모든 작성자는 어뷰저가 아니다." 라는 가정을 기반으로 특정패턴이 등장할 확률을 계산해서 <strong>어뷰저였을 가능성</strong>을간접적으로 추측하는 방식을 취했다.</p><p>이 과정에서 누군가는 그 정도 확률로는 어뷰저라고 단정짓기 어렵다고판단할 수도 있고, 아닐 수도 있다. 또 추가적인 분석 결과가 있다면 어뷰저가능성이 더 높아질 수도 있다. 후자라면 언제든 댓글로 추가 분석할 내용을요청했으면 하는 바람이다.</p><h3 id="abuser-criteria">Abuser Criteria</h3><p>어뷰저는 어떤 존재일까? 이에 답하기 앞서, 어뷰징의 목적과 어뷰징이문제가 되는 상황에 대해 먼저 정리해보았다.</p><h4 id="어뷰징의-목적">어뷰징의 목적</h4><p>어뷰저들의 목표는 네이버의 댓글 정렬 기준에 맞추어 10위권 내에 드는것이다. 네이버 뉴스의 UI 상, top 10 내에 들면 그 기사를 읽는 누구나 쉽게그 댓글의 내용에 접하게 되기 때문이다. 그리고 그 내용이 대중을대표한다고 생각하기 때문에 쉽게 타인의 생각에 영향을 미칠 수 있다.</p><h4 id="어뷰징이-문제가-되는-상황">어뷰징이 문제가 되는 상황</h4><p>어뷰징이 문제가 되었던 이유는 공정하고 자연스러운 방식으로집계되었다고 믿었던 top 10 댓글이 실제로는 어떤 세력에 의해 의도를가지고 조작되었기 때문이었다. top 댓글이 특정 집단에 의해 조작되었다면,그것들이 과연 네이버 뉴스 플랫폼에 참여하는 사용자들의 생각을 대표하는댓글이라고 볼 수 있을까?</p><p>그래서, 이 글에서 이야기 할 어뷰저의 criteria는 다음과 같다.</p><blockquote><p>어뷰징의 목적을 달성해야 한다. 즉, 타인의 생각에 영향을 미칠 수있도록 <strong>작성한 댓글이 top 10 내에 한 번 이상 들었어야한다</strong>. 실제 어뷰저였어도 top 댓글이 아니어서 타인에게 영향을미치지 못했다면 어뷰저라고 불릴 자격(?)이 없다.<del><em>(안습)</em></del></p></blockquote><blockquote><p>자연적으로 발생하기 어려운, <strong>확률이 낮은 패턴이등장</strong>해야 한다.</p></blockquote><h3 id="data-preprocessing">Data preprocessing</h3><a href="/2019/07/25/naver-news-comments-analysis-1/" title="Naver News Comment Analysis (1)">Naver News Comment Analysis (1)</a><p>에서 사용했던 데이터에서 추가로 필터링이 필요했다. 크롤링한 댓글데이터에 hashing 된 아이디가 포함된 것이 2015년 12월 이후였기때문이다.</p><ul><li>사용한 댓글 데이터 기간: 2015.12.08 ~ 2018.05.25</li></ul><h3 id="analysis">Analysis</h3><p>먼저, <strong>정치</strong> 분야에서 댓글이 top 10 내에 들었던 횟수를작성자 별로 집계한 후, 횟수가 높은 순서대로 정렬하였을 때의 추이를살펴보았다.</p><p><img src="/assets/images/politics_top_user.png?style=centerme" width=50%><br></p><table><thead><tr class="header"><th>mean</th><th>stdev</th><th>max</th><th>75%</th><th>med</th><th>min</th></tr></thead><tbody><tr class="odd"><td>2.240442</td><td>4.607621</td><td>369</td><td>2</td><td>1</td><td>1</td></tr></tbody></table><p><br> 대부분의 작성자는 1~2번 정도 댓글이 top 10 내에 드는 반면, 일부사용자들은 100번 이상 순위권 내에 든다. 이 그래프만 본다면 자주 top 10에드는 사용자들 모두가 의심스러울 수 있지만 이런 skewed graph는 대부분의사회과학 데이터에서 발견되므로 이들을 어뷰저로 속단하긴 이르다. 검증을위해 다른 섹션(사회, 경제, 문화, IT, 세계)에 대해서도 마찬가지 방법으로그래프를 그려보았다.</p><figure><img src="/assets/images/paretto.png" alt="갓 파레토..." /><figcaption aria-hidden="true">갓 파레토...</figcaption></figure><p>자주 순위권 내에 드는 댓글을 작성한 사용자를 <strong><em>topuser</em></strong> 라고 했을 때, 다른 분야에서도 top user는 쉽게 찾아볼수 있었다. 어쩌면 이들은 (어뷰저가 아닌 이상) 네이버 뉴스 플랫폼에서높은 "공감수-비공감수"를 받을 수 있는 전략이 학습된 것은 아닐까? 기사가나오고 얼마 지나지 않아 댓글을 남기거나, 그 당시 분위기에 맞는 댓글의내용을 남기거나, 사실로 보여지는 데이터와 함께 댓글을 작성하거나 하는 등자신만의 전략이 있을 것이다.</p><p>그러나 이 전략들이 100%의 확률로(=항상) 통하지는 않았을 것이다.때로는 일찍 댓글을 작성했음에도 뒤늦게 작성한 댓글이 폭발적인 공감을이끌어내서 top 10에 들지 못했을 수도 있고, 당시의 전반적인 분위기에탑승하는 댓글을 남겼음에도 다른 댓글 중에 두드러지지 못해 공감을 받지못했을 수도 있다.</p><p>top user 간에 일반적인 top 10 성공률이 존재할 것이고 이는 normaldistribution을 따른다는 가설을 바탕으로 "top user가 작성한 전체 댓글 수대비 top 10에 들었던 댓글 수(=top 10 성공률)"를 계산해보았다.</p><h4 id="정치-top-users">정치 top users</h4><table><thead><tr class="header"><th>userId</th><th>top comment #</th><th>total comment #</th><th>top 10 성공률 (%)</th></tr></thead><tbody><tr class="odd"><td><strong>user 1</strong></td><td>369</td><td>386</td><td><strong>95.60</strong></td></tr><tr class="even"><td>user 2</td><td>339</td><td>380</td><td>89.21</td></tr><tr class="odd"><td><strong>user 3</strong></td><td>269</td><td>289</td><td><strong>93.08</strong></td></tr><tr class="even"><td>user 4</td><td>178</td><td>610</td><td>29.18</td></tr><tr class="odd"><td>user 5</td><td>178</td><td>1090</td><td>16.33</td></tr><tr class="even"><td>user 6</td><td>175</td><td>424</td><td>41.27</td></tr><tr class="odd"><td>user 7</td><td>174</td><td>818</td><td>21.27</td></tr><tr class="even"><td>user 8</td><td>155</td><td>316</td><td>49.05</td></tr><tr class="odd"><td>user 9</td><td>143</td><td>950</td><td>15.05</td></tr><tr class="even"><td>user 10</td><td>141</td><td>583</td><td>24.19</td></tr></tbody></table><p><br></p><h4 id="경제-top-users">경제 top users</h4><table><thead><tr class="header"><th>userId</th><th>top comment #</th><th>total comment #</th><th>top 10 성공률 (%)</th></tr></thead><tbody><tr class="odd"><td>user 11</td><td>289</td><td>1185</td><td>24.39</td></tr><tr class="even"><td>user 12</td><td>226</td><td>2935</td><td>7.70</td></tr><tr class="odd"><td>user 13</td><td>219</td><td>1656</td><td>13.22</td></tr><tr class="even"><td>user 14</td><td>183</td><td>1636</td><td>11.19</td></tr><tr class="odd"><td>user 15</td><td>173</td><td>1378</td><td>12.55</td></tr><tr class="even"><td>user 16</td><td>161</td><td>989</td><td>16.28</td></tr><tr class="odd"><td>user 17</td><td>160</td><td>654</td><td>24.46</td></tr><tr class="even"><td>user 18</td><td>157</td><td>2589</td><td>6.06</td></tr><tr class="odd"><td>user 19</td><td>139</td><td>1514</td><td>9.18</td></tr><tr class="even"><td>user 20</td><td>127</td><td>742</td><td>17.12</td></tr></tbody></table><p><br></p><h4 id="사회-top-users">사회 top users</h4><table><colgroup><col style="width: 14%" /><col style="width: 26%" /><col style="width: 29%" /><col style="width: 29%" /></colgroup><thead><tr class="header"><th>userId</th><th>top comment #</th><th>total comment #</th><th>top 10 성공률 (%)</th></tr></thead><tbody><tr class="odd"><td>user 21</td><td>366</td><td>953</td><td>38.41</td></tr><tr class="even"><td>user 22</td><td>308</td><td>1636</td><td>18.83</td></tr><tr class="odd"><td>user 23</td><td>271</td><td>935</td><td>28.98</td></tr><tr class="even"><td>user 24</td><td>241</td><td>1254</td><td>19.22</td></tr><tr class="odd"><td>user 25</td><td>233</td><td>1656</td><td>14.07</td></tr><tr class="even"><td>user 26</td><td>204</td><td>328</td><td>62.20</td></tr><tr class="odd"><td>user 27</td><td>191</td><td>719</td><td>26.56</td></tr><tr class="even"><td>user 28</td><td>168</td><td>625</td><td>26.88</td></tr><tr class="odd"><td>user 29</td><td>149</td><td>1190</td><td>12.52</td></tr><tr class="even"><td>user 30</td><td>148</td><td>1489</td><td>9.94</td></tr></tbody></table><p><br></p><h4 id="문화-top-users">문화 top users</h4><table><colgroup><col style="width: 14%" /><col style="width: 26%" /><col style="width: 29%" /><col style="width: 29%" /></colgroup><thead><tr class="header"><th>userId</th><th>top comment #</th><th>total comment #</th><th>top 10 성공률 (%)</th></tr></thead><tbody><tr class="odd"><td>user 31</td><td>373</td><td>1636</td><td>22.80</td></tr><tr class="even"><td>user 32</td><td>367</td><td>935</td><td>39.25</td></tr><tr class="odd"><td>user 33</td><td>301</td><td>1417</td><td>21.24</td></tr><tr class="even"><td>user 34</td><td>243</td><td>890</td><td>27.30</td></tr><tr class="odd"><td>user 35</td><td>220</td><td>1656</td><td>13.29</td></tr><tr class="even"><td>user 36</td><td>188</td><td>1943</td><td>9.68</td></tr><tr class="odd"><td>user 37</td><td>178</td><td>3245</td><td>5.49</td></tr><tr class="even"><td>user 38</td><td>172</td><td>2738</td><td>6.28</td></tr><tr class="odd"><td>user 39</td><td>164</td><td>200</td><td>82.00</td></tr><tr class="even"><td>user 40</td><td>151</td><td>719</td><td>21.00</td></tr></tbody></table><p><br></p><h4 id="it-top-users">IT top users</h4><table><colgroup><col style="width: 14%" /><col style="width: 26%" /><col style="width: 29%" /><col style="width: 29%" /></colgroup><thead><tr class="header"><th>userId</th><th>top comment #</th><th>total comment #</th><th>top 10 성공률 (%)</th></tr></thead><tbody><tr class="odd"><td>user 41</td><td>714</td><td>3123</td><td>22.86</td></tr><tr class="even"><td>user 42</td><td>572</td><td>3886</td><td>14.72</td></tr><tr class="odd"><td>user 43</td><td>442</td><td>2287</td><td>19.33</td></tr><tr class="even"><td>user 44</td><td>399</td><td>1468</td><td>27.18</td></tr><tr class="odd"><td>user 45</td><td>231</td><td>810</td><td>28.52</td></tr><tr class="even"><td>user 46</td><td>234</td><td>3010</td><td>7.77</td></tr><tr class="odd"><td>user 47</td><td>275</td><td>1622</td><td>16.95</td></tr><tr class="even"><td>user 48</td><td>317</td><td>1493</td><td>21.23</td></tr><tr class="odd"><td>user 49</td><td>346</td><td>2349</td><td>14.73</td></tr><tr class="even"><td>user 50</td><td>364</td><td>1185</td><td>30.72</td></tr></tbody></table><p><br></p><h4 id="세계-top-users">세계 top users</h4><table><colgroup><col style="width: 14%" /><col style="width: 26%" /><col style="width: 29%" /><col style="width: 29%" /></colgroup><thead><tr class="header"><th>userId</th><th>top comment #</th><th>total comment #</th><th>top 10 성공률 (%)</th></tr></thead><tbody><tr class="odd"><td>user 51</td><td>237</td><td>1636</td><td>14.49</td></tr><tr class="even"><td>user 52</td><td>214</td><td>1432</td><td>14.94</td></tr><tr class="odd"><td>user 53</td><td>145</td><td>615</td><td>23.58</td></tr><tr class="even"><td>user 54</td><td>148</td><td>1709</td><td>8.66</td></tr><tr class="odd"><td>user 55</td><td>155</td><td>611</td><td>25.37</td></tr><tr class="even"><td>user 56</td><td>156</td><td>1076</td><td>14.50</td></tr><tr class="odd"><td>user 57</td><td>162</td><td>864</td><td>18.75</td></tr><tr class="even"><td>user 58</td><td>165</td><td>2778</td><td>5.94</td></tr><tr class="odd"><td>user 59</td><td>165</td><td>1254</td><td>13.16</td></tr><tr class="even"><td>user 60</td><td>175</td><td>575</td><td>30.43</td></tr></tbody></table><p><br></p><p>top user의 top 10 성공률을 확률 변수 X라고 했을 때의 histogram과 모든유저가 전략을 바탕으로 활동하는 그룹이라고 가정했을 때 Gaussiandistribution으로 추정한 확률 분포이다. (Gaussian Mixture Model로distribution fitting한 결과는 <ahref="#Appendix-A-Gaussian-Mixture-Model-Fitting">Appendix A</a>참고)</p><p><img src="/assets/images/abuser-gaussian.png?style=centerme" width=80% alt="mean: 25.0, std: 20.5"><br></p><p>정치 섹션에서만 유일하게 <code>ratio &gt; 90%</code> 인 topuser(<code>user 1</code>, <code>user 3</code>)가 존재했으며 이들의 top10 전략 성공률은 다른 top user 대비 발생하기 어려울 정도로 (0.0053%,0.0079%) 높다고 해석할 수 있다.</p><p>숫자 이면의 패턴을 보기 위해 전체 정치면 기사들의 댓글 수와<code>user 1</code>, <code>user 3</code>의 전체 댓글 수, top 10 내에 든댓글 수를 그래프로 시각화 해보았다.</p><ul><li>c.f. 2016.3 ~ 2018.5 까지 굵직한이슈들<sup id="fnref:1"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://ko.wikipedia.org/wiki/2016년_대한민국&gt;">[1]</span></a></sup><sup id="fnref:2"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://ko.wikipedia.org/wiki/2017년_대한민국&gt;">[2]</span></a></sup><sup id="fnref:3"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://ko.wikipedia.org/wiki/2018년_대한민국&gt;">[3]</span></a></sup><ul><li>이세돌 vs. 알파고 (2016.3)</li><li>옥시 (2016.4)</li><li>최순실 태블릿 pc (2016.10)</li><li>박근혜 탄핵 소추안 (2016.12)</li><li>사드배치 / 박근혜 수감 / 세월호 인양(2017.3)</li><li>19대 대통령 선거 (문재인 당선) (2017.5)</li><li>이대목동 신생아 사망 (2017.12)</li><li>평창 동계 올림픽 (2018.2)</li><li>이명박 수감 (2018.3)</li><li>드루킹 (2018.4)</li><li>남북1차정상회담 <span class="citation"data-cites="판문점">@판문점</span> (2018.4)</li><li>남북2차정상회담 (2018.5)</li></ul></li></ul><h4 id="user-1-ratio-96"><code>user 1</code> (ratio: 96%)</h4><p><img src="/assets/images/user1.png?style=centerme" width=90%><br></p><p><span style="color:darkgray">회색 line</span>이 정치면 기사 댓글,<span style="color:blue">파란색 line</span>이 작성자가 쓴 전체 댓글 수,<span style="color:green">초록색 line</span>이 작성자가 쓴 댓글 중 top10 내에 들었던 댓글 수를 나타낸다.</p><p><code>user 1</code>이 주로 활동했던 시기는, 최순실 태블릿 pc 사건,박근혜 탄핵 및 19대 대통령 선거, 평창 동계올림픽 및 MB 다스 사건과맞물려 있었다. 댓글 내용을 시기 별로 뜯어보면, 다음과 같다.</p><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>title</th><th>article date</th><th>user top comments</th></tr></thead><tbody><tr class="odd"><td>朴대통령, '29일까지 대면조사' 檢 요청에 사흘째 묵묵부답</td><td>2016-11-25 15:12:00</td><td>대통령인 자가 자신의 관할하에 있는 검찰을 부정한다면 곧 국가도부정하겠다는 의미다. 이런 대통령은 더이상 대한민국 대통령이 아니다.</td></tr><tr class="even"><td>차은택 변호인 "차씨, 최순실 지시로 김기춘 실장 공관서 면담"</td><td>2016-11-27 16:04:00</td><td>김기춘의 진두지휘하에 박근혜 정권의 모든 불법들이 자행되었다. 정말악마같은 인간이다.</td></tr><tr class="odd"><td>변호인 "차은택, 崔 지시로 김기춘 만나…우병우 장모와골프도"(종합)</td><td>2016-11-27 16:46:00</td><td>김기춘의 진두지휘하에 박근혜 정권의 불법들이 자행되었다.구속수사해서 감옥에서 못나오게 만들어야 한다. 공작정치부터 공안탄압정경유착의 죄를 물어야 한다.</td></tr><tr class="even"><td></td><td></td><td></td></tr><tr class="odd"><td>청문회장도 지배한 '촛불민심'…與野 '재벌 봐주기' 없었다</td><td>2016-12-06 12:34:00</td><td>뇌물이었다는 진술을 이끌어내기에는 힘들것 같다. 지들이 감옥갈일이니까... 강요죄는 확실할듯...</td></tr><tr class="even"><td>재벌 총수들 "청와대 거절 어려워"…하나같이 대가성 부인</td><td>2016-12-06 12:45:00</td><td>뇌물이었다는 진술을 이끌어내기에는 힘들것 같다. 지들이 감옥갈일이니까... 강요죄는 확실할듯...</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>박한철 前소장 한표, '캐스팅보트' 될뻔한 아슬아슬 상황 나올까</td><td>2017-03-04 08:00:00</td><td>탄핵 기각에 표 던지면 그게 판사냐? 재판정에서 궤변을 늘어놓은 박측대리인들의 안하무인에 손을 들어준다면 판사이기를 포기하고 박근혜 부역에동참하겠다는 선언일 뿐이다.</td></tr><tr class="odd"><td>85시간 재판, 속기록 3000쪽…탄핵심판 이번주 결론날까</td><td>2017-03-05 09:00:00</td><td>탄핵 기각에 표 던지면 그게 판사냐? 재판정에서 궤변을 늘어놓은 박측대리인들의 안하무인에 손을 들어준다면 판사이기를 포기하고 박근혜 부역에동참하겠다는 선언일 뿐이다.</td></tr><tr class="even"><td></td><td></td><td></td></tr><tr class="odd"><td>[리얼미터] 다자 文 42.6% vs 安 37.2%…양자 文 47.6% vs 安 43.3%</td><td>2017-04-10 09:15:00</td><td>여론몰이에 흔들릴 국면이 아니다. 무쏘의 뿔처럼 나아가면 야합은흩어지고 굳건함이 승리할 것이다.</td></tr><tr class="even"><td>文 "김부겸 동지 미안하다…꼭 국민통합 해내겠다"</td><td>2017-04-22 08:01:00</td><td>김부겸의 진심이 느껴지고 그를 위로하고 뜻을 같이 하는 문재인의진심도 느껴진다. 남자들에게 이런 동지애는 죽음도 불사하게 만드는마력과도 깉다. 조~오타!!!</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>文 대통령 "내게 반대하라" 파격적 수석회의 시동(상보)</td><td>2017-05-25 12:56:00</td><td>요새 대통령의 행동과 지시사항을 보면 정말 준비된 겸손한 사람이란게진솔하게 느껴진다. 대한민국 국민인게 자랑스럽고 행복해진다.</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>文대통령 "사드 임시배치, 현재 정부가 취할 수 있는 최선의조치"(종합)</td><td>2017-09-08 21:13:00</td><td>국가의 지도자는 자신의 굳은 신념까지도 국가와 국민을 위해 잠시접어야할 용기가 필요할 때가있다. 그 지도자라고 왜 자신의 신념을 꺾음에자괴감과 고민이 없겠는가? 그는 자신을 지지하는 사람들만의 지도자가아니라 대한민국의 지도자이기 때문이다. 그의 고뇌찬 결단을 위로하며지켜보고 힘을 실어주고 싶다.</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>與 "안철수, 나라다운 나라 만드는 일 폄훼 말라"</td><td>2017-11-04 16:10:00</td><td>명버기 구하기에 혈안이 된 명바기 아바타!!!</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>김정은 위원장 "이른 시일내 만날 용의"…문 대통령에 방북요청(종합)</td><td>2018-02-10 15:56:00</td><td>남북 정상회담애서 허심탄회하게 모든 할 말 다해서 기필코 한반도비핵화와 평화를 이루어야 한다.</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>[현장영상] 방미 특사단, 트럼프 대통령 면담 결과 발표</td><td>2018-03-09 09:11:00</td><td>한반도 평화가 세계 평화다. 이런 평화 모드가 얼마만인가...</td></tr></tbody></table><ul><li>같은 내용의 댓글이 top 에 오른 것도 확인할 수 있었다.<ul><li>다른 섹션에 같은 댓글을 남기고도 top에 오른 적도 있다. (아래 표참고)</li></ul><table><colgroup><col style="width: 25%" /><col style="width: 25%" /><col style="width: 25%" /><col style="width: 25%" /></colgroup><thead><tr class="header"><th>title</th><th>section</th><th>article date</th><th>top user comments</th></tr></thead><tbody><tr class="odd"><td>[현장영상] 박근혜 前 대통령 법원으로 출발</td><td>society</td><td>2017-03-30 10:18:00</td><td>최고의 보안과 경호를 철저히 받을 수 있는 구치소를 거쳐 교도소로직행하면 나라도 안정되고 국민들도 평안하고 박근혜 자신도 언론의감시로부터 벗어날수 있다.</td></tr><tr class="even"><td>구속 갈림길에 선 박근혜 '웅변 대신 침묵' 선택</td><td>politics</td><td>2017-03-30 10:22:00</td><td>최고의 보안과 경호를 철저히 받을 수 있는 구치소를 거쳐 교도소로직행하면 나라도 안정되고 국민들도 평안하고 박근혜 자신도 언론의감시로부터 벗어날수 있다.</td></tr></tbody></table></li></ul><p><br></p><h4 id="user-3-ratio-93"><code>user 3</code> (ratio: 93%)</h4><p><img src="/assets/images/user3.png?style=centerme" width=90%><br></p><p><code>user 3</code> 의 주 활동 시기는 사드배치, 평창 동계올림픽 및북미회담과 맞물려있었다. 댓글을 자세히 보면 아래와 같다.</p><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th>title</th><th>article date</th><th>user top comments</th></tr></thead><tbody><tr class="odd"><td>정상회담 돌발 변수는 '사드'…청 "모든 가능성 준비"</td><td>2017-06-25 20:20:00</td><td>국익과국가안보가최우선입니다~~~~</td></tr><tr class="even"><td>송영무 "사드, 비준 아닌 국회 검증…고액연봉·음주운전 송구"(종합)</td><td>2017-06-28 12:18:00</td><td>이유미녹취록에맛짱구치고놀아난언론은×</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>이유미-이준서 중 한 명은 거짓말…윗선 수사 불가피</td><td>2017-06-28 20:52:00</td><td>이유미녹취록에맛장구치고놀아난언론은~~~~????</td></tr><tr class="odd"><td>軍, 송영무 인사청문회서 공개된 '군사기밀 유출' 조사 착수</td><td>2017-06-29 12:24:00</td><td>자유당놈들답다도둑놈들</td></tr><tr class="even"><td></td><td></td><td></td></tr><tr class="odd"><td>文대통령, 내일 트럼프와 만난다…취임 후 첫 韓美정상회담</td><td>2017-06-29 13:54:00</td><td>국익과국가안보가최우선입니다부디좋은결과있으시길~</td></tr><tr class="even"><td>[단독] '제보 조작' 수사망 좁혀오자 安 독대한 이준서…왜?</td><td>2017-06-29 20:19:00</td><td>철수야~ 깜빵갈시간이다가오네~~~~</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>트럼프, 文대통령 부부에 백악관 사적공간 '트리티 룸'깜짝공개(종합)</td><td>2017-06-30 13:23:00</td><td>문재인대통령님~ 멋저부러요~♡♡♡</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>한반도 이슈서 '주도권' 확보 성과…한미FTA 재협상 '숙제'(종합)</td><td>2017-07-01 10:05:00</td><td>국익과국가안보가최우선입니다부디좋은결과있으시길~~~~~~~~♡♡</td></tr><tr class="odd"><td></td><td></td><td></td></tr><tr class="even"><td>남북 "4월말 정상회담 판문점서 개최"…특사단 발표(종합)</td><td>2018-03-06 20:24:00</td><td>이게 실화나ㅡ ㅡㅡㅡㅡ</td></tr><tr class="odd"><td>문 대통령 "서울·평양·판문점 중 北이 판문점 정상회담 선택"</td><td>2018-03-07 15:28:00</td><td>문통 지지 합니다</td></tr><tr class="even"><td>문 대통령 "국외 대북 비밀접촉 없어…저쪽에 놀아나는 것 아냐"</td><td>2018-03-07 16:53:00</td><td>문 대통 령님 지지 합니다</td></tr><tr class="odd"><td>文대통령 "북핵목표는 비핵화…제재완화, 지금은 불가능"(종합)</td><td>2018-03-07 17:16:00</td><td>문대통령님 지지합니다</td></tr><tr class="even"><td>[현장영상] 방미 특사단, 트럼프 대통령 면담 결과 발표</td><td>2018-03-09 09:11:00</td><td>이게 실화냐ㅡㅡㅡㅡ</td></tr></tbody></table><h2 id="conclusions">Conclusions</h2><p>어뷰저를 <strong>타인의 생각에 영향을 미치고 비정상적인 행태를 보이는사용자</strong>로 정의하였고, 이 기준에 따라 어뷰저로 의심되는 사용자를찾아내고자 하였다.</p><ol type="1"><li>순공감 기준, 댓글이 10위권 내에 들었던 횟수가 많았던 작성자중에서</li><li>작성한 댓글 수 대비 top 10 댓글 수의 비율이 일반적이지 않은작성자</li></ol><p>개인적으로, <strong>1.</strong>과 <strong>2.</strong>의 기준에 드는사용자는 <code>user 1</code>, <code>user 3</code> 라는 생각이다. 작성한전체 댓글 수는 다른 사용자들에 비해 적은 편이었지만 top 댓글에 들었던비율은 가장 높았고, 그 수치가 일반적이지는 않았다.</p><p>어뷰저를 찾고자 시작한 분석이었지만 데이터를 살펴보면서 네이버 뉴스댓글이 가지는 단일하고 공개된 ranking system이 얼마나 위험한지를 오히려인식하게 되었다.</p><blockquote><p>분석한 기간에서 중복 제거한 기사의 수는 총 100,780개 였고, 만약 top10 댓글의 작성자가 모두 다른 사용자였다면 1,007,800명이 각자의 의견을개시했을 것이다. 하지만 실제 그 기간에 집계된 unique한 작성자는 총308,731 명에 불과했다. 게다가 중복 댓글까지 포함하면, 그 다양성은 조금더 떨어진다.</p></blockquote><p>이 같은 면에서 <strong>네이버 뉴스 댓글은 다양성을 충분히 수용하고있지 못하다는 생각이 들었다.</strong> "플랫폼이기 때문에 그럴 수 있지않을까?" 싶지만 페이스북이나 유튜브, 레딧같은 다른 플랫폼에서의 댓글을보면 무작정 호감순으로 정렬하지는 않는다. 이 플랫폼들의 기준이 문제가없다는 것은 아니다. 하지만 네이버 뉴스 보다 다양한 기준으로 댓글을정렬시키고 있으며 (최신순, 오래된 순, 공감을 많이 받은 순, relevance 순,호감 + vote의 크기 등) 이를 통해 다양한 의견이 쉽게 노출될 수 있는환경을 조성하였다는 점에서는 좀 더 높은 점수를 주고 싶다.</p><p>그래서 세 번째 글은, 간단하지만 다른 정렬 기준을 적용했을 때 발견되는새로운 댓글에 대해서 다뤄 볼 예정이다. 프로젝트 초창기에, 같이 작업을진행했던 재명님이 돌려본 결과가 있는데 이 것도 조금 다듬어서 올리면재밌을 것 같다! 3탄은... 휴가(<span class="citation"data-cites="Norway">@Norway</span>) 다녀오고 나서 작업해볼까 싶다. To becontinued...</p><h2 id="appendix-a-gaussian-mixture-model-fitting">Appendix A: GaussianMixture Model Fitting</h2><p>GMM의 <code>n_components</code> 최적 개수를 구하기 위해<code>silhouette score</code>를 계산하였다.</p><p><img src="/assets/images/silhouette_score.png?style=centerme" width=70%><br></p><p><code>score</code>가 가장 높은 <code>n_components=2</code> 이므로2개의 gaussian을 가정하여 fitting 해보면 아래 그림과 같다.</p><p><img src="/assets/images/gmm.png?style=centerme" width=80% alt="cluster 1: blue / cluster 2: orange"><br></p><table><thead><tr class="header"><th>userId</th><th>cluster 1 prob</th><th>cluster 2 prob</th><th>ratio (%)</th></tr></thead><tbody><tr class="odd"><td>user 1</td><td>2.27E-16</td><td>1</td><td>95.60</td></tr><tr class="even"><td>user 3</td><td>2.56E-15</td><td>1</td><td>93.08</td></tr><tr class="odd"><td>user 2</td><td>9.31E-14</td><td>1</td><td>89.21</td></tr><tr class="even"><td>user 39</td><td>4.96E-11</td><td>1</td><td>82.00</td></tr><tr class="odd"><td></td><td></td><td></td><td></td></tr><tr class="even"><td>user 6</td><td>8.11E-01</td><td>0.188898</td><td>41.27</td></tr><tr class="odd"><td></td><td></td><td></td><td></td></tr><tr class="even"><td>user 9</td><td>1.00E+00</td><td>0.000219</td><td>15.05</td></tr></tbody></table><h2 id="references">References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://ko.wikipedia.org/wiki/2016%EB%85%84_%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD">https://ko.wikipedia.org/wiki/2016년_대한민국</a><a href="#fnref:1" rev="footnote">↩︎</a></span></li><li id="fn:2"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://ko.wikipedia.org/wiki/2017%EB%85%84_%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD">https://ko.wikipedia.org/wiki/2017년_대한민국</a><a href="#fnref:2" rev="footnote">↩︎</a></span></li><li id="fn:3"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://ko.wikipedia.org/wiki/2018%EB%85%84_%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD">https://ko.wikipedia.org/wiki/2018년_대한민국</a><a href="#fnref:3" rev="footnote">↩︎</a></span></li></ol></div></div>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;span style=&quot;color:red&quot;&gt;NOTICE: 앞으로 소개될 내용은 NAVER와
무관하며, 오히려 NAVER 뉴스가 정치적인 편향성을 가지고 있지 않은
중립적인 플랫폼이라고 생각하기 때문에 분석을 하게 되었음을
알립니다.&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;2015년 12월부터 2018년 5월까지의 데이터로 소위 말하는 어뷰저의 존재를
확인해보았다. 여기서 말하는 어뷰저의 criteria는 다음과 같다.&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;타인의 생각에 영향을 미칠 수 있도록 작성한 댓글이 top 10 내에 한 번
이상 들었어야 한다. 실제 어뷰저였어도 top 댓글이 아니어서 타인에게
영향을 미치지 못했다면 어뷰저라고 불릴 자격(?)이 없다.&lt;/li&gt;
&lt;li&gt;발생하기 어려운 패턴을 보여야 한다.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;이 기준에 따라 분석한 결과, 총 386번 댓글을 남겼고 그 중에서
369번(95.6%) top 10 내에 들었던 유저와 총 289번 댓글을 남기고
269번(93.1%) top 10 내에 들었던 유저를 의심해보게 되었다.</summary>
    
    
    
    <category term="Tech" scheme="https://www.thespacemoon.com/categories/Tech/"/>
    
    <category term="ML" scheme="https://www.thespacemoon.com/categories/Tech/ML/"/>
    
    
    <category term="data analysis" scheme="https://www.thespacemoon.com/tags/data-analysis/"/>
    
  </entry>
  
  <entry>
    <title>Naver News Comment Analysis (1)</title>
    <link href="https://www.thespacemoon.com/2019/07/25/naver-news-comments-analysis-1/"/>
    <id>https://www.thespacemoon.com/2019/07/25/naver-news-comments-analysis-1/</id>
    <published>2019-07-25T13:23:00.000Z</published>
    <updated>2019-07-25T13:23:00.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>올초(3월)부터 같은 팀의 재명님과 <strong>네이버 뉴스 댓글</strong>데이터로 사이드 프로젝트를 시작했다. 직접 크롤링하신 데이터였는데, 그양이 방대해서 "이 정도 데이터가 있으면, 뭔갈 해볼 수 있겠지!" 라는가벼운 마음으로 사이드 프로젝트 제안을 덥석 받아물었다. 그리고 여느사이드 프로젝트가 그렇듯 그 과정은 결코 생각만큼 가볍지는않았더랬다...</p><p>마침 작년 사내 Hackday에서 Abuser Detection 분석으로 좋은 성과를얻었던터라 어뷰저 분석을 해보고 싶었고, 그 결과로 나름 재밌는 것들이발견되었다. 하지만 좋은 발표 자리(이를테면 파이콘이라든지,,,)에 등록할시기를 놓쳐서 논문을 arXiv에 올려두듯이 블로그에 댓글 분석한 내용을공유하고자 한다. <span id="more"></span></p><p>내용은 크게 <strong>뉴스 댓글 수집</strong>과 <strong>뉴스 댓글분석</strong> 파트로 나뉘며, 전자는 재명님이 후자는 내가 주로 담당해서정리하였다. 이번 글은 <strong>뉴스 댓글 분석</strong> 1편이다.</p><h2 id="data">Data</h2><h3 id="수집-기간">수집 기간</h3><p>2006.04.26 ~ 2018.05.25 (수집 시점: 2018.10)</p><h3 id="수집-내용">수집 내용</h3><p>네이버 뉴스의 6개 분야별(정치, 경제, 사회, 생활/문화, 세계, IT/과학)<strong>가장 많이 본 뉴스</strong> 30건</p><p><img src="/assets/images/naver_news_ranking.png?style=centerme" width=85% alt="네이버 뉴스 > 랭킹뉴스 화면 예시"><br></p><p>같은 기사이지만 2-3일 동안 랭킹뉴스에 오를 수 있으므로 중복 기사를제거해주었다. * 중복 제거 전 기사 #: 751,751 (약 75만) * 중복 제거 후기사 #: 643,226 (약 64만)</p><h3 id="분석에-사용한-필드">분석에 사용한 필드</h3><ul><li>기사: 기사 id, 기사 제목, 기사 입력 시각, 기사 내용, 언론사, 기사감정</li><li>댓글: 댓글 작성 기사id, 작성자 hashed id, 댓글 작성 시각, 댓글 내용,공감수, 비공감수</li></ul><h2 id="basic-statistics">Basic Statistics</h2><p>중복 제거된 기사에 대해, 기사 작성 시점을 기준으로 한 달 단위로기사에 달린 코멘트를 집계해서 그래프를 그리면 다음과 같다.</p><p><img src="/assets/images/news_cmnt_all_year.png?style=centerme" width=90%><br></p><p>네이버 뉴스 개편history<sup id="fnref:1"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://namu.wiki/w/네이버_뉴스&gt;">[1]</span></a></sup> 와 엮어서 이 그래프를 해석하면 재밌어진다.</p><h3 id="년">2009년</h3><ul><li>2009년 개편 때는 메인 페이지 뉴스 박스 편집권을 신문사에 넘겼고,기사를 클릭하면 바로 신문사 링크로 연결되게 바뀌었다. 이로 인해 네이버뉴스의 트래픽이 감소하게 되었고 예전과 비교해서 리플 개수나 조회수가<strong>상당히 줄어들었다</strong>.</li></ul><h3 id="년-1">2010년</h3><ul><li>2010년대 초반에 뉴스 스탠드가 도입되면서 메인화면 뉴스 편집권을포기하게 된다. 기사를 클릭하면 기본적으로 네이버 페이지가 아닌 언론사사이트로 연결된다.</li><li><strong>모바일로 댓글을 달 수 없었다</strong>. 또한 댓글 형태가 댓글제목을 클릭해야만 내용을 볼 수 있는 형태라서 결과적으로는 당시 뉴스 댓글란은 지금보다 훨씬 폐쇄적인 모양새였다.</li></ul><h3 id="년-2">2012년</h3><ul><li>검색과 지식인의 인기를 바탕으로 <strong>네이버가 2012년 1일 방문자1800만 명을 기록할 정도로 성장</strong>하는 동안, 네티즌의 뉴스 읽기방식도 달라졌다. 종이신문을 읽거나 신문방송의 홈페이지를 찾아가는 대신,네이버나 다음 등 포털의 뉴스캐스트를 통해 여러 언론사 기사를 한꺼번에읽는 사람들이 크게 늘어난 것이다. 이 때문에 뉴스 편집 기능을 수행하는포털을 언론사로 봐야 할 것이냐 아니냐 하는 논쟁이 언론관련 심의기구등에서 벌어지고 있기도하다.<sup id="fnref:2"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://www.wikitree.co.kr/main/news_view.php?id=71675&gt;">[2]</span></a></sup></li><li>2012년 중반부터 <strong>모바일로도 댓글을 달 수 있</strong>게되었다.</li><li>네이버 아이디로 로그인하지 않아도 트위터나 페이스북 등의 <strong>SNS계정으로 댓글을 달 수 있</strong>게 되었다. 이 때문에 네이버 영화 평점조작처럼 추천수 조작하기도 쉬워졌다. 네이버, 미투데이, 트위터, 페이스북,다음으로 한 번씩만 로그인해도 공감 및 비공감 5개를 줄 수 있다.</li></ul><h3 id="년-3">2016년</h3><ul><li><strong>10월, JTBC에서 최순실의 태블릿 pc를 발견하였고 최순실 게이트사건의 포문이 열리기 시작했다.</strong> (<del><em>트래픽 측면에서 네이버뉴스는 최순실에게 감사하는 마음이 없지 않아 있을것이다...</em></del>)</li><li>그리고 동시에, <strong>드루킹의 댓글 조작 사건</strong>도시작<sup id="fnref:3"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://www.mk.co.kr/news/society/view/2018/05/294952/&gt;">[3]</span></a></sup>되었다.<ul><li>2018년 5월 말, 특검법이 통과된 이후에 댓글이 줄었다는기사<sup id="fnref:4"><a href="#fn:4" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://web.archive.org/web/20180619040311/http://www.munhwa.com/news/view.html?no=2018061101070103011001&gt;">[4]</span></a></sup> 가 보도되었다. 2018년 6월 이후의 댓글이 있었다면그간 댓글부대의 위력이 어느 정도였는지 가늠해볼 수 있었을 것이다.</li></ul></li></ul><p>결론적으로 2016년 후반부 이후 폭발적인 댓글 수의 증가는 정치 및 사회영역의 엄청난 트래픽 덕분이었을 것이다. 가설 검증 차원에서 섹션 별로나누어 같은 방식으로 댓글을 집계해 보았다.<img src="/assets/images/news_cmnt_sections_year.png?style=centerme" width=90% alt="섹션 별 기사 댓글 (누적 그래프)"><img src="/assets/images/news_cmnt_politics_year.png?style=centerme" width=90% alt="타 섹션과 비교한 정치 기사 댓글 그래프"><img src="/assets/images/news_cmnt_society_year.png?style=centerme" width=90% alt="타 섹션과 비교한 사회 기사 댓글 그래프"></p><h2 id="news-sentiment-analysis">News Sentiment Analysis</h2><p>네이버 뉴스는 기사에 <strong>"좋아요"</strong> 를 시작으로<strong>"훈훈해요"</strong>, <strong>"슬퍼요"</strong>,<strong>"화나요"</strong>, <strong>"후속기사 원해요"</strong> 의 label을달 수 있게 만들었다. - "좋아요": 2014년 초 시작 - "훈훈해요", "슬퍼요","화나요", "후속기사 원해요": 2017년 초 시작</p><p><img src="/assets/images/news_emoji_only_like.png?style=centerme" width=75% alt='2016.10.20 기사. "좋아요"만 굉장히 많다.'><img src="/assets/images/news_emoji_all.png?style=centerme" width=75% alt='2017.10.20 기사. 다섯 가지 감정 모두 표를 받(긴)했다.'><br></p><p>"좋아요" 만 있을 때와 다섯 가지의 감정이 있을 때의 추이가 또재밌다.</p><p><img src="/assets/images/news_emoji_all_year.png?style=centerme" width=80%><br></p><p>"좋아요" 외의 다른 감정이 허가된 순간 이후로 "화나요" 가 급격히증가한다.</p><h3 id="정치">정치</h3><p><img src="/assets/images/news_emoji_politics_year.png?style=centerme" width=80%><br></p><ul><li>참고: 사드배치 (2017.03), 문재인 당선(2017.05)<sup id="fnref:5"><a href="#fn:5" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://ko.wikipedia.org/wiki/2017년_대한민국&gt;">[5]</span></a></sup>, 평창 동계 올림픽 (2018.02), 이명박 수감(2018.03)<sup id="fnref:6"><a href="#fn:6" rel="footnote"><spanclass="hint--top hint--error hint--medium hint--rounded hint--bounce"aria-label="&lt;https://ko.wikipedia.org/wiki/2018년_대한민국&gt;">[6]</span></a></sup></li></ul><h3 id="경제">경제</h3><p><img src="/assets/images/news_emoji_economy_year.png?style=centerme" width=80%></p><h3 id="사회">사회</h3><p><img src="/assets/images/news_emoji_society_year.png?style=centerme" width=80%></p><h3 id="문화">문화</h3><p><img src="/assets/images/news_emoji_life_year.png?style=centerme" width=80%><br></p><ul><li>2018년 2월에는 무슨 일이.. (추운 날씨, 성추행 등의 사건 때문으로추측됨)</li></ul><h3 id="it">IT</h3><p><img src="/assets/images/news_emoji_it_year.png?style=centerme" width=80%></p><h3 id="세계">세계</h3><p><img src="/assets/images/news_emoji_world_year.png?style=centerme" width=80%></p><h2 id="conclusions">Conclusions</h2><p>여기까지는 기초적인 데이터 탐색 작업이었다. 간단히 시간 순으로 댓글수를 집계하기만 해도 재미있는 분석 결과를 얻을 수 있었다. (가령,박근혜-최순실 게이트가 얼마나 큰 이슈였는지, 뉴스는 대부분 우리를 열받게하는 내용이라든지 등)</p><p>이 다음 분석은, <em>의심</em>하기만 했던 댓글 어뷰저 집단이 실제로존재하는지에 대해 다룰 예정이다. 마침 댓글 수집 기간과 드루킹의 댓글조작 기간이 맞물려 있어서 분석해 볼 수 있는 데이터가 손에 쥐어졌다.최대한 선입견없이 담백한 분석을 해보려고 노력했다. 정말인지 아닌지 다음글에서 확인해보자.</p><h2 id="references">References</h2><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://namu.wiki/w/%EB%84%A4%EC%9D%B4%EB%B2%84_%EB%89%B4%EC%8A%A4">https://namu.wiki/w/네이버_뉴스</a><a href="#fnref:1" rev="footnote">↩︎</a></span></li><li id="fn:2"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.wikitree.co.kr/main/news_view.php?id=71675">https://www.wikitree.co.kr/main/news_view.php?id=71675</a><a href="#fnref:2" rev="footnote">↩︎</a></span></li><li id="fn:3"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://www.mk.co.kr/news/society/view/2018/05/294952/">https://www.mk.co.kr/news/society/view/2018/05/294952/</a><a href="#fnref:3" rev="footnote">↩︎</a></span></li><li id="fn:4"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://web.archive.org/web/20180619040311/http://www.munhwa.com/news/view.html?no=2018061101070103011001">https://web.archive.org/web/20180619040311/http://www.munhwa.com/news/view.html?no=2018061101070103011001</a><a href="#fnref:4" rev="footnote">↩︎</a></span></li><li id="fn:5"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://ko.wikipedia.org/wiki/2017%EB%85%84_%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD">https://ko.wikipedia.org/wiki/2017년_대한민국</a><a href="#fnref:5" rev="footnote">↩︎</a></span></li><li id="fn:6"><spanstyle="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><spanstyle="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://ko.wikipedia.org/wiki/2018%EB%85%84_%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD">https://ko.wikipedia.org/wiki/2018년_대한민국</a><a href="#fnref:6" rev="footnote">↩︎</a></span></li></ol></div></div>]]></content>
    
    
    <summary type="html">&lt;p&gt;올초(3월)부터 같은 팀의 재명님과 &lt;strong&gt;네이버 뉴스 댓글&lt;/strong&gt;
데이터로 사이드 프로젝트를 시작했다. 직접 크롤링하신 데이터였는데, 그
양이 방대해서 &quot;이 정도 데이터가 있으면, 뭔갈 해볼 수 있겠지!&quot; 라는
가벼운 마음으로 사이드 프로젝트 제안을 덥석 받아물었다. 그리고 여느
사이드 프로젝트가 그렇듯 그 과정은 결코 생각만큼 가볍지는
않았더랬다...&lt;/p&gt;
&lt;p&gt;마침 작년 사내 Hackday에서 Abuser Detection 분석으로 좋은 성과를
얻었던터라 어뷰저 분석을 해보고 싶었고, 그 결과로 나름 재밌는 것들이
발견되었다. 하지만 좋은 발표 자리(이를테면 파이콘이라든지,,,)에 등록할
시기를 놓쳐서 논문을 arXiv에 올려두듯이 블로그에 댓글 분석한 내용을
공유하고자 한다.</summary>
    
    
    
    <category term="Tech" scheme="https://www.thespacemoon.com/categories/Tech/"/>
    
    <category term="ML" scheme="https://www.thespacemoon.com/categories/Tech/ML/"/>
    
    
    <category term="data analysis" scheme="https://www.thespacemoon.com/tags/data-analysis/"/>
    
  </entry>
  
</feed>
