---
title: "PyTorchì˜ IterableDatasetì„ ì‚¬ìš©í•´ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
layout: post
date: 2021-02-21 23:22
description: "PyTorch IterableDataset ì‚¬ìš©ë²•ê³¼ ì£¼ì˜ì . ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¡œë”© ì‹œ Dataset ëŒ€ë¹„ ì¥ë‹¨ì ê³¼ num_workers ì´ìŠˆ ì •ë¦¬."
tags:
- pytorch
categories: 
- Tech
- Engineering
toc: true
widgets:
   - type: toc
     position: left
   - type: categories
     position: left
   - type: recent_posts
     position: left
---

PyTorch 1.2 ì´ìƒë¶€í„° `torch.utils.data` ì—ì„œëŠ” í¬ê²Œ map-style dataset (`torch.utils.data.Dataset`) ê³¼ iterable dataset (`torch.utils.data.IterableDataset`) ì˜ ë‘ ì¢…ë¥˜ì˜ ë°ì´í„° í´ë˜ìŠ¤ë¥¼ ì§€ì›í•˜ê³  ìˆë‹¤. ë°ì´í„° ì‚¬ì´ì¦ˆê°€ í´ ë•ŒëŠ” `IterableDataset` ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ì€ë°, `Dataset` ê³¼ëŠ” ë”œë¦¬ ì•„ì§ ê°œë°œë˜ì–´ì•¼ í•  ê¸°ëŠ¥ì´ ë” í•„ìš”í•œ í´ë˜ìŠ¤ë¼ì„œ ì‚¬ìš©í•  ë•Œì— ìœ ì˜í•  ì ì´ ìˆì–´ ì •ë¦¬í•´ë³´ê²Œ ë˜ì—ˆë‹¤.


<!--more-->

## Map-style Dataset

1.2 ì´í•˜ ë²„ì „ì—ì„œ ì‚¬ìš©ë˜ë˜ map-style datasetì€ memoryì— ëª¨ë“  ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•  ìˆ˜ ìˆì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ê°€ì¥ ì¼ë°˜ì ì¸ dataset type ì´ë‹¤. custom dataset classë¥¼ ìƒì„±í•˜ê³ ì í•  ë•Œ `torch.utils.data.Dataset` ì„ ìƒì†ë°›ì•„ `__len__` , `__getitem__` ì„ êµ¬í˜„í•˜ë©´ ëœë‹¤.

```python
from torch.utils.data import Dataset

class MyMapDataset(Dataset):

    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        return self.data['text'][index]
```

## Iterable Dataset

í•˜ì§€ë§Œ í•™ìŠµ ë°ì´í„°ê°€ ë©”ëª¨ë¦¬ì— ë‹¤ ì˜¬ë¼ê°€ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ë°©ë²• ì¤‘ì— í•˜ë‚˜ë¡œ, `torch.utils.data.IterableDataset` ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤. Map-style Datasetê³¼ ë¹„ìŠ·í•˜ê²Œ `torch.utils.data.IterableDataset` ì„ ìƒì†ë°›ì•„ì„œ custom dataset classë¥¼ ìƒì„±í•˜ê³ , `__iter__` ë¥¼ ì„ ì–¸í•˜ë©´ ëœë‹¤.

```python
from torch.utils.data import IterableDataset

class MyIterableDataset(IterableDataset):

    def __init__(self, data_path):
        self.data_path = data_path

    def __iter__(self):
        iter_csv = pd.read_csv(self.data_path, sep='\t', iterator=True, chunksize=1)
        for line in iter_csv:
            line = line['text'].item()
            yield line
```

`Dataset`ì´ batch dataë¥¼ ìƒì„±í•  ë•Œ `map_dataset[index]`ë¥¼ ì‚¬ìš©í•œë‹¤ë©´, `IterableDataset`ì€  `next(iterable_dataset)` ì„ ì‚¬ìš©í•œë‹¤. ì´ ë•Œë¬¸ì— `DataLoader`ë¥¼ í†µí•´ `IterableDataset`ì„ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ `sampler` ì˜µì…˜ì˜ ì‚¬ìš©ì´ ì–´ë µë‹¤. ê·¸ë˜ì„œ random suffling ì„ í•˜ê³  ì‹¶ë‹¤ë©´ ë¯¸ë¦¬ ë°ì´í„°ì…‹ì„ shuffling í•œ ì´í›„ì— ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì´ ì¢‹ë‹¤.

## Going Parallel

[PyTorch ê³µì‹ë¬¸ì„œ](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset)ì— ë”°ë¥´ë©´ `IterableDataset`ì„ `num_workers > 0`ì˜ ì¡°ê±´ì—ì„œ ì‚¬ìš©í•  ë•Œ íŠ¹ë³„íˆ ë‹¤ìŒì„ ìœ ë…í•  ê²ƒì„ ì œì•ˆí•˜ê³  ìˆë‹¤.

> When `num_workers > 0`, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. `get_worker_info()`, when called in a worker process, returns information about the worker. It can be used in either the datasetâ€™s `__iter__()` method or the `DataLoader` â€˜s `worker_init_fn` option to modify each copyâ€™s behavior.

ìœ„ì˜ ë¬¸ì¥ì„ ì´í•´í•˜ë ¤ë©´ `num_workers` ì— ëŒ€í•œ ì´í•´ì™€, `num_workers > 0` ì¼ ë•Œ  `IterDataset` ì—ì„œ ì–´ë–¤ í˜„ìƒì´ ì¼ì–´ë‚˜ëŠ”ì§€ ì•Œì•„ì•¼í•œë‹¤.

<img src="/assets/images/pytorch-iterabledataset-numworkers.png?style=centerme" alt="num_workers == 2 ì¸ ê²½ìš° ë°œìƒí•˜ëŠ” ëª¨ìŠµì´ë‹¤. ìœ„ì˜ ë‘ ë¼ì¸ì€ subprocessì´ë©°, ë§¨ ì•„ë˜ ë¼ì¸ì€ main processì´ë‹¤. íŒŒë€ìƒ‰ ë°•ìŠ¤ëŠ” single datapointë¥¼ ë¡œë”©í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©° ë¶‰ì€ìƒ‰ ë°•ìŠ¤ëŠ” ë¡œë”©ëœ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ì˜ë¯¸í•œë‹¤. (image credit: https://medium.com/speechmatics/how-to-build-a-streaming-dataloader-with-pytorch-a66dd891d9dd)" width=90%>

`num_workers`ëŠ” ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¬ ë•Œ ì‚¬ìš©í•  subprocessì˜ ê°œìˆ˜ì´ë‹¤. `num_workers == 0` ì€ main processì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ëª¨ë¸ì— passí•˜ëŠ” ì‘ì—…ì„ ëª¨ë‘ ìˆ˜í–‰í•œë‹¤ëŠ” ëœ»ì´ë©°, `num_workers == 2`ëŠ” subprocess 2ê°œì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  main processì—ì„œëŠ” subprocessì—ì„œ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ modelì— passí•˜ëŠ” ì—­í• ë§Œ ë‹´ë‹¹í•œë‹¤. ë”°ë¼ì„œ `num_workers > 0` ì¼ ë•Œ data loadingì—ì„œì˜ ë³‘ëª©ì´ ì¤„ì–´ë“¤ë©° gpu utilization ì„ 100% ê°€ê¹Œì´ ëŒì–´ì˜¬ë¦´ ìˆ˜ ìˆë‹¤. 

ê·¸ëŸ¼, `num_workers > 0` ì¼ ë•Œ ì–´ë–¤ í˜„ìƒì´ ë°œìƒí•˜ëŠ”ì§€ ì‚´í´ë³´ì.

### Map-Style Dataset

```python
from torch.utils.data import DataLoader, Dataset, IterableDataset
import time

class MyMapDataset(Dataset):
    
    def __init__(self, data):
        self.data = data
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, index):
        worker = torch.utils.data.get_worker_info()
        worker_id = worker.id if worker is not None else -1
        
        start = time.time()
        time.sleep(0.1)
        end = time.time()
        
        return self.data[index], worker_id, start, end

data = range(16)
map_dataset = MyMapDataset(data)
```

- `num_workers == 0` ì¸ ê²½ìš°

```python
loader = DataLoader(map_dataset, batch_size=4, num_workers=0)
for d in loader:
    batch, worker_ids, starts, ends = d
    print(batch, worker_ids)

-----
tensor([0, 1, 2, 3]) tensor([-1, -1, -1, -1])
tensor([4, 5, 6, 7]) tensor([-1, -1, -1, -1])
tensor([ 8,  9, 10, 11]) tensor([-1, -1, -1, -1])
tensor([12, 13, 14, 15]) tensor([-1, -1, -1, -1])
```

- `num_workers == 2` ì¸ ê²½ìš°

```python
loader = DataLoader(map_dataset, batch_size=4, num_workers=2)
for d in loader:
    batch, worker_ids, starts, ends = d
    print(batch, worker_ids)

-----
tensor([0, 1, 2, 3]) tensor([0, 0, 0, 0])
tensor([4, 5, 6, 7]) tensor([1, 1, 1, 1])
tensor([ 8,  9, 10, 11]) tensor([0, 0, 0, 0])
tensor([12, 13, 14, 15]) tensor([1, 1, 1, 1])
```

ì˜ë„í•œëŒ€ë¡œ, subprocess ë³„ë¡œ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

### Iterable Dataset

```python
from torch.utils.data import DataLoader, Dataset, IterableDataset
import time

class MyIterableDataset(IterableDataset):
    
    def __init__(self, data):
        self.data = data
    
    def __iter__(self):
        for x in self.data:
            worker = torch.utils.data.get_worker_info()
            worker_id = worker.id if worker is not None else -1
        
            start = time.time()
            time.sleep(0.1)
            end = time.time()
        
            yield x, worker_id, start, end

data = range(16)
iterable_dataset = MyIterableDataset(data)
```

- `num_workers == 0`

```python
loader = DataLoader(iterable_dataset, batch_size=4, num_workers=0)
for d in loader:
    batch, worker_ids, starts, ends = d
    print(batch, worker_ids)

-----
tensor([0, 1, 2, 3]) tensor([-1, -1, -1, -1])
tensor([4, 5, 6, 7]) tensor([-1, -1, -1, -1])
tensor([ 8,  9, 10, 11]) tensor([-1, -1, -1, -1])
tensor([12, 13, 14, 15]) tensor([-1, -1, -1, -1])
```

- `num_workers == 2`

```python
loader = DataLoader(iterable_dataset, batch_size=4, num_workers=2)
for d in loader:
    batch, worker_ids, starts, ends = d
    print(batch, worker_ids)

----
tensor([0, 1, 2, 3]) tensor([0, 0, 0, 0])
tensor([0, 1, 2, 3]) tensor([1, 1, 1, 1])
tensor([4, 5, 6, 7]) tensor([0, 0, 0, 0])
tensor([4, 5, 6, 7]) tensor([1, 1, 1, 1])
tensor([ 8,  9, 10, 11]) tensor([0, 0, 0, 0])
tensor([ 8,  9, 10, 11]) tensor([1, 1, 1, 1])
tensor([12, 13, 14, 15]) tensor([0, 0, 0, 0])
tensor([12, 13, 14, 15]) tensor([1, 1, 1, 1])
```

âš ï¸ worker 0ê³¼ worker 1ì—ì„œ ê°™ì€ ë°ì´í„°ë¥¼ ë¡œë”©í•˜ê³  ìˆë‹¤. ì´ ì ì´ ê³µì‹ë¬¸ì„œì—ì„œ ì§€ì í•˜ê³  ìˆëŠ” ë‚´ìš©ì´ë‹¤. ê° ì›Œì»¤ë³„ë¡œ ê°™ì€ `__iter__()`ë¥¼  ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ê°™ì€ ë°ì´í„°ë¥¼ ë¡œë”©í•˜ê²Œ ëœë‹¤. **ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œëŠ”  `worker_init_fn` ì—ì„œ ì§ì ‘ ì›Œì»¤ ë³„ ë°ì´í„°ë¥¼ ì¬ë¶„ë°° ì‹œì¼œì¤˜ì•¼ í•œë‹¤.**

```python
def worker_init_fn(_):
    worker_info = torch.utils.data.get_worker_info()
    
    dataset = worker_info.dataset
    worker_id = worker_info.id
    split_size = len(dataset.data) // worker_info.num_workers
    
    dataset.data = dataset.data[worker_id * split_size: (worker_id + 1) * split_size]
```

```python
loader = DataLoader(iterable_dataset, batch_size=4, num_workers=2, worker_init_fn=worker_init_fn)
for d in loader:
    batch, worker_ids, starts, ends = d
    print(batch, worker_ids)

-----
tensor([0, 1, 2, 3]) tensor([0, 0, 0, 0])
tensor([ 8,  9, 10, 11]) tensor([1, 1, 1, 1])
tensor([4, 5, 6, 7]) tensor([0, 0, 0, 0])
tensor([12, 13, 14, 15]) tensor([1, 1, 1, 1])
```

`worker_init_fn` ì„ í†µí•´ ë¶„ë°°ì‹œì¼œì¤€ ê²°ê³¼ worker 0ê³¼ worker 1 ì—ì„œ ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤ ğŸ™‚

## Conclusions

- `IterableDataset` ì€ ë°ì´í„°ê°€ ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°€ì§€ ì•Šì„ë§Œí¼ í´ ë•Œ ì‚¬ìš©í•˜ë©´ ì¢‹ë‹¤.
- `Dataset`ê³¼ ë‹¤ë¥´ê²Œ `__iter__()`ë¥¼ ì„ ì–¸í•´ì„œ ë°ì´í„°ë¥¼ ë¶€ë¥¸ë‹¤.
    - í•˜ì§€ë§Œ ì´ íŠ¹ì§• ë•Œë¬¸ì— `Sampler` ì™€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.
    - ë˜í•œ `num_workers > 0` ì¸ ì„¸íŒ…ì—ì„œëŠ” ê° ì›Œì»¤ì—ì„œ ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆë„ë¡ `worker_init_fn`ì„ ì„ ì–¸í•´ì•¼ í•œë‹¤.

## References

- [How to Build a Streaming DataLoader with PyTorch | by David MacLeod | Speechmatics | Medium](https://medium.com/speechmatics/how-to-build-a-streaming-dataloader-with-pytorch-a66dd891d9dd)
- [https://inmoonlight.github.io/notebooks/html/2021-02-21-PyTorch-Dataset.html](https://inmoonlight.github.io/notebooks/html/2021-02-21-PyTorch-Dataset.html)
- [torch.utils.data â€” PyTorch 1.7.1 documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset)
